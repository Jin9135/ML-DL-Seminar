{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnXtrxuUswFU"
      },
      "source": [
        "#**Optimization & OvO**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsi4CNobs8U3",
        "outputId": "416cd2b8-f46f-44a0-d8e3-f44c5082e28c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ucimlrepo\n",
            "  Downloading ucimlrepo-0.0.7-py3-none-any.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ucimlrepo) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2020.12.5 in /usr/local/lib/python3.11/dist-packages (from ucimlrepo) (2025.1.31)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->ucimlrepo) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->ucimlrepo) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->ucimlrepo) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->ucimlrepo) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->ucimlrepo) (1.17.0)\n",
            "Downloading ucimlrepo-0.0.7-py3-none-any.whl (8.0 kB)\n",
            "Installing collected packages: ucimlrepo\n",
            "Successfully installed ucimlrepo-0.0.7\n"
          ]
        }
      ],
      "source": [
        "pip install ucimlrepo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZT47Jpt0tLI9"
      },
      "source": [
        "###L-BFGS-B (Limited-memory Broyden-Fletcher-Goldfarb-Shanno with Box constraints)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {
        "id": "vVayD5ZkqfsS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.optimize import minimize\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from itertools import combinations\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "np.random.seed(42)  # 시드 설정\n",
        "\n",
        "class LBFGS_SVM:\n",
        "    def __init__(self, C=0.01, svm_type='L2', c=1.0):\n",
        "        self.C = C  # 정규화 상수\n",
        "        self.svm_type = svm_type  # L1 또는 L2\n",
        "        self.w_best = None\n",
        "        self.b_best = None\n",
        "        self.c = c\n",
        "\n",
        "    def compute_slack_term(self, xi):\n",
        "        \"\"\" SVM 타입별 슬랙 변수 비용 계산 \"\"\"\n",
        "        if self.svm_type == 'L1':\n",
        "            return np.abs(xi)\n",
        "        elif self.svm_type == 'L2':\n",
        "            return xi ** 2\n",
        "        elif self.svm_type == 'Fair':\n",
        "            return np.sum(self.c**2 * ((xi / self.c) - np.log(1 + xi / self.c)))\n",
        "        elif self.svm_type == 'Cauchy':\n",
        "            return np.sum((self.c**2 / 2) * np.log(1 + (xi / self.c)**2))\n",
        "        elif self.svm_type == 'Welsch':\n",
        "            return np.sum((self.c**2 / 2) * (1 - np.exp(-(xi / self.c)**2)))\n",
        "        elif self.svm_type == 'Geman-McClure':\n",
        "            return np.sum(((xi**2)/2) / (1 + xi**2))\n",
        "        else:\n",
        "            raise ValueError(\"Unknown SVM type. Choose from ['L1', 'L2', 'Cauchy', 'Fair', 'Welsch', 'Geman-McClure'].\")\n",
        "\n",
        "    def fitness(self, params, X, y):\n",
        "        \"\"\" 손실 함수 계산 (Hinge Loss 기반) \"\"\"\n",
        "        n_features = X.shape[1]\n",
        "        w = params[:n_features]\n",
        "        b = params[n_features]\n",
        "        margins = y * (np.dot(X, w) + b)\n",
        "        slack = np.maximum(0, 1 - margins)  # Hinge Loss 적용\n",
        "        slack_term = self.compute_slack_term(slack)\n",
        "        regularization = 0.5 * np.dot(w, w)  # L2 Regularization\n",
        "        return regularization + self.C * np.sum(slack_term)  # 최소화할 손실 값\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        initial_params = np.zeros(n_features + 1)\n",
        "        bounds = [(None, None)] * n_features + [(None, None)]  # 경계 조건 추가\n",
        "        options = {'disp': True, 'maxiter': 5000, 'ftol': 1e-9}  # 최적화 옵션 추가: maxiter 증가, ftol 추가\n",
        "        result = minimize(self.fitness, initial_params, args=(X, y), method='L-BFGS-B', bounds=bounds, options=options)\n",
        "        # If optimization fails, try SLSQP solver\n",
        "        if not result.success:\n",
        "            result = minimize(self.fitness, initial_params, args=(X, y), method='SLSQP', bounds=bounds, options=options)\n",
        "\n",
        "        if result.success:\n",
        "            self.w_best = result.x[:n_features]\n",
        "            self.b_best = result.x[n_features]\n",
        "        else:\n",
        "            raise ValueError(\"Optimization failed: \" + result.message)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.sign(np.dot(X, self.w_best) + self.b_best)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {
        "id": "pkaTtUyutCrt"
      },
      "outputs": [],
      "source": [
        "def train_ovo(X, y, C, svm_type):\n",
        "    classes = np.unique(y)\n",
        "    models = []\n",
        "    class_pairs = list(combinations(classes, 2))\n",
        "    for (cl1, cl2) in class_pairs:\n",
        "        mask = (y == cl1) | (y == cl2)\n",
        "        X_bin = X[mask]\n",
        "        y_bin = y[mask]\n",
        "        y_bin = (y_bin == cl1).astype(int) * 2 - 1\n",
        "        model = LBFGS_SVM(C=C, svm_type=svm_type)\n",
        "        model.fit(X_bin, y_bin)\n",
        "        w = model.w_best\n",
        "        b = model.b_best\n",
        "        models.append(((cl1, cl2), (w, b)))\n",
        "    return models\n",
        "\n",
        "def predict_ovo(X, models):\n",
        "    votes = np.zeros((X.shape[0], len(models)))\n",
        "    classes = np.unique(np.hstack([pair for pair, _ in models]))\n",
        "    for i, ((cl1, cl2), (w, b)) in enumerate(models):\n",
        "        preds = np.sign(X.dot(w) + b)\n",
        "        votes[:, i] = np.where(preds == 1, cl1, cl2)\n",
        "    final_predictions = []\n",
        "    for j in range(votes.shape[0]):\n",
        "        valid_votes = votes[j][votes[j] >= 0]\n",
        "        if len(valid_votes) > 0:\n",
        "            bincount = np.bincount(valid_votes.astype(int))\n",
        "            final_predictions.append(bincount.argmax())\n",
        "        else:\n",
        "            final_predictions.append(np.random.choice(classes))\n",
        "    return np.array(final_predictions)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DYvMYSttRkI"
      },
      "source": [
        "###1. Genetic Algorithm (GA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "Xn8t5QNstTVR"
      },
      "outputs": [],
      "source": [
        "class GA_SVM:\n",
        "    def __init__(self, C=1.0, pop_size=20, max_iter=100, mutation_rate=0.1, crossover_rate=0.7, svm_type='L2', c=1.0):\n",
        "        self.C = C  # 정규화 상수\n",
        "        self.pop_size = pop_size  # 개체 수\n",
        "        self.max_iter = max_iter  # 세대 수\n",
        "        self.mutation_rate = mutation_rate  # 돌연변이 확률\n",
        "        self.crossover_rate = crossover_rate  # 교차 확률\n",
        "        self.svm_type = svm_type  # 'L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure'\n",
        "        self.w_best = None\n",
        "        self.b_best = None\n",
        "        self.c = c\n",
        "\n",
        "    def compute_slack_term(self, xi):\n",
        "        if self.svm_type == 'L1':\n",
        "            return np.abs(xi)\n",
        "        elif self.svm_type == 'L2':\n",
        "            return xi ** 2\n",
        "        elif self.svm_type == 'Fair':\n",
        "            return np.sum(self.c**2 * ((xi / self.c) - np.log(1 + xi / self.c)))\n",
        "        elif self.svm_type == 'Cauchy':\n",
        "            return np.sum((self.c**2 / 2) * np.log(1 + (xi / self.c)**2))\n",
        "        elif self.svm_type == 'Welsch':\n",
        "            return np.sum((self.c**2 / 2) * (1 - np.exp(-(xi / self.c)**2)))\n",
        "        elif self.svm_type == 'Geman-McClure':\n",
        "            return np.sum(((xi**2)/2) / (1 + xi**2))\n",
        "        else:\n",
        "            raise ValueError(\"Unknown SVM type. Choose from ['L1', 'L2', 'Cauchy', 'Fair', 'Welsch', 'Geman-McClure'].\")\n",
        "\n",
        "    def fitness(self, w, b, X, y):\n",
        "        margins = y * (np.dot(X, w) + b)\n",
        "        slack = np.maximum(0, 1 - margins)\n",
        "        slack_term = self.compute_slack_term(slack)\n",
        "        regularization = 0.5 * np.dot(w, w)\n",
        "        return regularization + self.C * np.sum(slack_term)\n",
        "\n",
        "    def initialize_population(self, n_features):\n",
        "        population = [(np.random.randn(n_features), np.random.randn()) for _ in range(self.pop_size)]\n",
        "        return population\n",
        "\n",
        "    def selection(self, population, fitness_values):\n",
        "        probabilities = 1 / (fitness_values + 1e-6)\n",
        "        probabilities /= probabilities.sum()\n",
        "        selected_indices = np.random.choice(len(population), size=len(population), p=probabilities)\n",
        "        return [population[i] for i in selected_indices]\n",
        "\n",
        "    def crossover(self, parent1, parent2):\n",
        "        w1, b1 = parent1\n",
        "        w2, b2 = parent2\n",
        "        if np.random.rand() < self.crossover_rate:\n",
        "            point = np.random.randint(len(w1))\n",
        "            new_w = np.concatenate((w1[:point], w2[point:]))\n",
        "            new_b = (b1 + b2) / 2\n",
        "        else:\n",
        "            new_w, new_b = w1.copy(), b1\n",
        "        return new_w, new_b\n",
        "\n",
        "    def mutation(self, w, b):\n",
        "        if np.random.rand() < self.mutation_rate:\n",
        "            mutation_vector = np.random.randn(*w.shape) * 0.1\n",
        "            w += mutation_vector\n",
        "            b += np.random.randn() * 0.1\n",
        "        return w, b\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        population = self.initialize_population(n_features)\n",
        "        fitness_values = np.array([self.fitness(w, b, X, y) for w, b in population])\n",
        "        np.random.seed(42)  # 시드 설정\n",
        "        for _ in range(self.max_iter):\n",
        "            selected_population = self.selection(population, fitness_values)\n",
        "            new_population = []\n",
        "            for i in range(0, len(selected_population), 2):\n",
        "                p1, p2 = selected_population[i], selected_population[(i + 1) % len(selected_population)]\n",
        "                offspring1 = self.crossover(p1, p2)\n",
        "                offspring2 = self.crossover(p2, p1)\n",
        "                new_population.append(self.mutation(*offspring1))\n",
        "                new_population.append(self.mutation(*offspring2))\n",
        "            population = new_population\n",
        "            fitness_values = np.array([self.fitness(w, b, X, y) for w, b in population])\n",
        "        best_idx = np.argmin(fitness_values)\n",
        "        self.w_best, self.b_best = population[best_idx]\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.sign(np.dot(X, self.w_best) + self.b_best)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "AAk-jyeCtTYO"
      },
      "outputs": [],
      "source": [
        "def train_ovo(X, y, C, svm_type):\n",
        "    classes = np.unique(y)\n",
        "    models = []\n",
        "    class_pairs = list(combinations(classes, 2))\n",
        "    for (cl1, cl2) in class_pairs:\n",
        "        mask = (y == cl1) | (y == cl2)\n",
        "        X_bin = X[mask]\n",
        "        y_bin = y[mask]\n",
        "        y_bin = (y_bin == cl1).astype(int) * 2 - 1\n",
        "        model = GA_SVM(C=C, svm_type=svm_type)\n",
        "        model.fit(X_bin, y_bin)\n",
        "        w = model.w_best\n",
        "        b = model.b_best\n",
        "        models.append(((cl1, cl2), (w, b)))\n",
        "    return models\n",
        "\n",
        "def predict_ovo(X, models):\n",
        "    votes = np.zeros((X.shape[0], len(models)))\n",
        "    classes = np.unique(np.hstack([pair for pair, _ in models]))\n",
        "    for i, ((cl1, cl2), (w, b)) in enumerate(models):\n",
        "        preds = np.sign(X.dot(w) + b)\n",
        "        votes[:, i] = np.where(preds == 1, cl1, cl2)\n",
        "    final_predictions = []\n",
        "    for j in range(votes.shape[0]):\n",
        "        valid_votes = votes[j][votes[j] >= 0]\n",
        "        if len(valid_votes) > 0:\n",
        "            bincount = np.bincount(valid_votes.astype(int))\n",
        "            final_predictions.append(bincount.argmax())\n",
        "        else:\n",
        "            final_predictions.append(np.random.choice(classes))\n",
        "    return np.array(final_predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZD-fWtrtR1R"
      },
      "source": [
        "###SMO (Sequential Minial Optimization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "yoZ9zBsFtT6f"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "from itertools import combinations\n",
        "\n",
        "class SMO_SVM:\n",
        "    def __init__(self, C=1.0, kernel='linear', tol=1e-3, max_iter=1000, svm_type='L2', c=1.0, lr=0.01):\n",
        "        self.C = C\n",
        "        self.kernel = kernel\n",
        "        self.tol = tol\n",
        "        self.max_iter = max_iter\n",
        "        self.svm_type = svm_type\n",
        "        self.c = c\n",
        "        self.lr = lr\n",
        "        self.w_best = None  # 최적 가중치 벡터 추가\n",
        "        self.b_best = None  # 최적 편향 값 추가\n",
        "\n",
        "    def kernel_function(self, X, Y):\n",
        "        if self.kernel == 'linear':\n",
        "            return np.dot(X, Y.T)\n",
        "        else:\n",
        "            raise ValueError(\"Only linear kernel is supported in this implementation.\")\n",
        "\n",
        "    def compute_slack_term(self, xi):\n",
        "        xi = np.maximum(xi, 1e-6)\n",
        "\n",
        "        if self.svm_type == 'Fair':\n",
        "            return np.sum(self.c**2 * ((xi / self.c) - np.log(1 + xi / self.c)))\n",
        "        elif self.svm_type == 'Cauchy':\n",
        "            return np.sum((self.c**2 / 2) * np.log(1 + (xi / self.c)**2))\n",
        "        elif self.svm_type == 'Welsch':\n",
        "            return np.sum((self.c**2 / 2) * (1 - np.exp(-(xi / self.c)**2)))\n",
        "        elif self.svm_type == 'Geman-McClure':\n",
        "            return np.sum(((xi**2)/2) / (1 + xi**2))\n",
        "        elif self.svm_type == 'L1':\n",
        "            return np.abs(xi)\n",
        "        elif self.svm_type == 'L2':\n",
        "            return xi ** 2\n",
        "        else:\n",
        "            raise ValueError(\"Unknown SVM type. Choose from ['L1', 'L2', 'Cauchy', 'Fair', 'Welsch', 'Geman-McClure'].\")\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        self.alpha = np.zeros(n_samples)\n",
        "        self.b = 0\n",
        "        self.w = np.zeros(n_features)\n",
        "\n",
        "        for _ in range(self.max_iter):\n",
        "            for i in range(n_samples):\n",
        "                xi, yi = X[i], y[i]\n",
        "                margin = yi * (np.dot(self.w, xi) + self.b)\n",
        "                slack = max(0, 1 - margin)\n",
        "                slack_term = self.compute_slack_term(slack)\n",
        "\n",
        "                if slack > 0:\n",
        "                    delta_alpha = self.C * (1 - margin) - slack_term\n",
        "                    self.alpha[i] = np.clip(self.alpha[i] + self.lr * delta_alpha, 0, self.C)\n",
        "                    self.w += self.lr * self.alpha[i] * yi * xi\n",
        "                    self.b += self.lr * self.alpha[i] * yi\n",
        "\n",
        "        self.w_best = self.w  # 최적의 가중치 저장\n",
        "        self.b_best = self.b  # 최적의 편향 값 저장\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.sign(np.dot(X, self.w_best) + self.b_best)  # b 대신 b_best 사용\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "mXqyiWhRtT9w"
      },
      "outputs": [],
      "source": [
        "def train_ovo(X, y, C, svm_type):\n",
        "    classes = np.unique(y)\n",
        "    models = []\n",
        "    class_pairs = list(combinations(classes, 2))\n",
        "    for (cl1, cl2) in class_pairs:\n",
        "        mask = (y == cl1) | (y == cl2)\n",
        "        X_bin = X[mask]\n",
        "        y_bin = y[mask]\n",
        "        y_bin = (y_bin == cl1).astype(int) * 2 - 1\n",
        "        # Convert X_bin and y_bin to NumPy arrays if they are pandas Series\n",
        "        X_bin = X_bin.to_numpy() if isinstance(X_bin, pd.Series) else X_bin\n",
        "        y_bin = y_bin.to_numpy() if isinstance(y_bin, pd.Series) else y_bin\n",
        "        model = SMO_SVM(C=C, svm_type=svm_type)\n",
        "        model.fit(X_bin, y_bin)\n",
        "        w = model.w\n",
        "        b = model.b\n",
        "        models.append(((cl1, cl2), (w, b)))\n",
        "    return models\n",
        "\n",
        "def predict_ovo(X, models):\n",
        "    votes = np.zeros((X.shape[0], len(models)))\n",
        "    classes = np.unique(np.hstack([pair for pair, _ in models]))\n",
        "    for i, ((cl1, cl2), (w, b)) in enumerate(models):\n",
        "        preds = np.sign(X.dot(w) + b)\n",
        "        votes[:, i] = np.where(preds == 1, cl1, cl2)\n",
        "    final_predictions = []\n",
        "    for j in range(votes.shape[0]):\n",
        "        valid_votes = votes[j][votes[j] >= 0]\n",
        "        if len(valid_votes) > 0:\n",
        "            bincount = np.bincount(valid_votes.astype(int))\n",
        "            final_predictions.append(bincount.argmax())\n",
        "        else:\n",
        "            final_predictions.append(np.random.choice(classes))\n",
        "    return np.array(final_predictions)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pc7uliY_tSKB"
      },
      "source": [
        "###PSO (Particle Swarm Optimization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 177,
      "metadata": {
        "id": "xe9iCzf4tUxE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from itertools import combinations\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "class PSO_SVM:\n",
        "    def __init__(self, C=1.0, num_particles=30, max_iter=100, svm_type='L2', c=1.0):\n",
        "        self.C = C  # 정규화 상수\n",
        "        self.num_particles = num_particles  # PSO 입자 개수\n",
        "        self.max_iter = max_iter  # PSO 반복 횟수\n",
        "        self.svm_type = svm_type  # L1 또는 L2\n",
        "        self.w_best = None\n",
        "        self.b_best = None\n",
        "        self.c = c\n",
        "\n",
        "    def compute_slack_term(self, xi):\n",
        "        \"\"\" SVM 타입별 슬랙 변수 비용 계산 \"\"\"\n",
        "        if self.svm_type == 'L1':\n",
        "            return np.abs(xi)\n",
        "        elif self.svm_type == 'L2':\n",
        "            return xi ** 2\n",
        "        elif self.svm_type == 'Fair':\n",
        "            return np.sum(self.c**2 * ((xi / self.c) - np.log(1 + xi / self.c)))\n",
        "        elif self.svm_type == 'Cauchy':\n",
        "            return np.sum((self.c**2 / 2) * np.log(1 + (xi / self.c)**2))\n",
        "        elif self.svm_type == 'Welsch':\n",
        "            return np.sum((self.c**2 / 2) * (1 - np.exp(-(xi / self.c)**2)))\n",
        "        elif self.svm_type == 'Geman-McClure':\n",
        "            return np.sum(((xi**2)/2) / (1 + xi**2))\n",
        "        else:\n",
        "            raise ValueError(\"Unknown SVM type. Choose from ['L1', 'L2', 'Cauchy', 'Fair', 'Welsch', 'Geman-McClure'].\")\n",
        "\n",
        "    def fitness(self, w, b, X, y):\n",
        "        \"\"\" 손실 함수 계산 (Hinge Loss 기반) \"\"\"\n",
        "        margins = y * (np.dot(X, w) + b)\n",
        "        slack = np.maximum(0, 1 - margins)  # Hinge Loss 적용\n",
        "        slack_term = self.compute_slack_term(slack)\n",
        "        regularization = 0.5 * np.dot(w, w)  # L2 Regularization\n",
        "        return regularization + self.C * np.sum(slack_term)  # 최소화할 손실 값\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # PSO 초기화\n",
        "        w_particles = np.random.randn(self.num_particles, n_features)  # 초기 w 값\n",
        "        b_particles = np.random.randn(self.num_particles)  # 초기 b 값\n",
        "        velocities_w = np.random.randn(self.num_particles, n_features) * 0.1  # 속도 초기화\n",
        "        velocities_b = np.random.randn(self.num_particles) * 0.1\n",
        "\n",
        "        # 개별 최적 및 전역 최적 초기화\n",
        "        p_best_w = np.copy(w_particles)\n",
        "        p_best_b = np.copy(b_particles)\n",
        "        p_best_scores = np.array([self.fitness(w, b, X, y) for w, b in zip(w_particles, b_particles)])\n",
        "\n",
        "        g_best_index = np.argmin(p_best_scores)\n",
        "        g_best_w = p_best_w[g_best_index]\n",
        "        g_best_b = p_best_b[g_best_index]\n",
        "\n",
        "        # PSO 학습 진행\n",
        "        w_inertia = 0.7  # 관성 계수\n",
        "        c1 = 1.5  # 개인 최적화 계수\n",
        "        c2 = 1.5  # 글로벌 최적화 계수\n",
        "\n",
        "        for _ in range(self.max_iter):\n",
        "            for i in range(self.num_particles):\n",
        "                r1, r2 = np.random.rand(), np.random.rand()\n",
        "                velocities_w[i] = (w_inertia * velocities_w[i] +\n",
        "                                   c1 * r1 * (p_best_w[i] - w_particles[i]) +\n",
        "                                   c2 * r2 * (g_best_w - w_particles[i]))\n",
        "                velocities_b[i] = (w_inertia * velocities_b[i] +\n",
        "                                   c1 * r1 * (p_best_b[i] - b_particles[i]) +\n",
        "                                   c2 * r2 * (g_best_b - b_particles[i]))\n",
        "\n",
        "                # 업데이트된 위치\n",
        "                w_particles[i] += velocities_w[i]\n",
        "                b_particles[i] += velocities_b[i]\n",
        "\n",
        "                # 새로운 피트니스 값 계산\n",
        "                new_fitness = self.fitness(w_particles[i], b_particles[i], X, y)\n",
        "\n",
        "                # 최적값 갱신\n",
        "                if new_fitness < p_best_scores[i]:\n",
        "                    p_best_w[i] = w_particles[i]\n",
        "                    p_best_b[i] = b_particles[i]\n",
        "                    p_best_scores[i] = new_fitness\n",
        "\n",
        "            # 전체 최적 갱신\n",
        "            g_best_index = np.argmin(p_best_scores)\n",
        "            g_best_w = p_best_w[g_best_index]\n",
        "            g_best_b = p_best_b[g_best_index]\n",
        "\n",
        "        self.w_best = g_best_w\n",
        "        self.b_best = g_best_b\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.sign(np.dot(X, self.w_best) + self.b_best)\n",
        "def train_ovo(X, y, C, svm_type):\n",
        "    classes = np.unique(y)\n",
        "    models = []\n",
        "    class_pairs = list(combinations(classes, 2))\n",
        "    for (cl1, cl2) in class_pairs:\n",
        "        mask = (y == cl1) | (y == cl2)\n",
        "        X_bin = X[mask]\n",
        "        y_bin = y[mask]\n",
        "        y_bin = (y_bin == cl1).astype(int) * 2 - 1\n",
        "        model = PSO_SVM(C=C, svm_type=svm_type)\n",
        "        model.fit(X_bin, y_bin)\n",
        "        w = model.w_best\n",
        "        b = model.b_best\n",
        "        models.append(((cl1, cl2), (w, b)))\n",
        "    return models\n",
        "\n",
        "def predict_ovo(X, models):\n",
        "    votes = np.zeros((X.shape[0], len(models)))\n",
        "    classes = np.unique(np.hstack([pair for pair, _ in models]))\n",
        "    for i, ((cl1, cl2), (w, b)) in enumerate(models):\n",
        "        preds = np.sign(X.dot(w) + b)\n",
        "        votes[:, i] = np.where(preds == 1, cl1, cl2)\n",
        "    final_predictions = []\n",
        "    for j in range(votes.shape[0]):\n",
        "        valid_votes = votes[j][votes[j] >= 0]\n",
        "        if len(valid_votes) > 0:\n",
        "            bincount = np.bincount(valid_votes.astype(int))\n",
        "            final_predictions.append(bincount.argmax())\n",
        "        else:\n",
        "            final_predictions.append(np.random.choice(classes))\n",
        "    return np.array(final_predictions)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "metadata": {
        "id": "gdfPzADCtUz8"
      },
      "outputs": [],
      "source": [
        "def train_ovo(X, y, C, svm_type):\n",
        "    classes = np.unique(y)\n",
        "    models = []\n",
        "    class_pairs = list(combinations(classes, 2))\n",
        "    for (cl1, cl2) in class_pairs:\n",
        "        mask = (y == cl1) | (y == cl2)\n",
        "        X_bin = X[mask]\n",
        "        y_bin = y[mask]\n",
        "        y_bin = (y_bin == cl1).astype(int) * 2 - 1\n",
        "        model = PSO_SVM(C=C, svm_type=svm_type)\n",
        "        model.fit(X_bin, y_bin)\n",
        "        w = model.w_best\n",
        "        b = model.b_best\n",
        "        models.append(((cl1, cl2), (w, b)))\n",
        "    return models\n",
        "\n",
        "def predict_ovo(X, models):\n",
        "    votes = np.zeros((X.shape[0], len(models)))\n",
        "    classes = np.unique(np.hstack([pair for pair, _ in models]))\n",
        "    for i, ((cl1, cl2), (w, b)) in enumerate(models):\n",
        "        preds = np.sign(X.dot(w) + b)\n",
        "        votes[:, i] = np.where(preds == 1, cl1, cl2)\n",
        "    final_predictions = []\n",
        "    for j in range(votes.shape[0]):\n",
        "        valid_votes = votes[j][votes[j] >= 0]\n",
        "        if len(valid_votes) > 0:\n",
        "            bincount = np.bincount(valid_votes.astype(int))\n",
        "            final_predictions.append(bincount.argmax())\n",
        "        else:\n",
        "            final_predictions.append(np.random.choice(classes))\n",
        "    return np.array(final_predictions)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-d6Hn0gRtSOB"
      },
      "source": [
        "###ACO (Ant Colony Optimization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "1N4-N5sztVhc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from itertools import combinations\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "class ACO_SVM:\n",
        "    def __init__(self, C=1.0, num_ants=30, max_iter=100, decay=0.5, alpha=1, beta=2, svm_type='L2', c=1.0):\n",
        "        self.C = C  # 정규화 상수\n",
        "        self.num_ants = num_ants  # 개미 개수\n",
        "        self.max_iter = max_iter  # 반복 횟수\n",
        "        self.decay = decay  # 페로몬 증발 계수\n",
        "        self.alpha = alpha  # 페로몬 영향도\n",
        "        self.beta = beta  # 휴리스틱 정보 영향도\n",
        "        self.svm_type = svm_type  # L1 또는 L2\n",
        "        self.w_best = None\n",
        "        self.b_best = None\n",
        "        self.c = c\n",
        "\n",
        "    def compute_slack_term(self, xi):\n",
        "        \"\"\" SVM 타입별 슬랙 변수 비용 계산 \"\"\"\n",
        "        if self.svm_type == 'L1':\n",
        "            return np.abs(xi)\n",
        "        elif self.svm_type == 'L2':\n",
        "            return xi ** 2\n",
        "        elif self.svm_type == 'Fair':\n",
        "            return np.sum(self.c**2 * ((xi / self.c) - np.log(1 + xi / self.c)))\n",
        "        elif self.svm_type == 'Cauchy':\n",
        "            return np.sum((self.c**2 / 2) * np.log(1 + (xi / self.c)**2))\n",
        "        elif self.svm_type == 'Welsch':\n",
        "            return np.sum((self.c**2 / 2) * (1 - np.exp(-(xi / self.c)**2)))\n",
        "        elif self.svm_type == 'Geman-McClure':\n",
        "            return np.sum(((xi**2)/2) / (1 + xi**2))\n",
        "        else:\n",
        "            raise ValueError(\"Unknown SVM type. Choose from ['L1', 'L2', 'Cauchy', 'Fair', 'Welsch', 'Geman-McClure'].\")\n",
        "\n",
        "    def fitness(self, w, b, X, y):\n",
        "        \"\"\" 손실 함수 계산 (Hinge Loss 기반) \"\"\"\n",
        "        margins = y * (np.dot(X, w) + b)\n",
        "        slack = np.maximum(0, 1 - margins)  # Hinge Loss 적용\n",
        "        slack_term = self.compute_slack_term(slack)\n",
        "        regularization = 0.5 * np.dot(w, w)  # L2 Regularization\n",
        "        return regularization + self.C * np.sum(slack_term)  # 최소화할 손실 값\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # 개미들의 초기 해 (랜덤 가중치 및 바이어스)\n",
        "        w_ants = np.random.randn(self.num_ants, n_features)\n",
        "        b_ants = np.random.randn(self.num_ants)\n",
        "        pheromones = np.ones(self.num_ants)  # 초기 페로몬 값 동일\n",
        "\n",
        "        best_fitness = float('inf')\n",
        "        g_best_w, g_best_b = None, None\n",
        "\n",
        "        for _ in range(self.max_iter):\n",
        "            fitness_values = np.array([self.fitness(w, b, X, y) for w, b in zip(w_ants, b_ants)])\n",
        "\n",
        "            # 가장 좋은 개미 선택\n",
        "            best_index = np.argmin(fitness_values)\n",
        "            if fitness_values[best_index] < best_fitness:\n",
        "                best_fitness = fitness_values[best_index]\n",
        "                g_best_w, g_best_b = w_ants[best_index], b_ants[best_index]\n",
        "\n",
        "            # 페로몬 업데이트 (좋은 해에 페로몬 증가)\n",
        "            pheromones = (1 - self.decay) * pheromones  # 증발 적용\n",
        "            pheromones[best_index] += 1 / (1 + best_fitness)  # 좋은 해 강화\n",
        "\n",
        "            # 개미들의 새로운 탐색 방향 선택\n",
        "            probabilities = (pheromones ** self.alpha) * ((1 / (1 + fitness_values)) ** self.beta)\n",
        "            probabilities /= np.sum(probabilities)\n",
        "\n",
        "            selected_indices = np.random.choice(self.num_ants, size=self.num_ants, p=probabilities)\n",
        "            w_ants = w_ants[selected_indices] + np.random.randn(self.num_ants, n_features) * 0.1\n",
        "            b_ants = b_ants[selected_indices] + np.random.randn(self.num_ants) * 0.1\n",
        "\n",
        "        self.w_best = g_best_w\n",
        "        self.b_best = g_best_b\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.sign(np.dot(X, self.w_best) + self.b_best)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "_Ax8GcdPtVkk"
      },
      "outputs": [],
      "source": [
        "def train_ovo(X, y, C, svm_type):\n",
        "    classes = np.unique(y)\n",
        "    models = []\n",
        "    class_pairs = list(combinations(classes, 2))\n",
        "    for (cl1, cl2) in class_pairs:\n",
        "        mask = (y == cl1) | (y == cl2)\n",
        "        X_bin = X[mask]\n",
        "        y_bin = y[mask]\n",
        "        y_bin = (y_bin == cl1).astype(int) * 2 - 1\n",
        "        model = ACO_SVM(C=C, svm_type=svm_type)\n",
        "        model.fit(X_bin, y_bin)\n",
        "        w = model.w_best\n",
        "        b = model.b_best\n",
        "        models.append(((cl1, cl2), (w, b)))\n",
        "    return models\n",
        "\n",
        "def predict_ovo(X, models):\n",
        "    votes = np.zeros((X.shape[0], len(models)))\n",
        "    classes = np.unique(np.hstack([pair for pair, _ in models]))\n",
        "    for i, ((cl1, cl2), (w, b)) in enumerate(models):\n",
        "        preds = np.sign(X.dot(w) + b)\n",
        "        votes[:, i] = np.where(preds == 1, cl1, cl2)\n",
        "    final_predictions = []\n",
        "    for j in range(votes.shape[0]):\n",
        "        valid_votes = votes[j][votes[j] >= 0]\n",
        "        if len(valid_votes) > 0:\n",
        "            bincount = np.bincount(valid_votes.astype(int))\n",
        "            final_predictions.append(bincount.argmax())\n",
        "        else:\n",
        "            final_predictions.append(np.random.choice(classes))\n",
        "    return np.array(final_predictions)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXCVw0KOtSSg"
      },
      "source": [
        "###HS (Harmony Search)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "U5FD1H62tWe7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from itertools import combinations\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "class HS_SVM:\n",
        "    def __init__(self, C=1.0, hm_size=20, max_iter=100, hmcr=0.9, par=0.3, bw=0.1, svm_type='L2', c=1.0):\n",
        "        self.C = C  # 정규화 상수\n",
        "        self.hm_size = hm_size  # 하모니 메모리 크기\n",
        "        self.max_iter = max_iter  # 반복 횟수\n",
        "        self.hmcr = hmcr  # 하모니 메모리 고려율 (기존 해를 선택할 확률)\n",
        "        self.par = par  # 피치 조정 비율 (기존 해를 변형할 확률)\n",
        "        self.bw = bw  # 변형 크기\n",
        "        self.svm_type = svm_type  # 'L1' 또는 'L2'\n",
        "        self.w_best = None\n",
        "        self.b_best = None\n",
        "        self.c = c\n",
        "\n",
        "    def compute_slack_term(self, xi):\n",
        "        \"\"\" SVM 타입별 슬랙 변수 비용 계산 \"\"\"\n",
        "        if self.svm_type == 'L1':\n",
        "            return np.abs(xi)\n",
        "        elif self.svm_type == 'L2':\n",
        "            return xi ** 2\n",
        "        elif self.svm_type == 'Fair':\n",
        "            return np.sum(self.c**2 * ((xi / self.c) - np.log(1 + xi / self.c)))\n",
        "        elif self.svm_type == 'Cauchy':\n",
        "            return np.sum((self.c**2 / 2) * np.log(1 + (xi / self.c)**2))\n",
        "        elif self.svm_type == 'Welsch':\n",
        "            return np.sum((self.c**2 / 2) * (1 - np.exp(-(xi / self.c)**2)))\n",
        "        elif self.svm_type == 'Geman-McClure':\n",
        "            return np.sum(((xi**2)/2) / (1 + xi**2))\n",
        "        else:\n",
        "            raise ValueError(\"Unknown SVM type. Choose from ['L1', 'L2', 'Cauchy', 'Fair', 'Welsch', 'Geman-McClure'].\")\n",
        "\n",
        "    def fitness(self, w, b, X, y):\n",
        "        \"\"\" 손실 함수 계산 (Hinge Loss 기반) \"\"\"\n",
        "        margins = y * (np.dot(X, w) + b)\n",
        "        slack = np.maximum(0, 1 - margins)  # Hinge Loss 적용\n",
        "        slack_term = self.compute_slack_term(slack)\n",
        "\n",
        "        regularization = 0.5 * np.dot(w, w)  # L2 Regularization\n",
        "        return regularization + self.C * np.sum(slack_term)  # 최소화할 손실 값\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # 초기 하모니 메모리 (랜덤 가중치 및 바이어스)\n",
        "        harmony_memory = [(np.random.randn(n_features), np.random.randn()) for _ in range(self.hm_size)]\n",
        "        fitness_values = np.array([self.fitness(w, b, X, y) for w, b in harmony_memory])\n",
        "\n",
        "        for _ in range(self.max_iter):\n",
        "            # 새로운 해 생성\n",
        "            new_w, new_b = np.zeros(n_features), 0\n",
        "\n",
        "            for j in range(n_features):\n",
        "                if np.random.rand() < self.hmcr:  # 기존 해에서 선택할 확률\n",
        "                    new_w[j] = harmony_memory[np.random.randint(self.hm_size)][0][j]\n",
        "                    if np.random.rand() < self.par:  # 피치 조정\n",
        "                        new_w[j] += np.random.uniform(-self.bw, self.bw)\n",
        "                else:  # 랜덤 탐색\n",
        "                    new_w[j] = np.random.randn()\n",
        "\n",
        "            if np.random.rand() < self.hmcr:\n",
        "                new_b = harmony_memory[np.random.randint(self.hm_size)][1]\n",
        "                if np.random.rand() < self.par:\n",
        "                    new_b += np.random.uniform(-self.bw, self.bw)\n",
        "            else:\n",
        "                new_b = np.random.randn()\n",
        "\n",
        "            # 새로운 해의 적합도 평가\n",
        "            new_fitness = self.fitness(new_w, new_b, X, y)\n",
        "\n",
        "            # 기존 해 중 최악의 해와 비교 후 교체\n",
        "            worst_idx = np.argmax(fitness_values)\n",
        "            if new_fitness < fitness_values[worst_idx]:\n",
        "                harmony_memory[worst_idx] = (new_w, new_b)\n",
        "                fitness_values[worst_idx] = new_fitness\n",
        "\n",
        "        # 최적 해 선택\n",
        "        best_idx = np.argmin(fitness_values)\n",
        "        self.w_best, self.b_best = harmony_memory[best_idx]\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.sign(np.dot(X, self.w_best) + self.b_best)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "I3aET8k4tWh7"
      },
      "outputs": [],
      "source": [
        "def train_ovo(X, y, C, svm_type):\n",
        "    classes = np.unique(y)\n",
        "    models = []\n",
        "    class_pairs = list(combinations(classes, 2))\n",
        "    for (cl1, cl2) in class_pairs:\n",
        "        mask = (y == cl1) | (y == cl2)\n",
        "        X_bin = X[mask]\n",
        "        y_bin = y[mask]\n",
        "        y_bin = (y_bin == cl1).astype(int) * 2 - 1\n",
        "        model = HS_SVM(C=C, hm_size=20, max_iter=100, hmcr=0.9, par=0.3, bw=0.1, svm_type=svm_type)\n",
        "        model.fit(X_bin, y_bin)\n",
        "        w = model.w_best\n",
        "        b = model.b_best\n",
        "        models.append(((cl1, cl2), (w, b)))\n",
        "    return models\n",
        "\n",
        "def predict_ovo(X, models):\n",
        "    votes = np.zeros((X.shape[0], len(models)))\n",
        "    classes = np.unique(np.hstack([pair for pair, _ in models]))\n",
        "    for i, ((cl1, cl2), (w, b)) in enumerate(models):\n",
        "        preds = np.sign(X.dot(w) + b)\n",
        "        votes[:, i] = np.where(preds == 1, cl1, cl2)\n",
        "    final_predictions = []\n",
        "    for j in range(votes.shape[0]):\n",
        "        valid_votes = votes[j][votes[j] >= 0]\n",
        "        if len(valid_votes) > 0:\n",
        "            bincount = np.bincount(valid_votes.astype(int))\n",
        "            final_predictions.append(bincount.argmax())\n",
        "        else:\n",
        "            final_predictions.append(np.random.choice(classes))\n",
        "    return np.array(final_predictions)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-VWWHbTv9hM"
      },
      "source": [
        "#**Optimization & OvR**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqdxKtiowkJK"
      },
      "source": [
        "###L-BFGS-B (Limited-memory Broyden-Fletcher-Goldfarb-Shanno with Box constraints)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "id": "BKoEJzniwkJK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.optimize import minimize\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from itertools import combinations\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "np.random.seed(42)  # 시드 설정\n",
        "\n",
        "class LBFGS_SVM:\n",
        "    def __init__(self, C=0.01, svm_type='L2', c=1.0):\n",
        "        self.C = C  # 정규화 상수\n",
        "        self.svm_type = svm_type  # L1 또는 L2\n",
        "        self.w_best = None\n",
        "        self.b_best = None\n",
        "        self.c = c\n",
        "\n",
        "    def compute_slack_term(self, xi):\n",
        "        \"\"\" SVM 타입별 슬랙 변수 비용 계산 \"\"\"\n",
        "        if self.svm_type == 'L1':\n",
        "            return np.abs(xi)\n",
        "        elif self.svm_type == 'L2':\n",
        "            return xi ** 2\n",
        "        elif self.svm_type == 'Fair':\n",
        "            return np.sum(self.c**2 * ((xi / self.c) - np.log(1 + xi / self.c)))\n",
        "        elif self.svm_type == 'Cauchy':\n",
        "            return np.sum((self.c**2 / 2) * np.log(1 + (xi / self.c)**2))\n",
        "        elif self.svm_type == 'Welsch':\n",
        "            return np.sum((self.c**2 / 2) * (1 - np.exp(-(xi / self.c)**2)))\n",
        "        elif self.svm_type == 'Geman-McClure':\n",
        "            return np.sum(((xi**2)/2) / (1 + xi**2))\n",
        "        else:\n",
        "            raise ValueError(\"Unknown SVM type. Choose from ['L1', 'L2', 'Cauchy', 'Fair', 'Welsch', 'Geman-McClure'].\")\n",
        "\n",
        "    def fitness(self, params, X, y):\n",
        "        \"\"\" 손실 함수 계산 (Hinge Loss 기반) \"\"\"\n",
        "        n_features = X.shape[1]\n",
        "        w = params[:n_features]\n",
        "        b = params[n_features]\n",
        "        margins = y * (np.dot(X, w) + b)\n",
        "        slack = np.maximum(0, 1 - margins)  # Hinge Loss 적용\n",
        "        slack_term = self.compute_slack_term(slack)\n",
        "        regularization = 0.5 * np.dot(w, w)  # L2 Regularization\n",
        "        return regularization + self.C * np.sum(slack_term)  # 최소화할 손실 값\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        initial_params = np.zeros(n_features + 1)\n",
        "        bounds = [(None, None)] * n_features + [(None, None)]  # 경계 조건 추가\n",
        "        options = {'disp': True, 'maxiter': 5000, 'ftol': 1e-9}  # 최적화 옵션 추가: maxiter 증가, ftol 추가\n",
        "        result = minimize(self.fitness, initial_params, args=(X, y), method='L-BFGS-B', bounds=bounds, options=options)\n",
        "        # If optimization fails, try SLSQP solver\n",
        "        if not result.success:\n",
        "            result = minimize(self.fitness, initial_params, args=(X, y), method='SLSQP', bounds=bounds, options=options)\n",
        "\n",
        "        if result.success:\n",
        "            self.w_best = result.x[:n_features]\n",
        "            self.b_best = result.x[n_features]\n",
        "        else:\n",
        "            raise ValueError(\"Optimization failed: \" + result.message)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.sign(np.dot(X, self.w_best) + self.b_best)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "dboqMjDhwkJL"
      },
      "outputs": [],
      "source": [
        "# OvR training function using LBFGS_SVM\n",
        "def train_ovr(X, y, C, svm_type):\n",
        "    classes = np.unique(y)  # 모든 클래스 목록을 가져옴\n",
        "    models = []  # 학습된 모델을 저장할 리스트\n",
        "\n",
        "    for cl in classes:\n",
        "        y_bin = (y == cl).astype(int) * 2 - 1  # 현재 클래스는 +1, 나머지는 -1로 변환\n",
        "        model = LBFGS_SVM(C=C, svm_type=svm_type)  # SVM 모델 생성\n",
        "        model.fit(X, y_bin)  # 모델 학습\n",
        "\n",
        "        w = model.w_best  # 최적화된 가중치 벡터\n",
        "        b = model.b_best  # 최적화된 편향\n",
        "\n",
        "        models.append((w, b))  # 학습된 모델을 리스트에 저장\n",
        "\n",
        "    return models  # 모든 클래스별 학습된 모델 반환\n",
        "\n",
        "def predict_ovr(X, models):\n",
        "    predictions = [X.dot(w) + b for w, b in models]  # 각 클래스별 마진 계산\n",
        "    return np.argmax(predictions, axis=0)  # 가장 높은 마진 값을 가진 클래스를 예측\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYVLte-IwkJL"
      },
      "source": [
        "###1. Genetic Algorithm (GA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYBf_swpwkJL"
      },
      "outputs": [],
      "source": [
        "class GA_SVM:\n",
        "    def __init__(self, C=1.0, pop_size=20, max_iter=100, mutation_rate=0.1, crossover_rate=0.7, svm_type='L2', c=1.0):\n",
        "        self.C = C  # 정규화 상수\n",
        "        self.pop_size = pop_size  # 개체 수\n",
        "        self.max_iter = max_iter  # 세대 수\n",
        "        self.mutation_rate = mutation_rate  # 돌연변이 확률\n",
        "        self.crossover_rate = crossover_rate  # 교차 확률\n",
        "        self.svm_type = svm_type  # 'L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure'\n",
        "        self.w_best = None\n",
        "        self.b_best = None\n",
        "        self.c = c\n",
        "\n",
        "    def compute_slack_term(self, xi):\n",
        "        if self.svm_type == 'L1':\n",
        "            return np.abs(xi)\n",
        "        elif self.svm_type == 'L2':\n",
        "            return xi ** 2\n",
        "        elif self.svm_type == 'Fair':\n",
        "            return np.sum(self.c**2 * ((xi / self.c) - np.log(1 + xi / self.c)))\n",
        "        elif self.svm_type == 'Cauchy':\n",
        "            return np.sum((self.c**2 / 2) * np.log(1 + (xi / self.c)**2))\n",
        "        elif self.svm_type == 'Welsch':\n",
        "            return np.sum((self.c**2 / 2) * (1 - np.exp(-(xi / self.c)**2)))\n",
        "        elif self.svm_type == 'Geman-McClure':\n",
        "            return np.sum(((xi**2)/2) / (1 + xi**2))\n",
        "        else:\n",
        "            raise ValueError(\"Unknown SVM type. Choose from ['L1', 'L2', 'Cauchy', 'Fair', 'Welsch', 'Geman-McClure'].\")\n",
        "\n",
        "    def fitness(self, w, b, X, y):\n",
        "        margins = y * (np.dot(X, w) + b)\n",
        "        slack = np.maximum(0, 1 - margins)\n",
        "        slack_term = self.compute_slack_term(slack)\n",
        "        regularization = 0.5 * np.dot(w, w)\n",
        "        return regularization + self.C * np.sum(slack_term)\n",
        "\n",
        "    def initialize_population(self, n_features):\n",
        "        population = [(np.random.randn(n_features), np.random.randn()) for _ in range(self.pop_size)]\n",
        "        return population\n",
        "\n",
        "    def selection(self, population, fitness_values):\n",
        "        probabilities = 1 / (fitness_values + 1e-6)\n",
        "        probabilities /= probabilities.sum()\n",
        "        selected_indices = np.random.choice(len(population), size=len(population), p=probabilities)\n",
        "        return [population[i] for i in selected_indices]\n",
        "\n",
        "    def crossover(self, parent1, parent2):\n",
        "        w1, b1 = parent1\n",
        "        w2, b2 = parent2\n",
        "        if np.random.rand() < self.crossover_rate:\n",
        "            point = np.random.randint(len(w1))\n",
        "            new_w = np.concatenate((w1[:point], w2[point:]))\n",
        "            new_b = (b1 + b2) / 2\n",
        "        else:\n",
        "            new_w, new_b = w1.copy(), b1\n",
        "        return new_w, new_b\n",
        "\n",
        "    def mutation(self, w, b):\n",
        "        if np.random.rand() < self.mutation_rate:\n",
        "            mutation_vector = np.random.randn(*w.shape) * 0.1\n",
        "            w += mutation_vector\n",
        "            b += np.random.randn() * 0.1\n",
        "        return w, b\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        population = self.initialize_population(n_features)\n",
        "        fitness_values = np.array([self.fitness(w, b, X, y) for w, b in population])\n",
        "        np.random.seed(42)  # 시드 설정\n",
        "        for _ in range(self.max_iter):\n",
        "            selected_population = self.selection(population, fitness_values)\n",
        "            new_population = []\n",
        "            for i in range(0, len(selected_population), 2):\n",
        "                p1, p2 = selected_population[i], selected_population[(i + 1) % len(selected_population)]\n",
        "                offspring1 = self.crossover(p1, p2)\n",
        "                offspring2 = self.crossover(p2, p1)\n",
        "                new_population.append(self.mutation(*offspring1))\n",
        "                new_population.append(self.mutation(*offspring2))\n",
        "            population = new_population\n",
        "            fitness_values = np.array([self.fitness(w, b, X, y) for w, b in population])\n",
        "        best_idx = np.argmin(fitness_values)\n",
        "        self.w_best, self.b_best = population[best_idx]\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.sign(np.dot(X, self.w_best) + self.b_best)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4xDnM5d2wkJL"
      },
      "outputs": [],
      "source": [
        "# OvR training function\n",
        "def train_ovr(X, y, C, svm_type):\n",
        "    classes = np.unique(y)  # 모든 클래스 목록을 가져옴\n",
        "    models = []  # 학습된 모델을 저장할 리스트\n",
        "\n",
        "    for cl in classes:\n",
        "        y_bin = (y == cl).astype(int) * 2 - 1  # 현재 클래스는 +1, 나머지는 -1로 변환\n",
        "        model = GA_SVM(C=C, svm_type=svm_type)  # SVM 모델 생성\n",
        "        model.fit(X, y_bin)  # 모델 학습\n",
        "\n",
        "        w = model.w_best  # 최적화된 가중치 벡터\n",
        "        b = model.b_best  # 최적화된 편향\n",
        "\n",
        "        models.append((w, b))  # 학습된 모델을 리스트에 저장\n",
        "\n",
        "    return models  # 모든 클래스별 학습된 모델 반환\n",
        "\n",
        "def predict_ovr(X, models):\n",
        "    predictions = [X.dot(w) + b for w, b in models]  # 각 클래스별 마진 계산\n",
        "    return np.argmax(predictions, axis=0)  # 가장 높은 마진 값을 가진 클래스를 예측\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dvc7x7owkJL"
      },
      "source": [
        "###SMO (Sequential Minial Optimization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "v909xZA5wkJL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "from itertools import combinations\n",
        "\n",
        "class SMO_SVM:\n",
        "    def __init__(self, C=1.0, kernel='linear', tol=1e-3, max_iter=1000, svm_type='L2', c=1.0, lr=0.01):\n",
        "        self.C = C\n",
        "        self.kernel = kernel\n",
        "        self.tol = tol\n",
        "        self.max_iter = max_iter\n",
        "        self.svm_type = svm_type\n",
        "        self.c = c\n",
        "        self.lr = lr\n",
        "        self.w_best = None  # 최적 가중치 벡터 추가\n",
        "        self.b_best = None  # 최적 편향 값 추가\n",
        "\n",
        "    def kernel_function(self, X, Y):\n",
        "        if self.kernel == 'linear':\n",
        "            return np.dot(X, Y.T)\n",
        "        else:\n",
        "            raise ValueError(\"Only linear kernel is supported in this implementation.\")\n",
        "\n",
        "    def compute_slack_term(self, xi):\n",
        "        xi = np.maximum(xi, 1e-6)\n",
        "\n",
        "        if self.svm_type == 'Fair':\n",
        "            return np.sum(self.c**2 * ((xi / self.c) - np.log(1 + xi / self.c)))\n",
        "        elif self.svm_type == 'Cauchy':\n",
        "            return np.sum((self.c**2 / 2) * np.log(1 + (xi / self.c)**2))\n",
        "        elif self.svm_type == 'Welsch':\n",
        "            return np.sum((self.c**2 / 2) * (1 - np.exp(-(xi / self.c)**2)))\n",
        "        elif self.svm_type == 'Geman-McClure':\n",
        "            return np.sum(((xi**2)/2) / (1 + xi**2))\n",
        "        elif self.svm_type == 'L1':\n",
        "            return np.abs(xi)\n",
        "        elif self.svm_type == 'L2':\n",
        "            return xi ** 2\n",
        "        else:\n",
        "            raise ValueError(\"Unknown SVM type. Choose from ['L1', 'L2', 'Cauchy', 'Fair', 'Welsch', 'Geman-McClure'].\")\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        self.alpha = np.zeros(n_samples)\n",
        "        self.b = 0\n",
        "        self.w = np.zeros(n_features)\n",
        "\n",
        "        for _ in range(self.max_iter):\n",
        "            for i in range(n_samples):\n",
        "                xi, yi = X[i], y[i]\n",
        "                margin = yi * (np.dot(self.w, xi) + self.b)\n",
        "                slack = max(0, 1 - margin)\n",
        "                slack_term = self.compute_slack_term(slack)\n",
        "\n",
        "                if slack > 0:\n",
        "                    delta_alpha = self.C * (1 - margin) - slack_term\n",
        "                    self.alpha[i] = np.clip(self.alpha[i] + self.lr * delta_alpha, 0, self.C)\n",
        "                    self.w += self.lr * self.alpha[i] * yi * xi\n",
        "                    self.b += self.lr * self.alpha[i] * yi\n",
        "\n",
        "        self.w_best = self.w  # 최적의 가중치 저장\n",
        "        self.b_best = self.b  # 최적의 편향 값 저장\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.sign(np.dot(X, self.w_best) + self.b_best)  # b 대신 b_best 사용\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "IncXMFZcwkJL"
      },
      "outputs": [],
      "source": [
        "# OvR training function using LBFGS_SVM\n",
        "def train_ovr(X, y, C, svm_type):\n",
        "    classes = np.unique(y)  # 모든 클래스 목록을 가져옴\n",
        "    models = []  # 학습된 모델을 저장할 리스트\n",
        "\n",
        "    for cl in classes:\n",
        "        y_bin = (y == cl).astype(int) * 2 - 1  # 현재 클래스는 +1, 나머지는 -1로 변환\n",
        "        model = SMO_SVM(C=C, svm_type=svm_type)  # SVM 모델 생성\n",
        "        model.fit(X, y_bin)  # 모델 학습\n",
        "\n",
        "        w = model.w_best  # 최적화된 가중치 벡터\n",
        "        b = model.b_best  # 최적화된 편향\n",
        "\n",
        "        models.append((w, b))  # 학습된 모델을 리스트에 저장\n",
        "\n",
        "    return models  # 모든 클래스별 학습된 모델 반환\n",
        "\n",
        "def predict_ovr(X, models):\n",
        "    predictions = [X.dot(w) + b for w, b in models]  # 각 클래스별 마진 계산\n",
        "    return np.argmax(predictions, axis=0)  # 가장 높은 마진 값을 가진 클래스를 예측\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70WCb2PFwkJL"
      },
      "source": [
        "###PSO (Particle Swarm Optimization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "qn52YMR8wkJL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from itertools import combinations\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "class PSO_SVM:\n",
        "    def __init__(self, C=1.0, num_particles=30, max_iter=100, svm_type='L2', c=1.0):\n",
        "        self.C = C  # 정규화 상수\n",
        "        self.num_particles = num_particles  # PSO 입자 개수\n",
        "        self.max_iter = max_iter  # PSO 반복 횟수\n",
        "        self.svm_type = svm_type  # L1 또는 L2\n",
        "        self.w_best = None\n",
        "        self.b_best = None\n",
        "        self.c = c\n",
        "\n",
        "    def compute_slack_term(self, xi):\n",
        "        \"\"\" SVM 타입별 슬랙 변수 비용 계산 \"\"\"\n",
        "        if self.svm_type == 'L1':\n",
        "            return np.abs(xi)\n",
        "        elif self.svm_type == 'L2':\n",
        "            return xi ** 2\n",
        "        elif self.svm_type == 'Fair':\n",
        "            return np.sum(self.c**2 * ((xi / self.c) - np.log(1 + xi / self.c)))\n",
        "        elif self.svm_type == 'Cauchy':\n",
        "            return np.sum((self.c**2 / 2) * np.log(1 + (xi / self.c)**2))\n",
        "        elif self.svm_type == 'Welsch':\n",
        "            return np.sum((self.c**2 / 2) * (1 - np.exp(-(xi / self.c)**2)))\n",
        "        elif self.svm_type == 'Geman-McClure':\n",
        "            return np.sum(((xi**2)/2) / (1 + xi**2))\n",
        "        else:\n",
        "            raise ValueError(\"Unknown SVM type. Choose from ['L1', 'L2', 'Cauchy', 'Fair', 'Welsch', 'Geman-McClure'].\")\n",
        "\n",
        "    def fitness(self, w, b, X, y):\n",
        "        \"\"\" 손실 함수 계산 (Hinge Loss 기반) \"\"\"\n",
        "        margins = y * (np.dot(X, w) + b)\n",
        "        slack = np.maximum(0, 1 - margins)  # Hinge Loss 적용\n",
        "        slack_term = self.compute_slack_term(slack)\n",
        "        regularization = 0.5 * np.dot(w, w)  # L2 Regularization\n",
        "        return regularization + self.C * np.sum(slack_term)  # 최소화할 손실 값\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # PSO 초기화\n",
        "        w_particles = np.random.randn(self.num_particles, n_features)  # 초기 w 값\n",
        "        b_particles = np.random.randn(self.num_particles)  # 초기 b 값\n",
        "        velocities_w = np.random.randn(self.num_particles, n_features) * 0.1  # 속도 초기화\n",
        "        velocities_b = np.random.randn(self.num_particles) * 0.1\n",
        "\n",
        "        # 개별 최적 및 전역 최적 초기화\n",
        "        p_best_w = np.copy(w_particles)\n",
        "        p_best_b = np.copy(b_particles)\n",
        "        p_best_scores = np.array([self.fitness(w, b, X, y) for w, b in zip(w_particles, b_particles)])\n",
        "\n",
        "        g_best_index = np.argmin(p_best_scores)\n",
        "        g_best_w = p_best_w[g_best_index]\n",
        "        g_best_b = p_best_b[g_best_index]\n",
        "\n",
        "        # PSO 학습 진행\n",
        "        w_inertia = 0.7  # 관성 계수\n",
        "        c1 = 1.5  # 개인 최적화 계수\n",
        "        c2 = 1.5  # 글로벌 최적화 계수\n",
        "\n",
        "        for _ in range(self.max_iter):\n",
        "            for i in range(self.num_particles):\n",
        "                r1, r2 = np.random.rand(), np.random.rand()\n",
        "                velocities_w[i] = (w_inertia * velocities_w[i] +\n",
        "                                   c1 * r1 * (p_best_w[i] - w_particles[i]) +\n",
        "                                   c2 * r2 * (g_best_w - w_particles[i]))\n",
        "                velocities_b[i] = (w_inertia * velocities_b[i] +\n",
        "                                   c1 * r1 * (p_best_b[i] - b_particles[i]) +\n",
        "                                   c2 * r2 * (g_best_b - b_particles[i]))\n",
        "\n",
        "                # 업데이트된 위치\n",
        "                w_particles[i] += velocities_w[i]\n",
        "                b_particles[i] += velocities_b[i]\n",
        "\n",
        "                # 새로운 피트니스 값 계산\n",
        "                new_fitness = self.fitness(w_particles[i], b_particles[i], X, y)\n",
        "\n",
        "                # 최적값 갱신\n",
        "                if new_fitness < p_best_scores[i]:\n",
        "                    p_best_w[i] = w_particles[i]\n",
        "                    p_best_b[i] = b_particles[i]\n",
        "                    p_best_scores[i] = new_fitness\n",
        "\n",
        "            # 전체 최적 갱신\n",
        "            g_best_index = np.argmin(p_best_scores)\n",
        "            g_best_w = p_best_w[g_best_index]\n",
        "            g_best_b = p_best_b[g_best_index]\n",
        "\n",
        "        self.w_best = g_best_w\n",
        "        self.b_best = g_best_b\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.sign(np.dot(X, self.w_best) + self.b_best)\n",
        "def train_ovo(X, y, C, svm_type):\n",
        "    classes = np.unique(y)\n",
        "    models = []\n",
        "    class_pairs = list(combinations(classes, 2))\n",
        "    for (cl1, cl2) in class_pairs:\n",
        "        mask = (y == cl1) | (y == cl2)\n",
        "        X_bin = X[mask]\n",
        "        y_bin = y[mask]\n",
        "        y_bin = (y_bin == cl1).astype(int) * 2 - 1\n",
        "        model = PSO_SVM(C=C, svm_type=svm_type)\n",
        "        model.fit(X_bin, y_bin)\n",
        "        w = model.w_best\n",
        "        b = model.b_best\n",
        "        models.append(((cl1, cl2), (w, b)))\n",
        "    return models\n",
        "\n",
        "def predict_ovo(X, models):\n",
        "    votes = np.zeros((X.shape[0], len(models)))\n",
        "    classes = np.unique(np.hstack([pair for pair, _ in models]))\n",
        "    for i, ((cl1, cl2), (w, b)) in enumerate(models):\n",
        "        preds = np.sign(X.dot(w) + b)\n",
        "        votes[:, i] = np.where(preds == 1, cl1, cl2)\n",
        "    final_predictions = []\n",
        "    for j in range(votes.shape[0]):\n",
        "        valid_votes = votes[j][votes[j] >= 0]\n",
        "        if len(valid_votes) > 0:\n",
        "            bincount = np.bincount(valid_votes.astype(int))\n",
        "            final_predictions.append(bincount.argmax())\n",
        "        else:\n",
        "            final_predictions.append(np.random.choice(classes))\n",
        "    return np.array(final_predictions)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "id": "jDTWEliuwkJM"
      },
      "outputs": [],
      "source": [
        "# OvR training function\n",
        "def train_ovr(X, y, C, svm_type):\n",
        "    classes = np.unique(y)  # 모든 클래스 목록을 가져옴\n",
        "    models = []  # 학습된 모델을 저장할 리스트\n",
        "\n",
        "    for cl in classes:\n",
        "        y_bin = (y == cl).astype(int) * 2 - 1  # 현재 클래스는 +1, 나머지는 -1로 변환\n",
        "        model = PSO_SVM(C=C, svm_type=svm_type)  # SVM 모델 생성\n",
        "        model.fit(X, y_bin)  # 모델 학습\n",
        "\n",
        "        w = model.w_best  # 최적화된 가중치 벡터\n",
        "        b = model.b_best  # 최적화된 편향\n",
        "\n",
        "        models.append((w, b))  # 학습된 모델을 리스트에 저장\n",
        "\n",
        "    return models  # 모든 클래스별 학습된 모델 반환\n",
        "\n",
        "def predict_ovr(X, models):\n",
        "    predictions = [X.dot(w) + b for w, b in models]  # 각 클래스별 마진 계산\n",
        "    return np.argmax(predictions, axis=0)  # 가장 높은 마진 값을 가진 클래스를 예측\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KnIM7dGwkJM"
      },
      "source": [
        "###ACO (Ant Colony Optimization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "id": "zE_YlB1WwkJM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from itertools import combinations\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "class ACO_SVM:\n",
        "    def __init__(self, C=1.0, num_ants=30, max_iter=100, decay=0.5, alpha=1, beta=2, svm_type='L2', c=1.0):\n",
        "        self.C = C  # 정규화 상수\n",
        "        self.num_ants = num_ants  # 개미 개수\n",
        "        self.max_iter = max_iter  # 반복 횟수\n",
        "        self.decay = decay  # 페로몬 증발 계수\n",
        "        self.alpha = alpha  # 페로몬 영향도\n",
        "        self.beta = beta  # 휴리스틱 정보 영향도\n",
        "        self.svm_type = svm_type  # L1 또는 L2\n",
        "        self.w_best = None\n",
        "        self.b_best = None\n",
        "        self.c = c\n",
        "\n",
        "    def compute_slack_term(self, xi):\n",
        "        \"\"\" SVM 타입별 슬랙 변수 비용 계산 \"\"\"\n",
        "        if self.svm_type == 'L1':\n",
        "            return np.abs(xi)\n",
        "        elif self.svm_type == 'L2':\n",
        "            return xi ** 2\n",
        "        elif self.svm_type == 'Fair':\n",
        "            return np.sum(self.c**2 * ((xi / self.c) - np.log(1 + xi / self.c)))\n",
        "        elif self.svm_type == 'Cauchy':\n",
        "            return np.sum((self.c**2 / 2) * np.log(1 + (xi / self.c)**2))\n",
        "        elif self.svm_type == 'Welsch':\n",
        "            return np.sum((self.c**2 / 2) * (1 - np.exp(-(xi / self.c)**2)))\n",
        "        elif self.svm_type == 'Geman-McClure':\n",
        "            return np.sum(((xi**2)/2) / (1 + xi**2))\n",
        "        else:\n",
        "            raise ValueError(\"Unknown SVM type. Choose from ['L1', 'L2', 'Cauchy', 'Fair', 'Welsch', 'Geman-McClure'].\")\n",
        "\n",
        "    def fitness(self, w, b, X, y):\n",
        "        \"\"\" 손실 함수 계산 (Hinge Loss 기반) \"\"\"\n",
        "        margins = y * (np.dot(X, w) + b)\n",
        "        slack = np.maximum(0, 1 - margins)  # Hinge Loss 적용\n",
        "        slack_term = self.compute_slack_term(slack)\n",
        "        regularization = 0.5 * np.dot(w, w)  # L2 Regularization\n",
        "        return regularization + self.C * np.sum(slack_term)  # 최소화할 손실 값\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # 개미들의 초기 해 (랜덤 가중치 및 바이어스)\n",
        "        w_ants = np.random.randn(self.num_ants, n_features)\n",
        "        b_ants = np.random.randn(self.num_ants)\n",
        "        pheromones = np.ones(self.num_ants)  # 초기 페로몬 값 동일\n",
        "\n",
        "        best_fitness = float('inf')\n",
        "        g_best_w, g_best_b = None, None\n",
        "\n",
        "        for _ in range(self.max_iter):\n",
        "            fitness_values = np.array([self.fitness(w, b, X, y) for w, b in zip(w_ants, b_ants)])\n",
        "\n",
        "            # 가장 좋은 개미 선택\n",
        "            best_index = np.argmin(fitness_values)\n",
        "            if fitness_values[best_index] < best_fitness:\n",
        "                best_fitness = fitness_values[best_index]\n",
        "                g_best_w, g_best_b = w_ants[best_index], b_ants[best_index]\n",
        "\n",
        "            # 페로몬 업데이트 (좋은 해에 페로몬 증가)\n",
        "            pheromones = (1 - self.decay) * pheromones  # 증발 적용\n",
        "            pheromones[best_index] += 1 / (1 + best_fitness)  # 좋은 해 강화\n",
        "\n",
        "            # 개미들의 새로운 탐색 방향 선택\n",
        "            probabilities = (pheromones ** self.alpha) * ((1 / (1 + fitness_values)) ** self.beta)\n",
        "            probabilities /= np.sum(probabilities)\n",
        "\n",
        "            selected_indices = np.random.choice(self.num_ants, size=self.num_ants, p=probabilities)\n",
        "            w_ants = w_ants[selected_indices] + np.random.randn(self.num_ants, n_features) * 0.1\n",
        "            b_ants = b_ants[selected_indices] + np.random.randn(self.num_ants) * 0.1\n",
        "\n",
        "        self.w_best = g_best_w\n",
        "        self.b_best = g_best_b\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.sign(np.dot(X, self.w_best) + self.b_best)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {
        "id": "KIvGptOkwkJM"
      },
      "outputs": [],
      "source": [
        "# OvR training function\n",
        "def train_ovr(X, y, C, svm_type):\n",
        "    classes = np.unique(y)  # 모든 클래스 목록을 가져옴\n",
        "    models = []  # 학습된 모델을 저장할 리스트\n",
        "\n",
        "    for cl in classes:\n",
        "        y_bin = (y == cl).astype(int) * 2 - 1  # 현재 클래스는 +1, 나머지는 -1로 변환\n",
        "        model = ACO_SVM(C=C, svm_type=svm_type)  # SVM 모델 생성\n",
        "        model.fit(X, y_bin)  # 모델 학습\n",
        "\n",
        "        w = model.w_best  # 최적화된 가중치 벡터\n",
        "        b = model.b_best  # 최적화된 편향\n",
        "\n",
        "        models.append((w, b))  # 학습된 모델을 리스트에 저장\n",
        "\n",
        "    return models  # 모든 클래스별 학습된 모델 반환\n",
        "\n",
        "def predict_ovr(X, models):\n",
        "    predictions = [X.dot(w) + b for w, b in models]  # 각 클래스별 마진 계산\n",
        "    return np.argmax(predictions, axis=0)  # 가장 높은 마진 값을 가진 클래스를 예측\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpVjPY_IwkJM"
      },
      "source": [
        "###HS (Harmony Search)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 191,
      "metadata": {
        "id": "Seoc-s5RwkJM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from itertools import combinations\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "class HS_SVM:\n",
        "    def __init__(self, C=1.0, hm_size=20, max_iter=100, hmcr=0.9, par=0.3, bw=0.1, svm_type='L2', c=1.0):\n",
        "        self.C = C  # 정규화 상수\n",
        "        self.hm_size = hm_size  # 하모니 메모리 크기\n",
        "        self.max_iter = max_iter  # 반복 횟수\n",
        "        self.hmcr = hmcr  # 하모니 메모리 고려율 (기존 해를 선택할 확률)\n",
        "        self.par = par  # 피치 조정 비율 (기존 해를 변형할 확률)\n",
        "        self.bw = bw  # 변형 크기\n",
        "        self.svm_type = svm_type  # 'L1' 또는 'L2'\n",
        "        self.w_best = None\n",
        "        self.b_best = None\n",
        "        self.c = c\n",
        "\n",
        "    def compute_slack_term(self, xi):\n",
        "        \"\"\" SVM 타입별 슬랙 변수 비용 계산 \"\"\"\n",
        "        if self.svm_type == 'L1':\n",
        "            return np.abs(xi)\n",
        "        elif self.svm_type == 'L2':\n",
        "            return xi ** 2\n",
        "        elif self.svm_type == 'Fair':\n",
        "            return np.sum(self.c**2 * ((xi / self.c) - np.log(1 + xi / self.c)))\n",
        "        elif self.svm_type == 'Cauchy':\n",
        "            return np.sum((self.c**2 / 2) * np.log(1 + (xi / self.c)**2))\n",
        "        elif self.svm_type == 'Welsch':\n",
        "            return np.sum((self.c**2 / 2) * (1 - np.exp(-(xi / self.c)**2)))\n",
        "        elif self.svm_type == 'Geman-McClure':\n",
        "            return np.sum(((xi**2)/2) / (1 + xi**2))\n",
        "        else:\n",
        "            raise ValueError(\"Unknown SVM type. Choose from ['L1', 'L2', 'Cauchy', 'Fair', 'Welsch', 'Geman-McClure'].\")\n",
        "\n",
        "    def fitness(self, w, b, X, y):\n",
        "        \"\"\" 손실 함수 계산 (Hinge Loss 기반) \"\"\"\n",
        "        margins = y * (np.dot(X, w) + b)\n",
        "        slack = np.maximum(0, 1 - margins)  # Hinge Loss 적용\n",
        "        slack_term = self.compute_slack_term(slack)\n",
        "\n",
        "        regularization = 0.5 * np.dot(w, w)  # L2 Regularization\n",
        "        return regularization + self.C * np.sum(slack_term)  # 최소화할 손실 값\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # 초기 하모니 메모리 (랜덤 가중치 및 바이어스)\n",
        "        harmony_memory = [(np.random.randn(n_features), np.random.randn()) for _ in range(self.hm_size)]\n",
        "        fitness_values = np.array([self.fitness(w, b, X, y) for w, b in harmony_memory])\n",
        "\n",
        "        for _ in range(self.max_iter):\n",
        "            # 새로운 해 생성\n",
        "            new_w, new_b = np.zeros(n_features), 0\n",
        "\n",
        "            for j in range(n_features):\n",
        "                if np.random.rand() < self.hmcr:  # 기존 해에서 선택할 확률\n",
        "                    new_w[j] = harmony_memory[np.random.randint(self.hm_size)][0][j]\n",
        "                    if np.random.rand() < self.par:  # 피치 조정\n",
        "                        new_w[j] += np.random.uniform(-self.bw, self.bw)\n",
        "                else:  # 랜덤 탐색\n",
        "                    new_w[j] = np.random.randn()\n",
        "\n",
        "            if np.random.rand() < self.hmcr:\n",
        "                new_b = harmony_memory[np.random.randint(self.hm_size)][1]\n",
        "                if np.random.rand() < self.par:\n",
        "                    new_b += np.random.uniform(-self.bw, self.bw)\n",
        "            else:\n",
        "                new_b = np.random.randn()\n",
        "\n",
        "            # 새로운 해의 적합도 평가\n",
        "            new_fitness = self.fitness(new_w, new_b, X, y)\n",
        "\n",
        "            # 기존 해 중 최악의 해와 비교 후 교체\n",
        "            worst_idx = np.argmax(fitness_values)\n",
        "            if new_fitness < fitness_values[worst_idx]:\n",
        "                harmony_memory[worst_idx] = (new_w, new_b)\n",
        "                fitness_values[worst_idx] = new_fitness\n",
        "\n",
        "        # 최적 해 선택\n",
        "        best_idx = np.argmin(fitness_values)\n",
        "        self.w_best, self.b_best = harmony_memory[best_idx]\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.sign(np.dot(X, self.w_best) + self.b_best)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "metadata": {
        "id": "bFD60ucuwkJM"
      },
      "outputs": [],
      "source": [
        "# OvR training function\n",
        "def train_ovr(X, y, C, svm_type):\n",
        "    classes = np.unique(y)  # 모든 클래스 목록을 가져옴\n",
        "    models = []  # 학습된 모델을 저장할 리스트\n",
        "\n",
        "    for cl in classes:\n",
        "        y_bin = (y == cl).astype(int) * 2 - 1  # 현재 클래스는 +1, 나머지는 -1로 변환\n",
        "        model = HS_SVM(C=C, svm_type=svm_type)  # SVM 모델 생성\n",
        "        model.fit(X, y_bin)  # 모델 학습\n",
        "\n",
        "        w = model.w_best  # 최적화된 가중치 벡터\n",
        "        b = model.b_best  # 최적화된 편향\n",
        "\n",
        "        models.append((w, b))  # 학습된 모델을 리스트에 저장\n",
        "\n",
        "    return models  # 모든 클래스별 학습된 모델 반환\n",
        "\n",
        "def predict_ovr(X, models):\n",
        "    predictions = [X.dot(w) + b for w, b in models]  # 각 클래스별 마진 계산\n",
        "    return np.argmax(predictions, axis=0)  # 가장 높은 마진 값을 가진 클래스를 예측\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8r4n9wcG5AH"
      },
      "source": [
        "#**Optimization & Direct**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AbWid8vG5AI"
      },
      "source": [
        "###L-BFGS-B (Limited-memory Broyden-Fletcher-Goldfarb-Shanno with Box constraints)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r9W8szaZ542r"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.optimize import minimize\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from itertools import combinations\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "np.random.seed(42)  # 시드 설정\n",
        "\n",
        "class LBFGS_SVM:\n",
        "    def __init__(self, C=0.01, svm_type='L2', c=1.0, num_classes=3):\n",
        "        self.C = C  # 정규화 상수\n",
        "        self.svm_type = svm_type  # L1 또는 L2\n",
        "        self.num_classes = num_classes  # 다중 클래스 수\n",
        "        self.W_best = None\n",
        "        self.b_best = None\n",
        "        self.c = c\n",
        "\n",
        "    def compute_slack_term(self, xi):\n",
        "        \"\"\" SVM 타입별 슬랙 변수 비용 계산 \"\"\"\n",
        "        if self.svm_type == 'L1':\n",
        "            return np.abs(xi)\n",
        "        elif self.svm_type == 'L2':\n",
        "            return xi ** 2\n",
        "        elif self.svm_type == 'Fair':\n",
        "            return np.sum(self.c**2 * ((xi / self.c) - np.log(1 + xi / self.c)))\n",
        "        elif self.svm_type == 'Cauchy':\n",
        "            return np.sum((self.c**2 / 2) * np.log(1 + (xi / self.c)**2))\n",
        "        elif self.svm_type == 'Welsch':\n",
        "            return np.sum((self.c**2 / 2) * (1 - np.exp(-(xi / self.c)**2)))\n",
        "        elif self.svm_type == 'Geman-McClure':\n",
        "            return np.sum(((xi**2)/2) / (1 + xi**2))\n",
        "        else:\n",
        "            raise ValueError(\"Unknown SVM type. Choose from ['L1', 'L2', 'Cauchy', 'Fair', 'Welsch', 'Geman-McClure'].\")\n",
        "\n",
        "    def fitness(self, params, X, y):\n",
        "        \"\"\" 다중 클래스 SVM 손실 함수 \"\"\"\n",
        "        n_features = X.shape[1]\n",
        "        W = params[:-self.num_classes].reshape((self.num_classes, n_features))\n",
        "        b = params[-self.num_classes:]\n",
        "\n",
        "        scores = X.dot(W.T) + b\n",
        "        margins = np.maximum(0, 1 - (scores[np.arange(X.shape[0]), y] - scores.T).T)\n",
        "        margins[np.arange(X.shape[0]), y] = 0  # 정답 클래스의 손실 제거\n",
        "\n",
        "        slack_term = self.compute_slack_term(margins)\n",
        "        regularization = np.sum(W**2) / 2  # L2 Regularization\n",
        "        return regularization + self.C * np.sum(slack_term)  # 최소화할 손실 값\n",
        "\n",
        "    def fitness(self, params, X, y):\n",
        "        \"\"\" 손실 함수 계산 (Hinge Loss 기반) \"\"\"\n",
        "        n_features = X.shape[1]\n",
        "        w = params[:n_features]\n",
        "        b = params[n_features]\n",
        "        margins = y * (np.dot(X, w) + b)\n",
        "        slack = np.maximum(0, 1 - margins)  # Hinge Loss 적용\n",
        "        slack_term = self.compute_slack_term(slack)\n",
        "        regularization = 0.5 * np.dot(w, w)  # L2 Regularization\n",
        "        return regularization + self.C * np.sum(slack_term)  # 최소화할 손실 값\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        initial_params = np.zeros(n_features + 1)\n",
        "        bounds = [(None, None)] * n_features + [(None, None)]  # 경계 조건 추가\n",
        "        options = {'disp': True, 'maxiter': 5000, 'ftol': 1e-9}  # 최적화 옵션 추가: maxiter 증가, ftol 추가\n",
        "        result = minimize(self.fitness, initial_params, args=(X, y), method='L-BFGS-B', bounds=bounds, options=options)\n",
        "        # If optimization fails, try SLSQP solver\n",
        "        if not result.success:\n",
        "            result = minimize(self.fitness, initial_params, args=(X, y), method='SLSQP', bounds=bounds, options=options)\n",
        "\n",
        "        if result.success:\n",
        "            self.w_best = result.x[:n_features]\n",
        "            self.b_best = result.x[n_features]\n",
        "        else:\n",
        "            raise ValueError(\"Optimization failed: \" + result.message)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        initial_params = np.zeros((self.num_classes * n_features) + self.num_classes)\n",
        "        bounds = [(None, None)] * len(initial_params)\n",
        "        options = {'disp': True, 'maxiter': 5000, 'ftol': 1e-9}  # 최적화 옵션 추가\n",
        "        result = minimize(self.fitness, initial_params, args=(X, y), method='L-BFGS-B', bounds=bounds, options=options)\n",
        "\n",
        "        if result.success:\n",
        "            self.W_best = result.x[:-self.num_classes].reshape((self.num_classes, n_features))\n",
        "            self.b_best = result.x[-self.num_classes:]\n",
        "        else:\n",
        "            raise ValueError(\"Optimization failed: \" + result.message)\n",
        "\n",
        "    def predict(self, X):\n",
        "        scores = X.dot(self.W_best.T) + self.b_best\n",
        "        return np.argmax(scores, axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sVpl6ACIG5AJ"
      },
      "outputs": [],
      "source": [
        "# Direct SVM multiclass training function\n",
        "def train_direct(X, y, C, num_classes, svm_type):\n",
        "    model = LBFGS_SVM(C=C, svm_type=svm_type, num_classes=num_classes)\n",
        "    model.fit(X, y)\n",
        "    return model.W_best, model.b_best  # 모델의 가중치와 바이어스를 반환\n",
        "\n",
        "# Multiclass prediction function\n",
        "def predict_direct(X, weights, biases):\n",
        "    scores = X.dot(weights.T) + biases\n",
        "    return np.argmax(scores, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTyXn81TG5AK"
      },
      "source": [
        "###1. Genetic Algorithm (GA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {
        "id": "XzMm6avNG5AK"
      },
      "outputs": [],
      "source": [
        "class GA_SVM:\n",
        "  def __init__(self, C=1.0, pop_size=20, max_iter=100, mutation_rate=0.1, crossover_rate=0.7, svm_type='L2', c=1.0, num_classes=3):\n",
        "      self.C = C  # 정규화 상수\n",
        "      self.pop_size = pop_size  # 개체 수\n",
        "      self.max_iter = max_iter  # 세대 수\n",
        "      self.mutation_rate = mutation_rate  # 돌연변이 확률\n",
        "      self.crossover_rate = crossover_rate  # 교차 확률\n",
        "      self.svm_type = svm_type  # 'L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure'\n",
        "      self.W_best = None\n",
        "      self.b_best = None\n",
        "      self.c = c\n",
        "      self.num_classes = num_classes\n",
        "\n",
        "  def compute_slack_term(self, xi):\n",
        "      if self.svm_type == 'L1':\n",
        "          return np.abs(xi)\n",
        "      elif self.svm_type == 'L2':\n",
        "          return xi ** 2\n",
        "      elif self.svm_type == 'Fair':\n",
        "          return np.sum(self.c**2 * ((xi / self.c) - np.log(1 + xi / self.c)))\n",
        "      elif self.svm_type == 'Cauchy':\n",
        "          return np.sum((self.c**2 / 2) * np.log(1 + (xi / self.c)**2))\n",
        "      elif self.svm_type == 'Welsch':\n",
        "          return np.sum((self.c**2 / 2) * (1 - np.exp(-(xi / self.c)**2)))\n",
        "      elif self.svm_type == 'Geman-McClure':\n",
        "          return np.sum(((xi**2)/2) / (1 + xi**2))\n",
        "      else:\n",
        "          raise ValueError(\"Unknown SVM type. Choose from ['L1', 'L2', 'Cauchy', 'Fair', 'Welsch', 'Geman-McClure'].\")\n",
        "\n",
        "  def fitness(self, W, b, X, y):\n",
        "      scores = X.dot(W.T) + b\n",
        "      margins = np.maximum(0, 1 - (scores[np.arange(X.shape[0]), y] - scores.T).T)\n",
        "      margins[np.arange(X.shape[0]), y] = 0  # 정답 클래스의 손실 제거\n",
        "      slack_term = self.compute_slack_term(margins)\n",
        "      regularization = np.sum(W**2) / 2  # L2 Regularization\n",
        "      return regularization + self.C * np.sum(slack_term)\n",
        "\n",
        "  def initialize_population(self, n_features):\n",
        "      population = [(np.random.randn(self.num_classes, n_features), np.random.randn(self.num_classes)) for _ in range(self.pop_size)]\n",
        "      return population\n",
        "\n",
        "  def selection(self, population, fitness_values):\n",
        "      probabilities = 1 / (fitness_values + 1e-6)\n",
        "      probabilities /= probabilities.sum()\n",
        "      selected_indices = np.random.choice(len(population), size=len(population), p=probabilities)\n",
        "      return [population[i] for i in selected_indices]\n",
        "\n",
        "  def crossover(self, parent1, parent2):\n",
        "      W1, b1 = parent1\n",
        "      W2, b2 = parent2\n",
        "      if np.random.rand() < self.crossover_rate:\n",
        "          point = np.random.randint(W1.shape[1])\n",
        "          new_W = np.hstack((W1[:, :point], W2[:, point:]))\n",
        "          new_b = (b1 + b2) / 2\n",
        "      else:\n",
        "          new_W, new_b = W1.copy(), b1\n",
        "      return new_W, new_b\n",
        "\n",
        "  def mutation(self, W, b):\n",
        "      if np.random.rand() < self.mutation_rate:\n",
        "          mutation_matrix = np.random.randn(*W.shape) * 0.1\n",
        "          W += mutation_matrix\n",
        "          b += np.random.randn(*b.shape) * 0.1\n",
        "      return W, b\n",
        "\n",
        "  def fit(self, X, y):\n",
        "      n_samples, n_features = X.shape\n",
        "      population = self.initialize_population(n_features)\n",
        "      fitness_values = np.array([self.fitness(W, b, X, y) for W, b in population])\n",
        "      np.random.seed(42)  # 시드 설정\n",
        "      for _ in range(self.max_iter):\n",
        "          selected_population = self.selection(population, fitness_values)\n",
        "          new_population = []\n",
        "          for i in range(0, len(selected_population), 2):\n",
        "              p1, p2 = selected_population[i], selected_population[(i + 1) % len(selected_population)]\n",
        "              offspring1 = self.crossover(p1, p2)\n",
        "              offspring2 = self.crossover(p2, p1)\n",
        "              new_population.append(self.mutation(*offspring1))\n",
        "              new_population.append(self.mutation(*offspring2))\n",
        "          population = new_population\n",
        "          fitness_values = np.array([self.fitness(W, b, X, y) for W, b in population])\n",
        "      best_idx = np.argmin(fitness_values)\n",
        "      self.W_best, self.b_best = population[best_idx]\n",
        "\n",
        "  def predict(self, X):\n",
        "      scores = X.dot(self.W_best.T) + self.b_best\n",
        "      return np.argmax(scores, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "metadata": {
        "id": "3VNGv9G1G5AK"
      },
      "outputs": [],
      "source": [
        "# Direct SVM multiclass training function\n",
        "def train_direct(X, y, C, num_classes, svm_type):\n",
        "    model = GA_SVM(C=C, svm_type=svm_type, num_classes=num_classes)\n",
        "    model.fit(X, y)\n",
        "    return model.W_best, model.b_best  # 모델의 가중치와 바이어스를 반환\n",
        "\n",
        "# Multiclass prediction function\n",
        "def predict_direct(X, weights, biases):\n",
        "    scores = X.dot(weights.T) + biases\n",
        "    return np.argmax(scores, axis=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stUCqlMaG5AL"
      },
      "source": [
        "###SMO (Sequential Minial Optimization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "4VSsVm2tG5AL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "from itertools import combinations\n",
        "\n",
        "class SMO_SVM:\n",
        "    def __init__(self, C=1.0, kernel='linear', tol=1e-3, max_iter=1000, svm_type='L2', c=1.0, lr=0.01, num_classes=3):\n",
        "      self.C = C\n",
        "      self.kernel = kernel\n",
        "      self.tol = tol\n",
        "      self.max_iter = max_iter\n",
        "      self.svm_type = svm_type\n",
        "      self.c = c\n",
        "      self.lr = lr\n",
        "      self.num_classes = num_classes\n",
        "      self.W_best = None  # 최적 가중치 행렬 추가\n",
        "      self.b_best = None  # 최적 편향 벡터 추가\n",
        "    def kernel_function(self, X, Y):\n",
        "      if self.kernel == 'linear':\n",
        "          return np.dot(X, Y.T)\n",
        "      else:\n",
        "          raise ValueError(\"Only linear kernel is supported in this implementation.\")\n",
        "\n",
        "    def compute_slack_term(self, xi):\n",
        "        xi = np.maximum(xi, 1e-6)\n",
        "        if self.svm_type == 'Fair':\n",
        "            return np.sum(self.c**2 * ((xi / self.c) - np.log(1 + xi / self.c)))\n",
        "        elif self.svm_type == 'Cauchy':\n",
        "            return np.sum((self.c**2 / 2) * np.log(1 + (xi / self.c)**2))\n",
        "        elif self.svm_type == 'Welsch':\n",
        "            return np.sum((self.c**2 / 2) * (1 - np.exp(-(xi / self.c)**2)))\n",
        "        elif self.svm_type == 'Geman-McClure':\n",
        "            return np.sum(((xi**2)/2) / (1 + xi**2))\n",
        "        elif self.svm_type == 'L1':\n",
        "            return np.abs(xi)\n",
        "        elif self.svm_type == 'L2':\n",
        "            return xi ** 2\n",
        "        else:\n",
        "            raise ValueError(\"Unknown SVM type. Choose from ['L1', 'L2', 'Cauchy', 'Fair', 'Welsch', 'Geman-McClure'].\")\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        self.alpha = np.zeros((n_samples, self.num_classes))\n",
        "        self.b = np.zeros(self.num_classes)\n",
        "        self.W = np.zeros((self.num_classes, n_features))\n",
        "\n",
        "        for _ in range(self.max_iter):\n",
        "            for i in range(n_samples):\n",
        "                xi, yi = X[i], y[i]\n",
        "                margins = self.W.dot(xi) + self.b\n",
        "                correct_class_margin = margins[yi]\n",
        "                margin_diff = 1 - (correct_class_margin - margins)\n",
        "                margin_diff[yi] = 0  # 정답 클래스의 손실 제거\n",
        "                slack = np.maximum(0, margin_diff)\n",
        "                slack_term = self.compute_slack_term(slack)\n",
        "\n",
        "                if np.any(slack > 0):\n",
        "                    delta_alpha = self.C * (1 - margin_diff) - slack_term\n",
        "                    self.alpha[i, yi] = np.clip(self.alpha[i, yi] + self.lr * delta_alpha[yi], 0, self.C)\n",
        "                    self.W[yi] += self.lr * self.alpha[i, yi] * xi\n",
        "                    self.b[yi] += self.lr * self.alpha[i, yi]\n",
        "\n",
        "        self.W_best = self.W  # 최적의 가중치 행렬 저장\n",
        "        self.b_best = self.b  # 최적의 편향 벡터 저장\n",
        "\n",
        "    def predict(self, X):\n",
        "        scores = X.dot(self.W_best.T) + self.b_best\n",
        "        return np.argmax(scores, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "LmkLwi-8G5AL"
      },
      "outputs": [],
      "source": [
        "# Direct SVM multiclass training function\n",
        "def train_direct(X, y, C, num_classes, svm_type):\n",
        "    model = SMO_SVM(C=C, svm_type=svm_type, num_classes=num_classes)\n",
        "    model.fit(X, y)\n",
        "    return model.W_best, model.b_best  # 모델의 가중치와 바이어스를 반환\n",
        "\n",
        "# Multiclass prediction function\n",
        "def predict_direct(X, weights, biases):\n",
        "    scores = X.dot(weights.T) + biases\n",
        "    return np.argmax(scores, axis=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNeDbczSG5AM"
      },
      "source": [
        "###PSO (Particle Swarm Optimization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFM9Ig5XG5AM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from itertools import combinations\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "class PSO_SVM:\n",
        "    def __init__(self, C=1.0, num_particles=30, max_iter=100, svm_type='L2', c=1.0, num_classes=3):\n",
        "        self.C = C  # 정규화 상수\n",
        "        self.num_particles = num_particles  # PSO 입자 개수\n",
        "        self.max_iter = max_iter  # PSO 반복 횟수\n",
        "        self.svm_type = svm_type  # L1 또는 L2\n",
        "        self.num_classes = num_classes\n",
        "        self.W_best = None\n",
        "        self.b_best = None\n",
        "        self.c = c\n",
        "    def compute_slack_term(self, xi):\n",
        "        if self.svm_type == 'L1':\n",
        "            return np.abs(xi)\n",
        "        elif self.svm_type == 'L2':\n",
        "            return xi ** 2\n",
        "        elif self.svm_type == 'Fair':\n",
        "            return np.sum(self.c**2 * ((xi / self.c) - np.log(1 + xi / self.c)))\n",
        "        elif self.svm_type == 'Cauchy':\n",
        "            return np.sum((self.c**2 / 2) * np.log(1 + (xi / self.c)**2))\n",
        "        elif self.svm_type == 'Welsch':\n",
        "            return np.sum((self.c**2 / 2) * (1 - np.exp(-(xi / self.c)**2)))\n",
        "        elif self.svm_type == 'Geman-McClure':\n",
        "            return np.sum(((xi**2)/2) / (1 + xi**2))\n",
        "        else:\n",
        "            raise ValueError(\"Unknown SVM type. Choose from ['L1', 'L2', 'Cauchy', 'Fair', 'Welsch', 'Geman-McClure'].\")\n",
        "\n",
        "    def fitness(self, W, b, X, y):\n",
        "        scores = X.dot(W.T) + b\n",
        "        margins = np.maximum(0, 1 - (scores[np.arange(X.shape[0]), y] - scores.T).T)\n",
        "        margins[np.arange(X.shape[0]), y] = 0\n",
        "        slack_term = self.compute_slack_term(margins)\n",
        "        regularization = np.sum(W**2) / 2\n",
        "        return regularization + self.C * np.sum(slack_term)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        W_particles = np.random.randn(self.num_particles, self.num_classes, n_features)\n",
        "        b_particles = np.random.randn(self.num_particles, self.num_classes)\n",
        "        velocities_W = np.random.randn(self.num_particles, self.num_classes, n_features) * 0.1\n",
        "        velocities_b = np.random.randn(self.num_particles, self.num_classes) * 0.1\n",
        "\n",
        "        p_best_W = np.copy(W_particles)\n",
        "        p_best_b = np.copy(b_particles)\n",
        "        p_best_scores = np.array([self.fitness(W, b, X, y) for W, b in zip(W_particles, b_particles)])\n",
        "\n",
        "        g_best_index = np.argmin(p_best_scores)\n",
        "        g_best_W = p_best_W[g_best_index]\n",
        "        g_best_b = p_best_b[g_best_index]\n",
        "\n",
        "        w_inertia = 0.7\n",
        "        c1, c2 = 1.5, 1.5\n",
        "\n",
        "        for _ in range(self.max_iter):\n",
        "            for i in range(self.num_particles):\n",
        "                r1, r2 = np.random.rand(), np.random.rand()\n",
        "                velocities_W[i] = (w_inertia * velocities_W[i] +\n",
        "                                  c1 * r1 * (p_best_W[i] - W_particles[i]) +\n",
        "                                  c2 * r2 * (g_best_W - W_particles[i]))\n",
        "                velocities_b[i] = (w_inertia * velocities_b[i] +\n",
        "                                  c1 * r1 * (p_best_b[i] - b_particles[i]) +\n",
        "                                  c2 * r2 * (g_best_b - b_particles[i]))\n",
        "\n",
        "                W_particles[i] += velocities_W[i]\n",
        "                b_particles[i] += velocities_b[i]\n",
        "                new_fitness = self.fitness(W_particles[i], b_particles[i], X, y)\n",
        "\n",
        "                if new_fitness < p_best_scores[i]:\n",
        "                    p_best_W[i] = W_particles[i]\n",
        "                    p_best_b[i] = b_particles[i]\n",
        "                    p_best_scores[i] = new_fitness\n",
        "\n",
        "            g_best_index = np.argmin(p_best_scores)\n",
        "            g_best_W = p_best_W[g_best_index]\n",
        "            g_best_b = p_best_b[g_best_index]\n",
        "\n",
        "        self.W_best = g_best_W\n",
        "        self.b_best = g_best_b\n",
        "\n",
        "    def predict(self, X):\n",
        "        scores = X.dot(self.W_best.T) + self.b_best\n",
        "        return np.argmax(scores, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uX8h5qlPG5AM"
      },
      "outputs": [],
      "source": [
        "# Direct SVM multiclass training function\n",
        "def train_direct(X, y, C, num_classes, svm_type):\n",
        "    model = PSO_SVM(C=C, svm_type=svm_type, num_classes=num_classes)\n",
        "    model.fit(X, y)\n",
        "    return model.W_best, model.b_best  # 모델의 가중치와 바이어스를 반환\n",
        "\n",
        "# Multiclass prediction function\n",
        "def predict_direct(X, weights, biases):\n",
        "    scores = X.dot(weights.T) + biases\n",
        "    return np.argmax(scores, axis=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhdUf7pDG5AN"
      },
      "source": [
        "###ACO (Ant Colony Optimization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "id": "sWaq6jD9G5AN"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from itertools import combinations\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "class ACO_SVM:\n",
        "    def __init__(self, C=1.0, num_ants=30, max_iter=100, decay=0.5, alpha=1, beta=2, svm_type='L2', c=1.0, num_classes=3):\n",
        "        self.C = C  # 정규화 상수\n",
        "        self.num_ants = num_ants  # 개미 개수\n",
        "        self.max_iter = max_iter  # 반복 횟수\n",
        "        self.decay = decay  # 페로몬 증발 계수\n",
        "        self.alpha = alpha  # 페로몬 영향도\n",
        "        self.beta = beta  # 휴리스틱 정보 영향도\n",
        "        self.svm_type = svm_type  # L1 또는 L2\n",
        "        self.num_classes = num_classes\n",
        "        self.W_best = None\n",
        "        self.b_best = None\n",
        "        self.c = c\n",
        "    def compute_slack_term(self, xi):\n",
        "        if self.svm_type == 'L1':\n",
        "            return np.abs(xi)\n",
        "        elif self.svm_type == 'L2':\n",
        "            return xi ** 2\n",
        "        elif self.svm_type == 'Fair':\n",
        "            return np.sum(self.c**2 * ((xi / self.c) - np.log(1 + xi / self.c)))\n",
        "        elif self.svm_type == 'Cauchy':\n",
        "            return np.sum((self.c**2 / 2) * np.log(1 + (xi / self.c)**2))\n",
        "        elif self.svm_type == 'Welsch':\n",
        "            return np.sum((self.c**2 / 2) * (1 - np.exp(-(xi / self.c)**2)))\n",
        "        elif self.svm_type == 'Geman-McClure':\n",
        "            return np.sum(((xi**2)/2) / (1 + xi**2))\n",
        "        else:\n",
        "            raise ValueError(\"Unknown SVM type. Choose from ['L1', 'L2', 'Cauchy', 'Fair', 'Welsch', 'Geman-McClure'].\")\n",
        "\n",
        "    def fitness(self, W, b, X, y):\n",
        "        scores = X.dot(W.T) + b\n",
        "        margins = np.maximum(0, 1 - (scores[np.arange(X.shape[0]), y] - scores.T).T)\n",
        "        margins[np.arange(X.shape[0]), y] = 0\n",
        "        slack_term = self.compute_slack_term(margins)\n",
        "        regularization = np.sum(W**2) / 2\n",
        "        return regularization + self.C * np.sum(slack_term)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        W_ants = np.random.randn(self.num_ants, self.num_classes, n_features)\n",
        "        b_ants = np.random.randn(self.num_ants, self.num_classes)\n",
        "        pheromones = np.ones(self.num_ants)\n",
        "\n",
        "        best_fitness = float('inf')\n",
        "        g_best_W, g_best_b = None, None\n",
        "\n",
        "        for _ in range(self.max_iter):\n",
        "            fitness_values = np.array([self.fitness(W, b, X, y) for W, b in zip(W_ants, b_ants)])\n",
        "            best_index = np.argmin(fitness_values)\n",
        "            if fitness_values[best_index] < best_fitness:\n",
        "                best_fitness = fitness_values[best_index]\n",
        "                g_best_W, g_best_b = W_ants[best_index], b_ants[best_index]\n",
        "\n",
        "            pheromones = (1 - self.decay) * pheromones\n",
        "            pheromones[best_index] += 1 / (1 + best_fitness)\n",
        "\n",
        "            probabilities = (pheromones ** self.alpha) * ((1 / (1 + fitness_values)) ** self.beta)\n",
        "            probabilities /= np.sum(probabilities)\n",
        "\n",
        "            selected_indices = np.random.choice(self.num_ants, size=self.num_ants, p=probabilities)\n",
        "            W_ants = W_ants[selected_indices] + np.random.randn(self.num_ants, self.num_classes, n_features) * 0.1\n",
        "            b_ants = b_ants[selected_indices] + np.random.randn(self.num_ants, self.num_classes) * 0.1\n",
        "\n",
        "        self.W_best = g_best_W\n",
        "        self.b_best = g_best_b\n",
        "\n",
        "    def predict(self, X):\n",
        "        scores = X.dot(self.W_best.T) + self.b_best\n",
        "        return np.argmax(scores, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "id": "JFZtLGCeG5AN"
      },
      "outputs": [],
      "source": [
        "# Direct SVM multiclass training function\n",
        "def train_direct(X, y, C, num_classes, svm_type):\n",
        "    model = ACO_SVM(C=C, svm_type=svm_type, num_classes=num_classes)\n",
        "    model.fit(X, y)\n",
        "    return model.W_best, model.b_best  # 모델의 가중치와 바이어스를 반환\n",
        "\n",
        "# Multiclass prediction function\n",
        "def predict_direct(X, weights, biases):\n",
        "    scores = X.dot(weights.T) + biases\n",
        "    return np.argmax(scores, axis=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvqcqw92G5AO"
      },
      "source": [
        "###HS (Harmony Search)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "8_9YRW6KG5AO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from itertools import combinations\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "class HS_SVM:\n",
        "    def __init__(self, C=1.0, hm_size=20, max_iter=100, hmcr=0.9, par=0.3, bw=0.1, svm_type='L2', c=1.0, num_classes=3):\n",
        "        self.C = C  # 정규화 상수\n",
        "        self.hm_size = hm_size  # 하모니 메모리 크기\n",
        "        self.max_iter = max_iter  # 반복 횟수\n",
        "        self.hmcr = hmcr  # 하모니 메모리 고려율 (기존 해를 선택할 확률)\n",
        "        self.par = par  # 피치 조정 비율 (기존 해를 변형할 확률)\n",
        "        self.bw = bw  # 변형 크기\n",
        "        self.svm_type = svm_type  # 'L1' 또는 'L2'\n",
        "        self.num_classes = num_classes\n",
        "        self.W_best = None\n",
        "        self.b_best = None\n",
        "        self.c = c\n",
        "\n",
        "    def compute_slack_term(self, xi):\n",
        "        if self.svm_type == 'L1':\n",
        "            return np.abs(xi)\n",
        "        elif self.svm_type == 'L2':\n",
        "            return xi ** 2\n",
        "        elif self.svm_type == 'Fair':\n",
        "            return np.sum(self.c**2 * ((xi / self.c) - np.log(1 + xi / self.c)))\n",
        "        elif self.svm_type == 'Cauchy':\n",
        "            return np.sum((self.c**2 / 2) * np.log(1 + (xi / self.c)**2))\n",
        "        elif self.svm_type == 'Welsch':\n",
        "            return np.sum((self.c**2 / 2) * (1 - np.exp(-(xi / self.c)**2)))\n",
        "        elif self.svm_type == 'Geman-McClure':\n",
        "            return np.sum(((xi**2)/2) / (1 + xi**2))\n",
        "        else:\n",
        "            raise ValueError(\"Unknown SVM type. Choose from ['L1', 'L2', 'Cauchy', 'Fair', 'Welsch', 'Geman-McClure'].\")\n",
        "\n",
        "    def fitness(self, W, b, X, y):\n",
        "        scores = X.dot(W.T) + b\n",
        "        margins = np.maximum(0, 1 - (scores[np.arange(X.shape[0]), y] - scores.T).T)\n",
        "        margins[np.arange(X.shape[0]), y] = 0\n",
        "        slack_term = self.compute_slack_term(margins)\n",
        "        regularization = np.sum(W**2) / 2\n",
        "        return regularization + self.C * np.sum(slack_term)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        harmony_memory = [(np.random.randn(self.num_classes, n_features), np.random.randn(self.num_classes)) for _ in range(self.hm_size)]\n",
        "        fitness_values = np.array([self.fitness(W, b, X, y) for W, b in harmony_memory])\n",
        "\n",
        "        for _ in range(self.max_iter):\n",
        "            new_W, new_b = np.zeros((self.num_classes, n_features)), np.zeros(self.num_classes)\n",
        "            for j in range(n_features):\n",
        "                if np.random.rand() < self.hmcr:\n",
        "                    new_W[:, j] = harmony_memory[np.random.randint(self.hm_size)][0][:, j]\n",
        "                    if np.random.rand() < self.par:\n",
        "                        new_W[:, j] += np.random.uniform(-self.bw, self.bw, self.num_classes)\n",
        "                else:\n",
        "                    new_W[:, j] = np.random.randn(self.num_classes)\n",
        "\n",
        "            if np.random.rand() < self.hmcr:\n",
        "                new_b = harmony_memory[np.random.randint(self.hm_size)][1]\n",
        "                if np.random.rand() < self.par:\n",
        "                    new_b += np.random.uniform(-self.bw, self.bw, self.num_classes)\n",
        "            else:\n",
        "                new_b = np.random.randn(self.num_classes)\n",
        "\n",
        "            new_fitness = self.fitness(new_W, new_b, X, y)\n",
        "            worst_idx = np.argmax(fitness_values)\n",
        "            if new_fitness < fitness_values[worst_idx]:\n",
        "                harmony_memory[worst_idx] = (new_W, new_b)\n",
        "                fitness_values[worst_idx] = new_fitness\n",
        "\n",
        "        best_idx = np.argmin(fitness_values)\n",
        "        self.W_best, self.b_best = harmony_memory[best_idx]\n",
        "\n",
        "    def predict(self, X):\n",
        "        scores = X.dot(self.W_best.T) + self.b_best\n",
        "        return np.argmax(scores, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "fVRY1K_IG5AO"
      },
      "outputs": [],
      "source": [
        "# Direct SVM multiclass training function\n",
        "def train_direct(X, y, C, num_classes, svm_type):\n",
        "    model = HS_SVM(C=C, svm_type=svm_type, num_classes=num_classes)\n",
        "    model.fit(X, y)\n",
        "    return model.W_best, model.b_best  # 모델의 가중치와 바이어스를 반환\n",
        "\n",
        "# Multiclass prediction function\n",
        "def predict_direct(X, weights, biases):\n",
        "    scores = X.dot(weights.T) + biases\n",
        "    return np.argmax(scores, axis=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWUPoG7VoeVW"
      },
      "source": [
        "#**OvO with various outliers**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4BXtOi4o98n"
      },
      "source": [
        "###iris data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fK51E0X_pmvP",
        "outputId": "575b117a-4037-4c0b-ddfd-a227e3ce8f16"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "기존 이상치 수: 1\n",
            "기존 이상치 비율: 0.67%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.stats import zscore\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# 데이터셋 로딩\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "df = pd.DataFrame(X, columns=data.feature_names)\n",
        "df['target'] = y  # 타겟 변수 추가\n",
        "\n",
        "# 데이터 정규화\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "df_scaled = pd.DataFrame(X_scaled, columns=data.feature_names)\n",
        "\n",
        "# 기존 이상치 식별\n",
        "df_zscores = df_scaled.apply(zscore)  # Z-score 계산\n",
        "is_existing_outlier = (df_zscores.abs() > 3).any(axis=1)  # 절대값이 3 이상인 경우 이상치로 판단\n",
        "\n",
        "# 기존 이상치 수 계산\n",
        "total_existing_outliers = is_existing_outlier.sum()\n",
        "\n",
        "# 기존 이상치 비율 계산\n",
        "total_samples = len(df)\n",
        "existing_outlier_percentage = (total_existing_outliers / total_samples) * 100\n",
        "\n",
        "print(f\"기존 이상치 수: {total_existing_outliers}\")\n",
        "print(f\"기존 이상치 비율: {existing_outlier_percentage:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 176,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_und-Y6EnrCp",
        "outputId": "6819676e-20fc-4687-eb2b-5428ffd4e792"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "확장된 데이터셋 크기: (154, 4)\n",
            "확장된 타겟 크기: (154,)\n",
            "이상치가 포함된 데이터셋의 총 샘플 수: 154\n",
            "전체 이상치 비율: 3.25%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import load_iris\n",
        "from scipy.stats import zscore\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# 데이터셋 로딩\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "df = pd.DataFrame(X, columns=iris.feature_names)\n",
        "df['target'] = y  # 타겟 변수 추가\n",
        "\n",
        "# 데이터 정규화\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "df_scaled = pd.DataFrame(X_scaled, columns=iris.feature_names)\n",
        "\n",
        "# 이상치 생성\n",
        "num_outliers = int(0.03 * len(df))  # 이상치를 설정\n",
        "outliers = []\n",
        "\n",
        "for _ in range(num_outliers):\n",
        "    outlier_sample = df_scaled.sample(n=1).copy()  # 임의의 샘플 선택\n",
        "    for col in df_scaled.columns:\n",
        "        if np.random.rand() > 0.5:\n",
        "            # 최대값보다 큰 이상치 생성\n",
        "            outlier_sample[col] = df_scaled[col].max() + np.random.rand() * 3\n",
        "        else:\n",
        "            # 최소값보다 작은 이상치 생성\n",
        "            outlier_sample[col] = df_scaled[col].min() - np.random.rand() * 3\n",
        "    outliers.append(outlier_sample)\n",
        "\n",
        "outliers_df = pd.concat(outliers, ignore_index=True)\n",
        "\n",
        "# 이상치의 타겟 값 랜덤 할당\n",
        "outliers_df['target'] = np.random.choice(y, num_outliers)\n",
        "\n",
        "# 원본 데이터에 이상치 추가\n",
        "df_extended = pd.concat([df, outliers_df], ignore_index=True)\n",
        "\n",
        "# 이상치 추가 후 다시 정규화\n",
        "X_extended_scaled = scaler.fit_transform(df_extended.drop(columns=['target']))\n",
        "df_extended_scaled = pd.DataFrame(X_extended_scaled, columns=iris.feature_names)\n",
        "\n",
        "# 새로운 X, y 정의\n",
        "X = df_extended_scaled.values  # NumPy 배열로 변환\n",
        "y = df_extended['target'].values  # 타겟 값 추출\n",
        "\n",
        "# 기존 이상치 식별\n",
        "df_zscores = df_scaled.apply(zscore)  # Z-score 계산\n",
        "is_existing_outlier = (df_zscores.abs() > 3).any(axis=1)  # 절대값이 3 이상인 경우 이상치로 판단\n",
        "\n",
        "total_existing_outliers = is_existing_outlier.sum()  # 기존 이상치 수\n",
        "total_outliers = total_existing_outliers + num_outliers  # 총 이상치 수\n",
        "total_samples = len(df_extended)  # 전체 샘플 수\n",
        "total_outlier_percentage = (total_outliers / total_samples) * 100  # 이상치 비율 계산\n",
        "\n",
        "# 데이터 확인\n",
        "print(f\"확장된 데이터셋 크기: {X.shape}\")\n",
        "print(f\"확장된 타겟 크기: {y.shape}\")\n",
        "print(f\"이상치가 포함된 데이터셋의 총 샘플 수: {len(X)}\")\n",
        "print(f\"전체 이상치 비율: {total_outlier_percentage:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHMPkItdpNyJ",
        "outputId": "b9b89a90-685f-40ed-fc61-4252b50e3756"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "확장된 데이터셋 크기: (157, 4)\n",
            "확장된 타겟 크기: (157,)\n",
            "이상치가 포함된 데이터셋의 총 샘플 수: 157\n",
            "전체 이상치 비율: 5.10%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import load_iris\n",
        "from scipy.stats import zscore\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# 데이터셋 로딩\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "df = pd.DataFrame(X, columns=iris.feature_names)\n",
        "df['target'] = y  # 타겟 변수 추가\n",
        "\n",
        "# 데이터 정규화\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "df_scaled = pd.DataFrame(X_scaled, columns=iris.feature_names)\n",
        "\n",
        "# 이상치 생성\n",
        "num_outliers = int(0.05 * len(df))  # 이상치를 설정\n",
        "outliers = []\n",
        "\n",
        "for _ in range(num_outliers):\n",
        "    outlier_sample = df_scaled.sample(n=1).copy()  # 임의의 샘플 선택\n",
        "    for col in df_scaled.columns:\n",
        "        if np.random.rand() > 0.5:\n",
        "            # 최대값보다 큰 이상치 생성\n",
        "            outlier_sample[col] = df_scaled[col].max() + np.random.rand() * 3\n",
        "        else:\n",
        "            # 최소값보다 작은 이상치 생성\n",
        "            outlier_sample[col] = df_scaled[col].min() - np.random.rand() * 3\n",
        "    outliers.append(outlier_sample)\n",
        "\n",
        "outliers_df = pd.concat(outliers, ignore_index=True)\n",
        "\n",
        "# 이상치의 타겟 값 랜덤 할당\n",
        "outliers_df['target'] = np.random.choice(y, num_outliers)\n",
        "\n",
        "# 원본 데이터에 이상치 추가\n",
        "df_extended = pd.concat([df, outliers_df], ignore_index=True)\n",
        "\n",
        "# 이상치 추가 후 다시 정규화\n",
        "X_extended_scaled = scaler.fit_transform(df_extended.drop(columns=['target']))\n",
        "df_extended_scaled = pd.DataFrame(X_extended_scaled, columns=iris.feature_names)\n",
        "\n",
        "# 새로운 X, y 정의\n",
        "X = df_extended_scaled.values  # NumPy 배열로 변환\n",
        "y = df_extended['target'].values  # 타겟 값 추출\n",
        "\n",
        "# 기존 이상치 식별\n",
        "df_zscores = df_scaled.apply(zscore)  # Z-score 계산\n",
        "is_existing_outlier = (df_zscores.abs() > 3).any(axis=1)  # 절대값이 3 이상인 경우 이상치로 판단\n",
        "\n",
        "total_existing_outliers = is_existing_outlier.sum()  # 기존 이상치 수\n",
        "total_outliers = total_existing_outliers + num_outliers  # 총 이상치 수\n",
        "total_samples = len(df_extended)  # 전체 샘플 수\n",
        "total_outlier_percentage = (total_outliers / total_samples) * 100  # 이상치 비율 계산\n",
        "\n",
        "# 데이터 확인\n",
        "print(f\"확장된 데이터셋 크기: {X.shape}\")\n",
        "print(f\"확장된 타겟 크기: {y.shape}\")\n",
        "print(f\"이상치가 포함된 데이터셋의 총 샘플 수: {len(X)}\")\n",
        "print(f\"전체 이상치 비율: {total_outlier_percentage:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtQip-vfpN0D",
        "outputId": "b255560a-dcbe-404b-9599-410da4162589"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "확장된 데이터셋 크기: (166, 4)\n",
            "확장된 타겟 크기: (166,)\n",
            "이상치가 포함된 데이터셋의 총 샘플 수: 166\n",
            "전체 이상치 비율: 10.24%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import load_iris\n",
        "from scipy.stats import zscore\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# 데이터셋 로딩\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "df = pd.DataFrame(X, columns=iris.feature_names)\n",
        "df['target'] = y  # 타겟 변수 추가\n",
        "\n",
        "# 데이터 정규화\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "df_scaled = pd.DataFrame(X_scaled, columns=iris.feature_names)\n",
        "\n",
        "# 이상치 생성\n",
        "num_outliers = int(0.11 * len(df))  # 이상치를 설정\n",
        "outliers = []\n",
        "\n",
        "for _ in range(num_outliers):\n",
        "    outlier_sample = df_scaled.sample(n=1).copy()  # 임의의 샘플 선택\n",
        "    for col in df_scaled.columns:\n",
        "        if np.random.rand() > 0.5:\n",
        "            # 최대값보다 큰 이상치 생성\n",
        "            outlier_sample[col] = df_scaled[col].max() + np.random.rand() * 3\n",
        "        else:\n",
        "            # 최소값보다 작은 이상치 생성\n",
        "            outlier_sample[col] = df_scaled[col].min() - np.random.rand() * 3\n",
        "    outliers.append(outlier_sample)\n",
        "\n",
        "outliers_df = pd.concat(outliers, ignore_index=True)\n",
        "\n",
        "# 이상치의 타겟 값 랜덤 할당\n",
        "outliers_df['target'] = np.random.choice(y, num_outliers)\n",
        "\n",
        "# 원본 데이터에 이상치 추가\n",
        "df_extended = pd.concat([df, outliers_df], ignore_index=True)\n",
        "\n",
        "# 이상치 추가 후 다시 정규화\n",
        "X_extended_scaled = scaler.fit_transform(df_extended.drop(columns=['target']))\n",
        "df_extended_scaled = pd.DataFrame(X_extended_scaled, columns=iris.feature_names)\n",
        "\n",
        "# 새로운 X, y 정의\n",
        "X = df_extended_scaled.values  # NumPy 배열로 변환\n",
        "y = df_extended['target'].values  # 타겟 값 추출\n",
        "\n",
        "# 기존 이상치 식별\n",
        "df_zscores = df_scaled.apply(zscore)  # Z-score 계산\n",
        "is_existing_outlier = (df_zscores.abs() > 3).any(axis=1)  # 절대값이 3 이상인 경우 이상치로 판단\n",
        "\n",
        "total_existing_outliers = is_existing_outlier.sum()  # 기존 이상치 수\n",
        "total_outliers = total_existing_outliers + num_outliers  # 총 이상치 수\n",
        "total_samples = len(df_extended)  # 전체 샘플 수\n",
        "total_outlier_percentage = (total_outliers / total_samples) * 100  # 이상치 비율 계산\n",
        "\n",
        "# 데이터 확인\n",
        "print(f\"확장된 데이터셋 크기: {X.shape}\")\n",
        "print(f\"확장된 타겟 크기: {y.shape}\")\n",
        "print(f\"이상치가 포함된 데이터셋의 총 샘플 수: {len(X)}\")\n",
        "print(f\"전체 이상치 비율: {total_outlier_percentage:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJGd9cJBpN2O",
        "outputId": "d4e99ebf-d599-4940-eb3b-e9b87653709a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        SVM Type  Accuracy\n",
            "0             L1     97.87\n",
            "1             L2     97.87\n",
            "2           Fair     95.74\n",
            "3         Cauchy     97.87\n",
            "4         Welsch     95.74\n",
            "5  Geman-McClure     95.74\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from itertools import combinations\n",
        "\n",
        "results = []\n",
        "\n",
        "# 데이터 전처리 및 분할\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# OvO 방식을 사용하여 각 SVM 유형에 대해 학습 및 평가\n",
        "for svm_type in ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']:\n",
        "    models_ovo = train_ovo(X_train, y_train, C=1.0, svm_type=svm_type)\n",
        "    y_pred_ovo = predict_ovo(X_test, models_ovo)\n",
        "\n",
        "    # y_pred_ovo 변수를 정의한 후에 데이터 타입 확인 및 변환\n",
        "    if y_test.dtype != np.int64:\n",
        "        y_test = y_test.astype(int)\n",
        "\n",
        "    if y_pred_ovo.dtype != np.int64:\n",
        "        y_pred_ovo = y_pred_ovo.astype(int)\n",
        "\n",
        "    accuracy_ovo = accuracy_score(y_test, y_pred_ovo)\n",
        "    results.append((svm_type, round(accuracy_ovo * 100, 2)))\n",
        "\n",
        "# 결과를 DataFrame으로 변환 및 출력\n",
        "results_df = pd.DataFrame(results, columns=['SVM Type', 'Accuracy'])\n",
        "print(results_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vCwdZcbpN4n",
        "outputId": "8b18cec7-002d-4990-8b35-482f6f677da8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                      3.25%  5.10%  10.24%\n",
            "Iris data (L-BFGS-B)                      \n",
            "L1                    97.87  95.83    92.0\n",
            "L2                    97.87  93.75    76.0\n",
            "Fair                  95.74  93.75    84.0\n",
            "Cauchy                97.87  93.75    94.0\n",
            "Welsch                95.74  91.67    94.0\n",
            "Geman-McClure         95.74  91.67    94.0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "accuracy_data = {\n",
        "    '3.25%' : [97.87, 97.87, 95.74, 97.87, 95.74, 95.74],\n",
        "    '5.10%' : [95.83, 93.75, 93.75, 93.75, 91.67, 91.67],\n",
        "    '10.24%' : [92.0, 76.0, 84.0, 94.0, 94.0, 94.0]\n",
        "}\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Iris data (L-BFGS-B)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gC-p79Wmp78E",
        "outputId": "5a8f1e3c-ac70-4431-888b-c035397aa789"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                3.25%  5.10%  10.24%\n",
            "Iris data (GA)                      \n",
            "L1              85.11  85.42    74.0\n",
            "L2              91.49  93.75    62.0\n",
            "Fair            80.85  75.00    84.0\n",
            "Cauchy          87.23  89.58    66.0\n",
            "Welsch          80.85  77.08    70.0\n",
            "Geman-McClure   85.11  81.25    70.0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "outlier = ['3%', '5%', '10%']\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "\n",
        "accuracy_data = {\n",
        "    '3.25%' : [85.11, 91.49, 80.85, 87.23, 80.85, 85.11],\n",
        "    '5.10%' : [85.42, 93.75, 75.00, 89.58, 77.08, 81.25],\n",
        "    '10.24%' : [74.0, 62.0, 84.0, 66.0, 70.0, 70.0]\n",
        "}\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Iris data (GA)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LdHGIl4Ip76L",
        "outputId": "652ac1d4-b1d8-429d-911c-92ed2eceb0ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                 3.25%  5.10%  10.24%\n",
            "Iris data (SMO)                      \n",
            "L1               36.17  37.50    34.0\n",
            "L2               36.17  37.50    34.0\n",
            "Fair             97.87  91.67    90.0\n",
            "Cauchy           97.87  91.67    90.0\n",
            "Welsch           97.87  91.67    90.0\n",
            "Geman-McClure    97.87  91.67    90.0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "outlier = ['3%', '5%', '10%']\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "\n",
        "accuracy_data = {\n",
        "    '3.25%' : [36.17, 36.17, 97.87, 97.87, 97.87, 97.87],\n",
        "    '5.10%' : [37.50, 37.50, 91.67, 91.67, 91.67, 91.67],\n",
        "    '10.24%' : [34.0, 34.0, 90.0, 90.0, 90.0, 90.0]\n",
        "}\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Iris data (SMO)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vEaYO1Ep74Q",
        "outputId": "cdcce1e3-cccf-4510-8c6b-2a8a1d94f0f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                 3.25%  5.10%  10.24%\n",
            "Iris data (PSO)                      \n",
            "L1               97.87  95.83    92.0\n",
            "L2               97.87  93.75    76.0\n",
            "Fair             95.74  93.75    84.0\n",
            "Cauchy           97.87  93.75    94.0\n",
            "Welsch           95.74  91.67    94.0\n",
            "Geman-McClure    95.74  91.67    94.0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "outlier = ['3%', '5%', '10%']\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "\n",
        "accuracy_data = {\n",
        "    '3.25%' : [97.87, 97.87, 95.74, 97.87, 95.74, 95.74],\n",
        "    '5.10%' : [95.83, 93.75, 93.75, 93.75, 91.67, 91.67],\n",
        "    '10.24%' : [92.0, 76.0, 84.0, 94.0, 94.0, 94.0]\n",
        "}\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Iris data (PSO)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0eQhqpCapN8E",
        "outputId": "ddd63cb1-9bde-4630-dd72-cee768e6d7ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                 3.25%  5.10%  10.24%\n",
            "Iris data (ACO)                      \n",
            "L1               97.78  97.87   95.83\n",
            "L2               97.78  97.87   93.75\n",
            "Fair             97.78  97.87   93.75\n",
            "Cauchy           97.78  97.87   93.75\n",
            "Welsch           97.78  95.74   91.67\n",
            "Geman-McClure    97.78  95.74   91.67\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "outlier = ['3%', '5%', '10%']\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "\n",
        "accuracy_data = {\n",
        "    '3.25%' : [97.78, 97.78, 97.78, 97.78, 97.78, 97.78],\n",
        "    '5.10%' : [97.87, 97.87, 97.87, 97.87, 95.74, 95.74],\n",
        "    '10.24%' : [95.83, 93.75, 93.75, 93.75, 91.67, 91.67]\n",
        "}\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Iris data (ACO)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MALlXApRocLs",
        "outputId": "9ee28313-3294-4b0e-d649-8b4464f8b066"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                3.25%  5.10%  10.24%\n",
            "Iris data (HS)                      \n",
            "L1              91.49  93.75    84.0\n",
            "L2              87.23  91.67    72.0\n",
            "Fair            95.74  91.67    66.0\n",
            "Cauchy          95.74  95.83    92.0\n",
            "Welsch          91.49  91.67    92.0\n",
            "Geman-McClure   89.36  85.42    88.0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "accuracy_data = {\n",
        "    '3.25%' : [91.49, 87.23, 95.74, 95.74, 91.49, 89.36],\n",
        "    '5.10%' : [93.75, 91.67, 91.67, 95.83, 91.67, 85.42],\n",
        "    '10.24%' : [84.0, 72.0, 66.0, 92.0, 92.0, 88.0]\n",
        "}\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Iris data (HS)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dF2RFLZFo0OL"
      },
      "source": [
        "### segment data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJ8QcmQSo0ON",
        "outputId": "fc604c03-3e4c-4d6d-833a-c300c7098ad6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "기존 이상치 수: 175\n",
            "기존 이상치 비율: 7.58%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from scipy.stats import zscore\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# 데이터셋 로딩\n",
        "data = fetch_openml(name='segment', version=1)\n",
        "X = data.data\n",
        "# Use LabelEncoder to convert categorical target to numerical\n",
        "encoder = LabelEncoder()\n",
        "y = encoder.fit_transform(data.target)\n",
        "df = pd.DataFrame(X, columns=data.feature_names)\n",
        "df['target'] = y  # 타겟 변수 추가\n",
        "\n",
        "# 데이터 정규화\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "df_scaled = pd.DataFrame(X_scaled, columns=data.feature_names)\n",
        "\n",
        "# 기존 이상치 식별\n",
        "df_zscores = df_scaled.apply(zscore)  # Z-score 계산\n",
        "is_existing_outlier = (df_zscores.abs() > 3).any(axis=1)  # 절대값이 3 이상인 경우 이상치로 판단\n",
        "\n",
        "# 기존 이상치 수 계산\n",
        "total_existing_outliers = is_existing_outlier.sum()\n",
        "\n",
        "# 기존 이상치 비율 계산\n",
        "total_samples = len(df)\n",
        "existing_outlier_percentage = (total_existing_outliers / total_samples) * 100\n",
        "\n",
        "print(f\"기존 이상치 수: {total_existing_outliers}\")\n",
        "print(f\"기존 이상치 비율: {existing_outlier_percentage:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ntbEK53go0OO",
        "outputId": "d057324e-8082-4d26-fbcc-44bfe787719b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "확장된 데이터셋 크기: (2374, 19)\n",
            "확장된 타겟 크기: (2374,)\n",
            "이상치가 포함된 데이터셋의 총 샘플 수: 2374\n",
            "전체 이상치 비율: 10.07%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import load_iris\n",
        "from scipy.stats import zscore\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# 데이터셋 로딩\n",
        "data = fetch_openml(name='segment', version=1)\n",
        "X = data.data\n",
        "# Use LabelEncoder to convert categorical target to numerical\n",
        "encoder = LabelEncoder()\n",
        "y = encoder.fit_transform(data.target)\n",
        "df = pd.DataFrame(X, columns=data.feature_names)\n",
        "df['target'] = y  # 타겟 변수 추가\n",
        "\n",
        "\n",
        "# 데이터 정규화\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "df_scaled = pd.DataFrame(X_scaled, columns=data.feature_names)\n",
        "\n",
        "# 이상치 생성\n",
        "num_outliers = int(0.028 * len(df))  # 이상치를 설정\n",
        "outliers = []\n",
        "\n",
        "for _ in range(num_outliers):\n",
        "    outlier_sample = df_scaled.sample(n=1).copy()  # 임의의 샘플 선택\n",
        "    for col in df_scaled.columns:\n",
        "        if np.random.rand() > 0.5:\n",
        "            # 최대값보다 큰 이상치 생성\n",
        "            outlier_sample[col] = df_scaled[col].max() + np.random.rand() * 3\n",
        "        else:\n",
        "            # 최소값보다 작은 이상치 생성\n",
        "            outlier_sample[col] = df_scaled[col].min() - np.random.rand() * 3\n",
        "    outliers.append(outlier_sample)\n",
        "\n",
        "outliers_df = pd.concat(outliers, ignore_index=True)\n",
        "\n",
        "# 이상치의 타겟 값 랜덤 할당\n",
        "outliers_df['target'] = np.random.choice(y, num_outliers)\n",
        "\n",
        "# 원본 데이터에 이상치 추가\n",
        "df_extended = pd.concat([df, outliers_df], ignore_index=True)\n",
        "\n",
        "# 이상치 추가 후 다시 정규화\n",
        "X_extended_scaled = scaler.fit_transform(df_extended.drop(columns=['target']))\n",
        "df_extended_scaled = pd.DataFrame(X_extended_scaled, columns=data.feature_names)\n",
        "\n",
        "# 새로운 X, y 정의\n",
        "X = df_extended_scaled.values  # NumPy 배열로 변환\n",
        "y = df_extended['target'].values  # 타겟 값 추출\n",
        "\n",
        "# 기존 이상치 식별\n",
        "df_zscores = df_scaled.apply(zscore)  # Z-score 계산\n",
        "is_existing_outlier = (df_zscores.abs() > 3).any(axis=1)  # 절대값이 3 이상인 경우 이상치로 판단\n",
        "\n",
        "total_existing_outliers = is_existing_outlier.sum()  # 기존 이상치 수\n",
        "total_outliers = total_existing_outliers + num_outliers  # 총 이상치 수\n",
        "total_samples = len(df_extended)  # 전체 샘플 수\n",
        "total_outlier_percentage = (total_outliers / total_samples) * 100  # 이상치 비율 계산\n",
        "\n",
        "# 데이터 확인\n",
        "print(f\"확장된 데이터셋 크기: {X.shape}\")\n",
        "print(f\"확장된 타겟 크기: {y.shape}\")\n",
        "print(f\"이상치가 포함된 데이터셋의 총 샘플 수: {len(X)}\")\n",
        "print(f\"전체 이상치 비율: {total_outlier_percentage:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jwo-w2to0OQ",
        "outputId": "efc7d64e-3c5f-488f-c029-371ede9aa5d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "        SVM Type  Accuracy\n",
            "0             L1     14.87\n",
            "1             L2     14.87\n",
            "2           Fair     94.39\n",
            "3         Cauchy     94.25\n",
            "4         Welsch     94.53\n",
            "5  Geman-McClure     94.67\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from itertools import combinations\n",
        "\n",
        "results = []\n",
        "\n",
        "# 데이터 전처리 및 분할\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# OvO 방식을 사용하여 각 SVM 유형에 대해 학습 및 평가\n",
        "for svm_type in ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']:\n",
        "    models_ovo = train_ovo(X_train, y_train, C=1.0, svm_type=svm_type)\n",
        "    y_pred_ovo = predict_ovo(X_test, models_ovo)\n",
        "\n",
        "    # y_pred_ovo 변수를 정의한 후에 데이터 타입 확인 및 변환\n",
        "    if y_test.dtype != np.int64:\n",
        "        y_test = y_test.astype(int)\n",
        "\n",
        "    if y_pred_ovo.dtype != np.int64:\n",
        "        y_pred_ovo = y_pred_ovo.astype(int)\n",
        "\n",
        "    accuracy_ovo = accuracy_score(y_test, y_pred_ovo)\n",
        "    results.append((svm_type, round(accuracy_ovo * 100, 2)))\n",
        "\n",
        "# 결과를 DataFrame으로 변환 및 출력\n",
        "results_df = pd.DataFrame(results, columns=['SVM Type', 'Accuracy'])\n",
        "print(results_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RiqZtMkxo0OR",
        "outputId": "0ef8d73f-7cf6-45f0-d739-444f58eef206"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                         7.58%  10.07%\n",
            "Segment data (L-BFGS-B)               \n",
            "L1                       93.65   92.71\n",
            "L2                       94.52   93.27\n",
            "Fair                     93.51   92.15\n",
            "Cauchy                   93.51   92.29\n",
            "Welsch                   93.80   93.41\n",
            "Geman-McClure            92.64   92.99\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "accuracy_data = {\n",
        "    '7.58%' : [93.65, 94.52, 93.51, 93.51, 93.80, 92.64],\n",
        "    '10.07%' : [92.71, 93.27, 92.15, 92.29, 93.41, 92.99]\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Segment data (L-BFGS-B)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwrJ31u-o0OR",
        "outputId": "3f63cfab-5e23-4b7d-d37f-e053b344921e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   7.58%  10.07%\n",
            "Segment data (GA)               \n",
            "L1                 85.57   85.41\n",
            "L2                 89.61   85.55\n",
            "Fair               87.59   85.97\n",
            "Cauchy             86.72   87.80\n",
            "Welsch             84.85   85.97\n",
            "Geman-McClure      86.72   85.41\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "outlier = ['3%', '5%', '10%']\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "\n",
        "accuracy_data = {\n",
        "    '7.58%' : [85.57, 89.61, 87.59, 86.72, 84.85, 86.72],\n",
        "    '10.07%' : [85.41, 85.55, 85.97, 87.80, 85.97, 85.41\n",
        "]\n",
        "}\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Segment data (GA)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWgi-NdRo0OS",
        "outputId": "fc168c9e-991d-4c19-fe30-8662f66e70f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                    7.58%  10.07%\n",
            "Segment data (SMO)               \n",
            "L1                  15.87   14.87\n",
            "L2                  15.87   14.87\n",
            "Fair                94.37   94.39\n",
            "Cauchy              94.52   94.25\n",
            "Welsch              94.52   94.53\n",
            "Geman-McClure       94.52   94.67\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "outlier = ['3%', '5%', '10%']\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "\n",
        "accuracy_data = {\n",
        "    '7.58%' : [15.87, 15.87, 94.37, 94.52, 94.52, 94.52],\n",
        "    '10.07%' : [14.87, 14.87, 94.39, 94.25, 94.53, 94.67]\n",
        "}\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Segment data (SMO)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEGU7KxI56xb",
        "outputId": "f1f91225-dad0-43e4-8bc8-176ee1ad4917"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                    7.58%  10.07%\n",
            "Segment data (PSO)               \n",
            "L1                  90.04   91.87\n",
            "L2                  90.91   92.15\n",
            "Fair                90.91   91.73\n",
            "Cauchy              89.47   92.57\n",
            "Welsch              92.35   91.73\n",
            "Geman-McClure       90.91   92.85\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "outlier = ['3%', '5%', '10%']\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "\n",
        "accuracy_data = {\n",
        "    '7.58%' : [90.04, 90.91, 90.91, 89.47, 92.35, 90.91],\n",
        "    '10.07%' : [91.87, 92.15, 91.73, 92.57, 91.73, 92.85]\n",
        "}\n",
        "\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Segment data (PSO)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAE2w4CZ563F",
        "outputId": "454f6e27-e5f1-48c6-d2b3-1e25deabbe9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                    7.58%  10.07%\n",
            "Segment data (ACO)               \n",
            "L1                  93.07   92.15\n",
            "L2                  92.93   89.76\n",
            "Fair                93.51   92.29\n",
            "Cauchy              93.36   93.13\n",
            "Welsch              93.22   93.41\n",
            "Geman-McClure       91.92   92.43\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "outlier = ['3%', '5%', '10%']\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "\n",
        "accuracy_data = {\n",
        "    '7.58%' : [93.07, 92.93, 93.51, 93.36, 93.22, 91.92],\n",
        "    '10.07%' : [91.87, 92.15, 91.73, 92.57, 91.73, 92.85]\n",
        "}\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Segment data (ACO)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9QicRClH56-K",
        "outputId": "2673a7c2-287b-40b6-ac35-1d80440936f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   7.58%  10.07%\n",
            "Segment data (HS)               \n",
            "L1                 85.86   84.71\n",
            "L2                 78.64   72.65\n",
            "Fair               82.25   85.55\n",
            "Cauchy             84.42   83.73\n",
            "Welsch             82.83   86.12\n",
            "Geman-McClure      81.10   86.12\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "outlier = ['3%', '5%', '10%']\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "\n",
        "accuracy_data = {\n",
        "    '7.58%' : [85.86, 78.64, 82.25, 84.42, 82.83, 81.10],\n",
        "    '10.07%' : [84.71, 72.65, 85.55, 83.73, 86.12, 86.12]\n",
        "}\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Segment data (HS)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnhhupsifrXC"
      },
      "source": [
        "### vehicle data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FrKXBGn4gbVy",
        "outputId": "e9e965cf-b230-4535-ae53-fcdc3a90e231"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "기존 이상치 수: 22\n",
            "기존 이상치 비율: 2.60%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from scipy.stats import zscore\n",
        "import random\n",
        "\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# 데이터셋 로딩\n",
        "data = fetch_openml(name='vehicle', version=1)\n",
        "X = data.data\n",
        "encoder = LabelEncoder()\n",
        "y = encoder.fit_transform(data.target)\n",
        "df = pd.DataFrame(X, columns=data.feature_names)\n",
        "df['target'] = y  # 타겟 변수 추가\n",
        "\n",
        "# 데이터 정규화\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "df_scaled = pd.DataFrame(X_scaled, columns=data.feature_names)\n",
        "\n",
        "# 기존 이상치 식별\n",
        "df_zscores = df_scaled.apply(zscore)  # Z-score 계산\n",
        "is_existing_outlier = (df_zscores.abs() > 3).any(axis=1)  # 절대값이 3 이상인 경우 이상치로 판단\n",
        "\n",
        "# 기존 이상치 수 계산\n",
        "total_existing_outliers = is_existing_outlier.sum()\n",
        "\n",
        "# 기존 이상치 비율 계산\n",
        "total_samples = len(df)\n",
        "existing_outlier_percentage = (total_existing_outliers / total_samples) * 100\n",
        "\n",
        "print(f\"기존 이상치 수: {total_existing_outliers}\")\n",
        "print(f\"기존 이상치 비율: {existing_outlier_percentage:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYFOv6gvgbVz",
        "outputId": "d2fa1f9e-d42e-41df-b23b-678b3c4f1237"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "확장된 데이터셋 크기: (850, 18)\n",
            "확장된 타겟 크기: (850,)\n",
            "이상치가 포함된 데이터셋의 총 샘플 수: 850\n",
            "전체 이상치 비율: 3.06%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import load_iris\n",
        "from scipy.stats import zscore\n",
        "import random\n",
        "\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# 데이터셋 로딩\n",
        "data = fetch_openml(name='vehicle', version=1)\n",
        "X = data.data\n",
        "# Use LabelEncoder to convert categorical target to numerical\n",
        "encoder = LabelEncoder()\n",
        "y = encoder.fit_transform(data.target)\n",
        "df = pd.DataFrame(X, columns=data.feature_names)\n",
        "df['target'] = y  # 타겟 변수 추가\n",
        "\n",
        "\n",
        "# 데이터 정규화\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "df_scaled = pd.DataFrame(X_scaled, columns=data.feature_names)\n",
        "\n",
        "# 이상치 생성\n",
        "num_outliers = int(0.0048 * len(df))  # 이상치를 설정\n",
        "outliers = []\n",
        "\n",
        "for _ in range(num_outliers):\n",
        "    outlier_sample = df_scaled.sample(n=1).copy()  # 임의의 샘플 선택\n",
        "    for col in df_scaled.columns:\n",
        "        if np.random.rand() > 0.5:\n",
        "            # 최대값보다 큰 이상치 생성\n",
        "            outlier_sample[col] = df_scaled[col].max() + np.random.rand() * 3\n",
        "        else:\n",
        "            # 최소값보다 작은 이상치 생성\n",
        "            outlier_sample[col] = df_scaled[col].min() - np.random.rand() * 3\n",
        "    outliers.append(outlier_sample)\n",
        "\n",
        "outliers_df = pd.concat(outliers, ignore_index=True)\n",
        "\n",
        "# 이상치의 타겟 값 랜덤 할당\n",
        "outliers_df['target'] = np.random.choice(y, num_outliers)\n",
        "\n",
        "# 원본 데이터에 이상치 추가\n",
        "df_extended = pd.concat([df, outliers_df], ignore_index=True)\n",
        "\n",
        "# 이상치 추가 후 다시 정규화\n",
        "X_extended_scaled = scaler.fit_transform(df_extended.drop(columns=['target']))\n",
        "df_extended_scaled = pd.DataFrame(X_extended_scaled, columns=data.feature_names)\n",
        "\n",
        "# 새로운 X, y 정의\n",
        "X = df_extended_scaled.values  # NumPy 배열로 변환\n",
        "y = df_extended['target'].values  # 타겟 값 추출\n",
        "\n",
        "# 기존 이상치 식별\n",
        "df_zscores = df_scaled.apply(zscore)  # Z-score 계산\n",
        "is_existing_outlier = (df_zscores.abs() > 3).any(axis=1)  # 절대값이 3 이상인 경우 이상치로 판단\n",
        "\n",
        "total_existing_outliers = is_existing_outlier.sum()  # 기존 이상치 수\n",
        "total_outliers = total_existing_outliers + num_outliers  # 총 이상치 수\n",
        "total_samples = len(df_extended)  # 전체 샘플 수\n",
        "total_outlier_percentage = (total_outliers / total_samples) * 100  # 이상치 비율 계산\n",
        "\n",
        "# 데이터 확인\n",
        "print(f\"확장된 데이터셋 크기: {X.shape}\")\n",
        "print(f\"확장된 타겟 크기: {y.shape}\")\n",
        "print(f\"이상치가 포함된 데이터셋의 총 샘플 수: {len(X)}\")\n",
        "print(f\"전체 이상치 비율: {total_outlier_percentage:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e1b908a-c58a-4a2f-c737-9c797dd8c239",
        "id": "Z02LOEixu-Ks"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "확장된 데이터셋 크기: (868, 18)\n",
            "확장된 타겟 크기: (868,)\n",
            "이상치가 포함된 데이터셋의 총 샘플 수: 868\n",
            "전체 이상치 비율: 5.07%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import load_iris\n",
        "from scipy.stats import zscore\n",
        "import random\n",
        "\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# 데이터셋 로딩\n",
        "data = fetch_openml(name='vehicle', version=1)\n",
        "X = data.data\n",
        "# Use LabelEncoder to convert categorical target to numerical\n",
        "encoder = LabelEncoder()\n",
        "y = encoder.fit_transform(data.target)\n",
        "df = pd.DataFrame(X, columns=data.feature_names)\n",
        "df['target'] = y  # 타겟 변수 추가\n",
        "\n",
        "\n",
        "# 데이터 정규화\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "df_scaled = pd.DataFrame(X_scaled, columns=data.feature_names)\n",
        "\n",
        "# 이상치 생성\n",
        "num_outliers = int(0.027 * len(df))  # 이상치를 설정\n",
        "outliers = []\n",
        "\n",
        "for _ in range(num_outliers):\n",
        "    outlier_sample = df_scaled.sample(n=1).copy()  # 임의의 샘플 선택\n",
        "    for col in df_scaled.columns:\n",
        "        if np.random.rand() > 0.5:\n",
        "            # 최대값보다 큰 이상치 생성\n",
        "            outlier_sample[col] = df_scaled[col].max() + np.random.rand() * 3\n",
        "        else:\n",
        "            # 최소값보다 작은 이상치 생성\n",
        "            outlier_sample[col] = df_scaled[col].min() - np.random.rand() * 3\n",
        "    outliers.append(outlier_sample)\n",
        "\n",
        "outliers_df = pd.concat(outliers, ignore_index=True)\n",
        "\n",
        "# 이상치의 타겟 값 랜덤 할당\n",
        "outliers_df['target'] = np.random.choice(y, num_outliers)\n",
        "\n",
        "# 원본 데이터에 이상치 추가\n",
        "df_extended = pd.concat([df, outliers_df], ignore_index=True)\n",
        "\n",
        "# 이상치 추가 후 다시 정규화\n",
        "X_extended_scaled = scaler.fit_transform(df_extended.drop(columns=['target']))\n",
        "df_extended_scaled = pd.DataFrame(X_extended_scaled, columns=data.feature_names)\n",
        "\n",
        "# 새로운 X, y 정의\n",
        "X = df_extended_scaled.values  # NumPy 배열로 변환\n",
        "y = df_extended['target'].values  # 타겟 값 추출\n",
        "\n",
        "# 기존 이상치 식별\n",
        "df_zscores = df_scaled.apply(zscore)  # Z-score 계산\n",
        "is_existing_outlier = (df_zscores.abs() > 3).any(axis=1)  # 절대값이 3 이상인 경우 이상치로 판단\n",
        "\n",
        "total_existing_outliers = is_existing_outlier.sum()  # 기존 이상치 수\n",
        "total_outliers = total_existing_outliers + num_outliers  # 총 이상치 수\n",
        "total_samples = len(df_extended)  # 전체 샘플 수\n",
        "total_outlier_percentage = (total_outliers / total_samples) * 100  # 이상치 비율 계산\n",
        "\n",
        "# 데이터 확인\n",
        "print(f\"확장된 데이터셋 크기: {X.shape}\")\n",
        "print(f\"확장된 타겟 크기: {y.shape}\")\n",
        "print(f\"이상치가 포함된 데이터셋의 총 샘플 수: {len(X)}\")\n",
        "print(f\"전체 이상치 비율: {total_outlier_percentage:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66d47058-490e-4a72-8cca-2233519b11b5",
        "id": "9ML-dqECu-96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "확장된 데이터셋 크기: (916, 18)\n",
            "확장된 타겟 크기: (916,)\n",
            "이상치가 포함된 데이터셋의 총 샘플 수: 916\n",
            "전체 이상치 비율: 10.04%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import load_iris\n",
        "from scipy.stats import zscore\n",
        "import random\n",
        "\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# 데이터셋 로딩\n",
        "data = fetch_openml(name='vehicle', version=1)\n",
        "X = data.data\n",
        "# Use LabelEncoder to convert categorical target to numerical\n",
        "encoder = LabelEncoder()\n",
        "y = encoder.fit_transform(data.target)\n",
        "df = pd.DataFrame(X, columns=data.feature_names)\n",
        "df['target'] = y  # 타겟 변수 추가\n",
        "\n",
        "\n",
        "# 데이터 정규화\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "df_scaled = pd.DataFrame(X_scaled, columns=data.feature_names)\n",
        "\n",
        "# 이상치 생성\n",
        "num_outliers = int(0.083 * len(df))  # 이상치를 설정\n",
        "outliers = []\n",
        "\n",
        "for _ in range(num_outliers):\n",
        "    outlier_sample = df_scaled.sample(n=1).copy()  # 임의의 샘플 선택\n",
        "    for col in df_scaled.columns:\n",
        "        if np.random.rand() > 0.5:\n",
        "            # 최대값보다 큰 이상치 생성\n",
        "            outlier_sample[col] = df_scaled[col].max() + np.random.rand() * 3\n",
        "        else:\n",
        "            # 최소값보다 작은 이상치 생성\n",
        "            outlier_sample[col] = df_scaled[col].min() - np.random.rand() * 3\n",
        "    outliers.append(outlier_sample)\n",
        "\n",
        "outliers_df = pd.concat(outliers, ignore_index=True)\n",
        "\n",
        "# 이상치의 타겟 값 랜덤 할당\n",
        "outliers_df['target'] = np.random.choice(y, num_outliers)\n",
        "\n",
        "# 원본 데이터에 이상치 추가\n",
        "df_extended = pd.concat([df, outliers_df], ignore_index=True)\n",
        "\n",
        "# 이상치 추가 후 다시 정규화\n",
        "X_extended_scaled = scaler.fit_transform(df_extended.drop(columns=['target']))\n",
        "df_extended_scaled = pd.DataFrame(X_extended_scaled, columns=data.feature_names)\n",
        "\n",
        "# 새로운 X, y 정의\n",
        "X = df_extended_scaled.values  # NumPy 배열로 변환\n",
        "y = df_extended['target'].values  # 타겟 값 추출\n",
        "\n",
        "# 기존 이상치 식별\n",
        "df_zscores = df_scaled.apply(zscore)  # Z-score 계산\n",
        "is_existing_outlier = (df_zscores.abs() > 3).any(axis=1)  # 절대값이 3 이상인 경우 이상치로 판단\n",
        "\n",
        "total_existing_outliers = is_existing_outlier.sum()  # 기존 이상치 수\n",
        "total_outliers = total_existing_outliers + num_outliers  # 총 이상치 수\n",
        "total_samples = len(df_extended)  # 전체 샘플 수\n",
        "total_outlier_percentage = (total_outliers / total_samples) * 100  # 이상치 비율 계산\n",
        "\n",
        "# 데이터 확인\n",
        "print(f\"확장된 데이터셋 크기: {X.shape}\")\n",
        "print(f\"확장된 타겟 크기: {y.shape}\")\n",
        "print(f\"이상치가 포함된 데이터셋의 총 샘플 수: {len(X)}\")\n",
        "print(f\"전체 이상치 비율: {total_outlier_percentage:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3gtvpWugbVz",
        "outputId": "ebf8161d-9acc-476d-e802-99742d1f28c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[88.64, 88.08, 89.62, 89.48, 89.34, 87.66]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from itertools import combinations\n",
        "\n",
        "results = []\n",
        "\n",
        "# 데이터 전처리 및 분할\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=40, stratify=y )\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# OvO 방식을 사용하여 각 SVM 유형에 대해 학습 및 평가\n",
        "for svm_type in ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']:\n",
        "    models_ovo = train_ovo(X_train, y_train, C=1.0, svm_type=svm_type)\n",
        "    y_pred_ovo = predict_ovo(X_test, models_ovo)\n",
        "\n",
        "    # y_pred_ovo 변수를 정의한 후에 데이터 타입 확인 및 변환\n",
        "    if y_test.dtype != np.int64:\n",
        "        y_test = y_test.astype(int)\n",
        "\n",
        "    if y_pred_ovo.dtype != np.int64:\n",
        "        y_pred_ovo = y_pred_ovo.astype(int)\n",
        "\n",
        "    accuracy_ovo = accuracy_score(y_test, y_pred_ovo)\n",
        "    results.append((svm_type, round(accuracy_ovo * 100, 2)))\n",
        "\n",
        "# 결과를 DataFrame으로 변환 및 출력\n",
        "results_df = pd.DataFrame(results, columns=['SVM Type', 'Accuracy'])\n",
        "print(results_df['Accuracy'].tolist())\n",
        "[87.66, 86.68, 88.5, 87.24, 88.92, 88.78]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "accuracy_data = {\n",
        "    '3.06%' : [25.49, 25.49, 78.43, 78.04, 78.82, 78.43],\n",
        "    '5.07%' : [73.56, 77.01, 73.95, 73.56, 73.18, 72.80], #해야함\n",
        "    '10.04%' : [70.18, 72.36, 70.91, 70.91, 65.82, 69.45] #해야함\n",
        "}\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Vehicle data (SMO)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)\n"
      ],
      "metadata": {
        "id": "_Dr9Mx_CwyUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZtNBkb7gbVz",
        "outputId": "b24756f1-2968-429e-ad9e-a8f5e90fd307"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                         3.06%  5.07%  10.04%\n",
            "Vehicle data (L-BFGS-B)                      \n",
            "L1                       76.86  73.56   70.18\n",
            "L2                       78.43  77.01   72.36\n",
            "Fair                     77.65  73.95   70.91\n",
            "Cauchy                   76.86  73.56   70.91\n",
            "Welsch                   72.55  73.18   65.82\n",
            "Geman-McClure            74.51  72.80   69.45\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "accuracy_data = {\n",
        "    '3.06%' : [76.86, 78.43, 77.65, 76.86, 72.55, 74.51],\n",
        "    '5.07%' : [73.56, 77.01, 73.95, 73.56, 73.18, 72.80],\n",
        "    '10.04%' : [70.18, 72.36, 70.91, 70.91, 65.82, 69.45]\n",
        "}\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Vehicle data (L-BFGS-B)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "accuracy_data = {\n",
        "    '3.06%' : [50.98, 47.84, 56.08, 47.06, 46.67, 61.57],\n",
        "    '5.07%' : [49.04, 55.56, 51.72, 51.34, 49.04, 56.32],\n",
        "    '10.04%' : [51.27, 42.91, 45.09, 53.45, 51.27, 45.09]\n",
        "}\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Vehicle data (GA)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xg6AHmyGwRR3",
        "outputId": "4b423b7c-8a60-478e-8f7a-42128c0e3c55"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   3.06%  5.07%  10.04%\n",
            "Vehicle data (GA)                      \n",
            "L1                 50.98  49.04   51.27\n",
            "L2                 47.84  55.56   42.91\n",
            "Fair               56.08  51.72   45.09\n",
            "Cauchy             47.06  51.34   53.45\n",
            "Welsch             46.67  49.04   51.27\n",
            "Geman-McClure      61.57  56.32   45.09\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "accuracy_data = {\n",
        "    '3.06%' : [71.37, 69.02, 70.2, 72.55, 71.37, 70.59],\n",
        "    '5.07%' : [67.82, 65.52, 67.82, 66.28, 68.97, 73.95],\n",
        "    '10.04%' : [63.64, 65.09, 63.64, 67.64, 66.91, 65.82]\n",
        "}\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Vehicle data (PSO)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cp_9j6JpwyQT",
        "outputId": "94a47cd0-8944-47cf-ddbb-23a53722b26e"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                    3.06%  5.07%  10.04%\n",
            "Vehicle data (PSO)                      \n",
            "L1                  71.37  67.82   63.64\n",
            "L2                  69.02  65.52   65.09\n",
            "Fair                70.20  67.82   63.64\n",
            "Cauchy              72.55  66.28   67.64\n",
            "Welsch              71.37  68.97   66.91\n",
            "Geman-McClure       70.59  73.95   65.82\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "accuracy_data = {\n",
        "    '3.06%' : [76.86, 76.08, 75.69, 77.65, 74.90, 74.12],\n",
        "    '5.07%' : [73.56, 74.71, 72.8, 71.26, 66.28, 69.73],\n",
        "    '10.04%' : [68.73, 70.18, 69.45, 70.55, 69.09, 69.82]\n",
        "}\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Vehicle data (ACO)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBKawSBxwyLt",
        "outputId": "264b318f-fcfc-4765-83c4-992fd10f3de4"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                    3.06%  5.07%  10.04%\n",
            "Vehicle data (ACO)                      \n",
            "L1                  76.86  73.56   68.73\n",
            "L2                  76.08  74.71   70.18\n",
            "Fair                75.69  72.80   69.45\n",
            "Cauchy              77.65  71.26   70.55\n",
            "Welsch              74.90  66.28   69.09\n",
            "Geman-McClure       74.12  69.73   69.82\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "accuracy_data = {\n",
        "    '3.06%' : [64.31, 49.41, 47.06, 59.61, 64.71, 64.31],\n",
        "    '5.07%' : [50.57, 41.00, 55.56, 63.60, 59.00, 62.45],\n",
        "    '10.04%' : [41.45, 43.27, 42.18, 53.82, 51.64, 56.00]\n",
        "}\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Vehicle data (HS)'\n",
        "# 데이터프레임 출력\n",
        "print(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tvK_QY8owyC7",
        "outputId": "a258e7cd-3461-4094-b1f4-ce7158a5903f"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   3.06%  5.07%  10.04%\n",
            "Vehicle data (HS)                      \n",
            "L1                 64.31  50.57   41.45\n",
            "L2                 49.41  41.00   43.27\n",
            "Fair               47.06  55.56   42.18\n",
            "Cauchy             59.61  63.60   53.82\n",
            "Welsch             64.71  59.00   51.64\n",
            "Geman-McClure      64.31  62.45   56.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lzo1BW9Q7BHi"
      },
      "source": [
        "#**OvR with various outliers**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nej9DwNY7MDY"
      },
      "source": [
        "###iris data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "goD2hIQQ7MDZ",
        "outputId": "d70182d8-061d-4e75-9068-0c1c3d4c1c99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "기존 이상치 수: 1\n",
            "기존 이상치 비율: 0.67%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.stats import zscore\n",
        "\n",
        "# 데이터셋 로딩\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "df = pd.DataFrame(X, columns=data.feature_names)\n",
        "df['target'] = y  # 타겟 변수 추가\n",
        "\n",
        "# 데이터 정규화\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "df_scaled = pd.DataFrame(X_scaled, columns=data.feature_names)\n",
        "\n",
        "# 기존 이상치 식별\n",
        "df_zscores = df_scaled.apply(zscore)  # Z-score 계산\n",
        "is_existing_outlier = (df_zscores.abs() > 3).any(axis=1)  # 절대값이 3 이상인 경우 이상치로 판단\n",
        "\n",
        "# 기존 이상치 수 계산\n",
        "total_existing_outliers = is_existing_outlier.sum()\n",
        "\n",
        "# 기존 이상치 비율 계산\n",
        "total_samples = len(df)\n",
        "existing_outlier_percentage = (total_existing_outliers / total_samples) * 100\n",
        "\n",
        "print(f\"기존 이상치 수: {total_existing_outliers}\")\n",
        "print(f\"기존 이상치 비율: {existing_outlier_percentage:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m32_LkFN7MDZ",
        "outputId": "ffab61c5-725e-4919-a8c5-a15a2de2ad04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "확장된 데이터셋 크기: (154, 4)\n",
            "확장된 타겟 크기: (154,)\n",
            "이상치가 포함된 데이터셋의 총 샘플 수: 154\n",
            "전체 이상치 비율: 3.25%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import load_iris\n",
        "from scipy.stats import zscore\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# 데이터셋 로딩\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "df = pd.DataFrame(X, columns=iris.feature_names)\n",
        "df['target'] = y  # 타겟 변수 추가\n",
        "\n",
        "# 데이터 정규화\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "df_scaled = pd.DataFrame(X_scaled, columns=iris.feature_names)\n",
        "\n",
        "# 이상치 생성\n",
        "num_outliers = int(0.03 * len(df))  # 이상치를 설정\n",
        "outliers = []\n",
        "\n",
        "for _ in range(num_outliers):\n",
        "    outlier_sample = df_scaled.sample(n=1).copy()  # 임의의 샘플 선택\n",
        "    for col in df_scaled.columns:\n",
        "        if np.random.rand() > 0.5:\n",
        "            # 최대값보다 큰 이상치 생성\n",
        "            outlier_sample[col] = df_scaled[col].max() + np.random.rand() * 3\n",
        "        else:\n",
        "            # 최소값보다 작은 이상치 생성\n",
        "            outlier_sample[col] = df_scaled[col].min() - np.random.rand() * 3\n",
        "    outliers.append(outlier_sample)\n",
        "\n",
        "outliers_df = pd.concat(outliers, ignore_index=True)\n",
        "\n",
        "# 이상치의 타겟 값 랜덤 할당\n",
        "outliers_df['target'] = np.random.choice(y, num_outliers)\n",
        "\n",
        "# 원본 데이터에 이상치 추가\n",
        "df_extended = pd.concat([df, outliers_df], ignore_index=True)\n",
        "\n",
        "# 이상치 추가 후 다시 정규화\n",
        "X_extended_scaled = scaler.fit_transform(df_extended.drop(columns=['target']))\n",
        "df_extended_scaled = pd.DataFrame(X_extended_scaled, columns=iris.feature_names)\n",
        "\n",
        "# 새로운 X, y 정의\n",
        "X = df_extended_scaled.values  # NumPy 배열로 변환\n",
        "y = df_extended['target'].values  # 타겟 값 추출\n",
        "\n",
        "# 기존 이상치 식별\n",
        "df_zscores = df_scaled.apply(zscore)  # Z-score 계산\n",
        "is_existing_outlier = (df_zscores.abs() > 3).any(axis=1)  # 절대값이 3 이상인 경우 이상치로 판단\n",
        "\n",
        "total_existing_outliers = is_existing_outlier.sum()  # 기존 이상치 수\n",
        "total_outliers = total_existing_outliers + num_outliers  # 총 이상치 수\n",
        "total_samples = len(df_extended)  # 전체 샘플 수\n",
        "total_outlier_percentage = (total_outliers / total_samples) * 100  # 이상치 비율 계산\n",
        "\n",
        "# 데이터 확인\n",
        "print(f\"확장된 데이터셋 크기: {X.shape}\")\n",
        "print(f\"확장된 타겟 크기: {y.shape}\")\n",
        "print(f\"이상치가 포함된 데이터셋의 총 샘플 수: {len(X)}\")\n",
        "print(f\"전체 이상치 비율: {total_outlier_percentage:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eGCaXs8G7MDZ",
        "outputId": "d0136c44-97c8-4cce-c832-892d9a1a1f73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "확장된 데이터셋 크기: (157, 4)\n",
            "확장된 타겟 크기: (157,)\n",
            "이상치가 포함된 데이터셋의 총 샘플 수: 157\n",
            "전체 이상치 비율: 5.10%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import load_iris\n",
        "from scipy.stats import zscore\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# 데이터셋 로딩\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "df = pd.DataFrame(X, columns=iris.feature_names)\n",
        "df['target'] = y  # 타겟 변수 추가\n",
        "\n",
        "# 데이터 정규화\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "df_scaled = pd.DataFrame(X_scaled, columns=iris.feature_names)\n",
        "\n",
        "# 이상치 생성\n",
        "num_outliers = int(0.05 * len(df))  # 이상치를 설정\n",
        "outliers = []\n",
        "\n",
        "for _ in range(num_outliers):\n",
        "    outlier_sample = df_scaled.sample(n=1).copy()  # 임의의 샘플 선택\n",
        "    for col in df_scaled.columns:\n",
        "        if np.random.rand() > 0.5:\n",
        "            # 최대값보다 큰 이상치 생성\n",
        "            outlier_sample[col] = df_scaled[col].max() + np.random.rand() * 3\n",
        "        else:\n",
        "            # 최소값보다 작은 이상치 생성\n",
        "            outlier_sample[col] = df_scaled[col].min() - np.random.rand() * 3\n",
        "    outliers.append(outlier_sample)\n",
        "\n",
        "outliers_df = pd.concat(outliers, ignore_index=True)\n",
        "\n",
        "# 이상치의 타겟 값 랜덤 할당\n",
        "outliers_df['target'] = np.random.choice(y, num_outliers)\n",
        "\n",
        "# 원본 데이터에 이상치 추가\n",
        "df_extended = pd.concat([df, outliers_df], ignore_index=True)\n",
        "\n",
        "# 이상치 추가 후 다시 정규화\n",
        "X_extended_scaled = scaler.fit_transform(df_extended.drop(columns=['target']))\n",
        "df_extended_scaled = pd.DataFrame(X_extended_scaled, columns=iris.feature_names)\n",
        "\n",
        "# 새로운 X, y 정의\n",
        "X = df_extended_scaled.values  # NumPy 배열로 변환\n",
        "y = df_extended['target'].values  # 타겟 값 추출\n",
        "\n",
        "# 기존 이상치 식별\n",
        "df_zscores = df_scaled.apply(zscore)  # Z-score 계산\n",
        "is_existing_outlier = (df_zscores.abs() > 3).any(axis=1)  # 절대값이 3 이상인 경우 이상치로 판단\n",
        "\n",
        "total_existing_outliers = is_existing_outlier.sum()  # 기존 이상치 수\n",
        "total_outliers = total_existing_outliers + num_outliers  # 총 이상치 수\n",
        "total_samples = len(df_extended)  # 전체 샘플 수\n",
        "total_outlier_percentage = (total_outliers / total_samples) * 100  # 이상치 비율 계산\n",
        "\n",
        "# 데이터 확인\n",
        "print(f\"확장된 데이터셋 크기: {X.shape}\")\n",
        "print(f\"확장된 타겟 크기: {y.shape}\")\n",
        "print(f\"이상치가 포함된 데이터셋의 총 샘플 수: {len(X)}\")\n",
        "print(f\"전체 이상치 비율: {total_outlier_percentage:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTMSvFGS7MDa",
        "outputId": "7f82c29a-829e-4228-d6c3-6569f6fbf9df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "확장된 데이터셋 크기: (166, 4)\n",
            "확장된 타겟 크기: (166,)\n",
            "이상치가 포함된 데이터셋의 총 샘플 수: 166\n",
            "전체 이상치 비율: 10.24%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import load_iris\n",
        "from scipy.stats import zscore\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# 데이터셋 로딩\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "df = pd.DataFrame(X, columns=iris.feature_names)\n",
        "df['target'] = y  # 타겟 변수 추가\n",
        "\n",
        "# 데이터 정규화\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "df_scaled = pd.DataFrame(X_scaled, columns=iris.feature_names)\n",
        "\n",
        "# 이상치 생성\n",
        "num_outliers = int(0.11 * len(df))  # 이상치를 설정\n",
        "outliers = []\n",
        "\n",
        "for _ in range(num_outliers):\n",
        "    outlier_sample = df_scaled.sample(n=1).copy()  # 임의의 샘플 선택\n",
        "    for col in df_scaled.columns:\n",
        "        if np.random.rand() > 0.5:\n",
        "            # 최대값보다 큰 이상치 생성\n",
        "            outlier_sample[col] = df_scaled[col].max() + np.random.rand() * 3\n",
        "        else:\n",
        "            # 최소값보다 작은 이상치 생성\n",
        "            outlier_sample[col] = df_scaled[col].min() - np.random.rand() * 3\n",
        "    outliers.append(outlier_sample)\n",
        "\n",
        "outliers_df = pd.concat(outliers, ignore_index=True)\n",
        "\n",
        "# 이상치의 타겟 값 랜덤 할당\n",
        "outliers_df['target'] = np.random.choice(y, num_outliers)\n",
        "\n",
        "# 원본 데이터에 이상치 추가\n",
        "df_extended = pd.concat([df, outliers_df], ignore_index=True)\n",
        "\n",
        "# 이상치 추가 후 다시 정규화\n",
        "X_extended_scaled = scaler.fit_transform(df_extended.drop(columns=['target']))\n",
        "df_extended_scaled = pd.DataFrame(X_extended_scaled, columns=iris.feature_names)\n",
        "\n",
        "# 새로운 X, y 정의\n",
        "X = df_extended_scaled.values  # NumPy 배열로 변환\n",
        "y = df_extended['target'].values  # 타겟 값 추출\n",
        "\n",
        "# 기존 이상치 식별\n",
        "df_zscores = df_scaled.apply(zscore)  # Z-score 계산\n",
        "is_existing_outlier = (df_zscores.abs() > 3).any(axis=1)  # 절대값이 3 이상인 경우 이상치로 판단\n",
        "\n",
        "total_existing_outliers = is_existing_outlier.sum()  # 기존 이상치 수\n",
        "total_outliers = total_existing_outliers + num_outliers  # 총 이상치 수\n",
        "total_samples = len(df_extended)  # 전체 샘플 수\n",
        "total_outlier_percentage = (total_outliers / total_samples) * 100  # 이상치 비율 계산\n",
        "\n",
        "# 데이터 확인\n",
        "print(f\"확장된 데이터셋 크기: {X.shape}\")\n",
        "print(f\"확장된 타겟 크기: {y.shape}\")\n",
        "print(f\"이상치가 포함된 데이터셋의 총 샘플 수: {len(X)}\")\n",
        "print(f\"전체 이상치 비율: {total_outlier_percentage:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFbS0xHz7MDa",
        "outputId": "fcafa202-cd4f-4591-907c-8d3fedb2dd7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "        SVM Type  Accuracy\n",
            "0             L1      64.0\n",
            "1             L2      66.0\n",
            "2           Fair      64.0\n",
            "3         Cauchy      68.0\n",
            "4         Welsch      64.0\n",
            "5  Geman-McClure      64.0\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from itertools import combinations\n",
        "\n",
        "results = []\n",
        "\n",
        "# 데이터 전처리 및 분할\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# OvR 방식을 사용하여 각 SVM 유형에 대해 학습 및 평가\n",
        "for svm_type in ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']:\n",
        "    models_ovr = train_ovr(X_train, y_train, C=1.0, svm_type=svm_type)\n",
        "    y_pred_ovr = predict_ovr(X_test, models_ovr)\n",
        "\n",
        "    # y_pred_ovr 변수를 정의한 후에 데이터 타입 확인 및 변환\n",
        "    if y_test.dtype != np.int64:\n",
        "        y_test = y_test.astype(int)\n",
        "\n",
        "    if y_pred_ovr.dtype != np.int64:\n",
        "        y_pred_ovr = y_pred_ovr.astype(int)\n",
        "\n",
        "    accuracy_ovr = accuracy_score(y_test, y_pred_ovr)\n",
        "    results.append((svm_type, round(accuracy_ovr * 100, 2)))\n",
        "\n",
        "# 결과를 DataFrame으로 변환 및 출력\n",
        "results_df = pd.DataFrame(results, columns=['SVM Type', 'Accuracy'])\n",
        "print(results_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbm4I4DG7MDa",
        "outputId": "b917b5d3-212a-4092-d31a-f75e71462d7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                      3.25%  5.10%  10.24%\n",
            "Iris data (L-BFGS-B)                      \n",
            "L1                    82.98  87.50    66.0\n",
            "L2                    91.49  93.75    64.0\n",
            "Fair                  89.36  91.67    64.0\n",
            "Cauchy                85.11  89.58    76.0\n",
            "Welsch                72.34  68.75    70.0\n",
            "Geman-McClure         78.72  68.75    68.0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "accuracy_data = {\n",
        "    '3.25%' : [82.98, 91.49, 89.36, 85.11, 72.34, 78.72],\n",
        "    '5.10%' : [87.50, 93.75, 91.67, 89.58, 68.75, 68.75],\n",
        "    '10.24%' : [66.0, 64.0, 64.0, 76.0, 70.0, 68.0]\n",
        "}\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Iris data (L-BFGS-B)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijmGdE8M7MDa",
        "outputId": "9b2559c1-37fb-4030-b76c-0513f3cc4d14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                3.25%  5.10%  10.24%\n",
            "Iris data (GA)                      \n",
            "L1              74.47  70.83    64.0\n",
            "L2              80.85  93.75    76.0\n",
            "Fair            76.60  79.17    74.0\n",
            "Cauchy          76.60  72.92    64.0\n",
            "Welsch          74.47  70.83    64.0\n",
            "Geman-McClure   74.47  64.58    64.0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "outlier = ['3%', '5%', '10%']\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "\n",
        "accuracy_data = {\n",
        "    '3.25%' : [74.47, 80.85, 76.60, 76.60, 74.47, 74.47],\n",
        "    '5.10%' : [70.83, 93.75, 79.17, 72.92, 70.83, 64.58],\n",
        "    '10.24%' : [64.0, 76.0, 74.0, 64.0, 64.0, 64.0]\n",
        "}\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Iris data (GA)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgAXRK0X7MDa",
        "outputId": "24b0fabf-08b9-4630-e764-2dac2a90c866"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                 3.25%  5.10%  10.24%\n",
            "Iris data (SMO)                      \n",
            "L1               31.91  29.17    34.0\n",
            "L2               31.91  29.17    34.0\n",
            "Fair             87.23  89.58    70.0\n",
            "Cauchy           87.23  89.58    70.0\n",
            "Welsch           87.23  89.58    70.0\n",
            "Geman-McClure    87.23  89.58    70.0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "outlier = ['3%', '5%', '10%']\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "\n",
        "accuracy_data = {\n",
        "    '3.25%' : [31.91,31.91, 87.23, 87.23, 87.23, 87.23],\n",
        "    '5.10%' : [29.17, 29.17, 89.58, 89.58, 89.58, 89.58],\n",
        "    '10.24%' : [34.0, 34.0, 70.0, 70.0, 70.0, 70.0]\n",
        "}\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Iris data (SMO)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-oN83mf7MDa",
        "outputId": "c6a19c69-8bb2-4a10-a552-0c3f498c5387"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                 3.25%  5.10%  10.24%\n",
            "Iris data (PSO)                      \n",
            "L1               82.98  85.42    66.0\n",
            "L2               91.49  93.75    64.0\n",
            "Fair             89.36  91.67    64.0\n",
            "Cauchy           85.11  89.58    76.0\n",
            "Welsch           74.47  60.42    64.0\n",
            "Geman-McClure    78.72  68.75    66.0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "outlier = ['3%', '5%', '10%']\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "\n",
        "accuracy_data = {\n",
        "    '3.25%' : [82.98, 91.49, 89.36, 85.11, 74.47, 78.72],\n",
        "    '5.10%' : [85.42, 93.75, 91.67, 89.58, 60.42, 68.75],\n",
        "    '10.24%' : [66.0, 64.0, 64.0, 76.0, 64.0, 66.0]\n",
        "}\n",
        "\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Iris data (PSO)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6tC1EBl7MDa",
        "outputId": "674b8190-79e7-46fd-d649-277e5ff1bae5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                 3.25%  5.10%  10.24%\n",
            "Iris data (ACO)                      \n",
            "L1               82.98  87.50    66.0\n",
            "L2               91.49  93.75    64.0\n",
            "Fair             87.23  91.67    64.0\n",
            "Cauchy           85.11  89.58    80.0\n",
            "Welsch           74.47  66.67    64.0\n",
            "Geman-McClure    78.72  72.92    66.0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "outlier = ['3%', '5%', '10%']\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "\n",
        "accuracy_data = {\n",
        "    '3.25%' : [82.98, 91.49, 87.23, 85.11, 74.47, 78.72],\n",
        "    '5.10%' : [87.50, 93.75, 91.67, 89.58, 66.67, 72.92],\n",
        "    '10.24%' : [66.0, 64.0, 64.0, 80.0, 64.0, 66.0]\n",
        "}\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Iris data (ACO)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1N5Fb6YC7MDa",
        "outputId": "6c481513-c596-4ffb-d77b-705df732e32f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                3.25%  5.10%  10.24%\n",
            "Iris data (HS)                      \n",
            "L1              91.49  78.72    64.0\n",
            "L2              87.23  85.11    66.0\n",
            "Fair            95.74  80.85    64.0\n",
            "Cauchy          95.74  80.85    68.0\n",
            "Welsch          91.49  74.47    64.0\n",
            "Geman-McClure   89.36  65.96    64.0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "accuracy_data = {\n",
        "    '3.25%' : [91.49, 87.23, 95.74, 95.74, 91.49, 89.36],\n",
        "    '5.10%' : [78.72, 85.11, 80.85, 80.85, 74.47, 65.96],\n",
        "    '10.24%' : [64.0, 66.0, 64.0, 68.0, 64.0, 64.0]\n",
        "}\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Iris data (HS)'\n",
        "\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md1-hHosjg5d"
      },
      "source": [
        "###segment data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qgOOCoyujg5e",
        "outputId": "0e5ca54e-4289-4734-abcd-36f4a7f9f5e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "기존 이상치 수: 175\n",
            "기존 이상치 비율: 7.58%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from scipy.stats import zscore\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# 데이터셋 로딩\n",
        "data = fetch_openml(name='segment', version=1)\n",
        "X = data.data\n",
        "# Use LabelEncoder to convert categorical target to numerical\n",
        "encoder = LabelEncoder()\n",
        "y = encoder.fit_transform(data.target)\n",
        "df = pd.DataFrame(X, columns=data.feature_names)\n",
        "df['target'] = y  # 타겟 변수 추가\n",
        "\n",
        "# 데이터 정규화\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "df_scaled = pd.DataFrame(X_scaled, columns=data.feature_names)\n",
        "\n",
        "# 기존 이상치 식별\n",
        "df_zscores = df_scaled.apply(zscore)  # Z-score 계산\n",
        "is_existing_outlier = (df_zscores.abs() > 3).any(axis=1)  # 절대값이 3 이상인 경우 이상치로 판단\n",
        "\n",
        "# 기존 이상치 수 계산\n",
        "total_existing_outliers = is_existing_outlier.sum()\n",
        "\n",
        "# 기존 이상치 비율 계산\n",
        "total_samples = len(df)\n",
        "existing_outlier_percentage = (total_existing_outliers / total_samples) * 100\n",
        "\n",
        "print(f\"기존 이상치 수: {total_existing_outliers}\")\n",
        "print(f\"기존 이상치 비율: {existing_outlier_percentage:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 190,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kj2xTmcpjg5f",
        "outputId": "0c4e9bc5-ab8a-4e1d-b5ca-0ed472b2954b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "확장된 데이터셋 크기: (2374, 19)\n",
            "확장된 타겟 크기: (2374,)\n",
            "이상치가 포함된 데이터셋의 총 샘플 수: 2374\n",
            "전체 이상치 비율: 10.07%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from scipy.stats import zscore\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# 데이터셋 로딩\n",
        "data = fetch_openml(name='segment', version=1)\n",
        "X = data.data\n",
        "# Use LabelEncoder to convert categorical target to numerical\n",
        "encoder = LabelEncoder()\n",
        "y = encoder.fit_transform(data.target)\n",
        "df = pd.DataFrame(X, columns=data.feature_names)\n",
        "df['target'] = y  # 타겟 변수 추가\n",
        "\n",
        "# 데이터 정규화\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "df_scaled = pd.DataFrame(X_scaled, columns=data.feature_names)\n",
        "\n",
        "# 이상치 생성\n",
        "num_outliers = int(0.028 * len(df))  # 이상치를 설정\n",
        "outliers = []\n",
        "\n",
        "for _ in range(num_outliers):\n",
        "    outlier_sample = df_scaled.sample(n=1).copy()  # 임의의 샘플 선택\n",
        "    for col in df_scaled.columns:\n",
        "        if np.random.rand() > 0.5:\n",
        "            # 최대값보다 큰 이상치 생성\n",
        "            outlier_sample[col] = df_scaled[col].max() + np.random.rand() * 3\n",
        "        else:\n",
        "            # 최소값보다 작은 이상치 생성\n",
        "            outlier_sample[col] = df_scaled[col].min() - np.random.rand() * 3\n",
        "    outliers.append(outlier_sample)\n",
        "\n",
        "outliers_df = pd.concat(outliers, ignore_index=True)\n",
        "\n",
        "# 이상치의 타겟 값 랜덤 할당\n",
        "outliers_df['target'] = np.random.choice(y, num_outliers)\n",
        "\n",
        "# 원본 데이터에 이상치 추가\n",
        "df_extended = pd.concat([df, outliers_df], ignore_index=True)\n",
        "\n",
        "# 이상치 추가 후 다시 정규화\n",
        "X_extended_scaled = scaler.fit_transform(df_extended.drop(columns=['target']))\n",
        "df_extended_scaled = pd.DataFrame(X_extended_scaled, columns=data.feature_names)\n",
        "\n",
        "# 새로운 X, y 정의\n",
        "X = df_extended_scaled.values  # NumPy 배열로 변환\n",
        "y = df_extended['target'].values  # 타겟 값 추출\n",
        "\n",
        "# 기존 이상치 식별\n",
        "df_zscores = df_scaled.apply(zscore)  # Z-score 계산\n",
        "is_existing_outlier = (df_zscores.abs() > 3).any(axis=1)  # 절대값이 3 이상인 경우 이상치로 판단\n",
        "\n",
        "total_existing_outliers = is_existing_outlier.sum()  # 기존 이상치 수\n",
        "total_outliers = total_existing_outliers + num_outliers  # 총 이상치 수\n",
        "total_samples = len(df_extended)  # 전체 샘플 수\n",
        "total_outlier_percentage = (total_outliers / total_samples) * 100  # 이상치 비율 계산\n",
        "\n",
        "# 데이터 확인\n",
        "print(f\"확장된 데이터셋 크기: {X.shape}\")\n",
        "print(f\"확장된 타겟 크기: {y.shape}\")\n",
        "print(f\"이상치가 포함된 데이터셋의 총 샘플 수: {len(X)}\")\n",
        "print(f\"전체 이상치 비율: {total_outlier_percentage:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 193,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hn2ORtZHjg5h",
        "outputId": "a62a0f3b-b99a-4de1-d0fc-278a14e9bce9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        SVM Type  Accuracy\n",
            "0             L1     67.18\n",
            "1             L2     50.91\n",
            "2           Fair     60.59\n",
            "3         Cauchy     68.86\n",
            "4         Welsch     63.25\n",
            "5  Geman-McClure     64.66\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from itertools import combinations\n",
        "\n",
        "results = []\n",
        "\n",
        "# 데이터 전처리 및 분할\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# OvR 방식을 사용하여 각 SVM 유형에 대해 학습 및 평가\n",
        "for svm_type in ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']:\n",
        "    models_ovr = train_ovr(X_train, y_train, C=1.0, svm_type=svm_type)\n",
        "    y_pred_ovr = predict_ovr(X_test, models_ovr)\n",
        "\n",
        "    # y_pred_ovr 변수를 정의한 후에 데이터 타입 확인 및 변환\n",
        "    if y_test.dtype != np.int64:\n",
        "        y_test = y_test.astype(int)\n",
        "\n",
        "    if y_pred_ovr.dtype != np.int64:\n",
        "        y_pred_ovr = y_pred_ovr.astype(int)\n",
        "\n",
        "    accuracy_ovr = accuracy_score(y_test, y_pred_ovr)\n",
        "    results.append((svm_type, round(accuracy_ovr * 100, 2)))\n",
        "\n",
        "# 결과를 DataFrame으로 변환 및 출력\n",
        "results_df = pd.DataFrame(results, columns=['SVM Type', 'Accuracy'])\n",
        "print(results_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLFQmBIajg5h",
        "outputId": "667afc08-0cee-4174-82da-63a5d8f5fa25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                         7.58%  10.07%\n",
            "Segment data (L-BFGS-B)               \n",
            "L1                       90.91   89.06\n",
            "L2                       90.76   89.76\n",
            "Fair                     90.19   89.48\n",
            "Cauchy                   90.76   90.32\n",
            "Welsch                   89.47   84.85\n",
            "Geman-McClure            89.47   88.22\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "accuracy_data = {\n",
        "    '7.58%' : [90.91, 90.76, 90.19, 90.76, 89.47, 89.47],\n",
        "    '10.07%' : [89.06, 89.76, 89.48, 90.32, 84.85, 88.22]\n",
        "}\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Segment data (L-BFGS-B)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffLYnM-9kR1o",
        "outputId": "d1253e81-dcde-445b-82e2-fd332ee6a819"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   7.58%  10.07%\n",
            "Segment data (GA)               \n",
            "L1                 69.99   75.60\n",
            "L2                 70.56   68.44\n",
            "Fair               76.19   66.06\n",
            "Cauchy             57.14   64.38\n",
            "Welsch             36.36   53.58\n",
            "Geman-McClure      52.67   67.32\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "accuracy_data = {\n",
        "    '7.58%' : [69.99, 70.56, 76.19, 57.14, 36.36, 52.67],\n",
        "    '10.07%' : [75.60, 68.44, 66.06, 64.38, 53.58, 67.32]\n",
        "}\n",
        "\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Segment data (GA)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QosEv8iXkTTx",
        "outputId": "53bbd8f2-b684-4ab0-88b6-63e4beb50f3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                    7.58%  10.07%\n",
            "Segment data (SMO)               \n",
            "L1                  12.41   14.87\n",
            "L2                  12.41   14.87\n",
            "Fair                91.05   91.02\n",
            "Cauchy              90.62   90.46\n",
            "Welsch              90.76   91.16\n",
            "Geman-McClure       91.05   90.74\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "accuracy_data = {\n",
        "    '7.58%' : [12.41, 12.41, 91.05, 90.62, 90.76, 91.05],\n",
        "    '10.07%' : [14.87, 14.87, 91.02, 90.46, 91.16, 90.74]\n",
        "}\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Segment data (SMO)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNMupjuskTxm",
        "outputId": "3e17ac34-08fd-4223-9323-30fa6be17344"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                    7.58%  10.07%\n",
            "Segment data (PSO)               \n",
            "L1                  85.71   82.75\n",
            "L2                  87.88   83.31\n",
            "Fair                87.59   85.41\n",
            "Cauchy              83.12   86.96\n",
            "Welsch              71.57   76.72\n",
            "Geman-McClure       83.84   74.19\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "accuracy_data = {\n",
        "    '7.58%' : [85.71, 87.88, 87.59, 83.12, 71.57, 83.84],\n",
        "    '10.07%' : [82.75, 83.31, 85.41, 86.96, 76.72, 74.19]\n",
        "}\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Segment data (PSO)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FifTK7GkUFz",
        "outputId": "ab71df5a-243a-4ab1-b3de-f58573fa35b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                    7.58%  10.07%\n",
            "Segment data (ACO)               \n",
            "L1                  89.75   88.36\n",
            "L2                  90.04   89.48\n",
            "Fair                90.62   88.50\n",
            "Cauchy              89.18   88.78\n",
            "Welsch              89.03   84.29\n",
            "Geman-McClure       86.72   86.82\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "accuracy_data = {\n",
        "    '7.58%' : [89.75, 90.04, 90.62, 89.18, 89.03, 86.72],\n",
        "    '10.07%' : [88.36, 89.48, 88.50, 88.78, 84.29, 86.82]\n",
        "}\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Segment data (ACO)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aulCqMJJkTox",
        "outputId": "c3c9c209-895b-4eb7-b733-9527f76f678b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   7.58%  10.07%\n",
            "Segment data (HS)               \n",
            "L1                 68.25   67.18\n",
            "L2                 54.83   50.91\n",
            "Fair               61.04   60.59\n",
            "Cauchy             55.56   68.86\n",
            "Welsch             55.12   63.25\n",
            "Geman-McClure      53.54   64.66\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "accuracy_data = {\n",
        "    '7.58%' : [68.25, 54.83, 61.04, 55.56, 55.12, 53.54],\n",
        "    '10.07%' : [67.18, 50.91, 60.59, 68.86, 63.25, 64.66]\n",
        "}\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Segment data (HS)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJhvImfn1ZJ8"
      },
      "source": [
        "### vehicle data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be2a0065-5e0c-4db3-adb2-35b7fcb6e10d",
        "id": "SjGg0iOh1ZJ9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "기존 이상치 수: 22\n",
            "기존 이상치 비율: 2.60%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from scipy.stats import zscore\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# 데이터셋 로딩\n",
        "data = fetch_openml(name='vehicle', version=1)\n",
        "X = data.data\n",
        "encoder = LabelEncoder()\n",
        "y = encoder.fit_transform(data.target)\n",
        "df = pd.DataFrame(X, columns=data.feature_names)\n",
        "df['target'] = y  # 타겟 변수 추가\n",
        "\n",
        "# 데이터 정규화\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "df_scaled = pd.DataFrame(X_scaled, columns=data.feature_names)\n",
        "\n",
        "# 기존 이상치 식별\n",
        "df_zscores = df_scaled.apply(zscore)  # Z-score 계산\n",
        "is_existing_outlier = (df_zscores.abs() > 3).any(axis=1)  # 절대값이 3 이상인 경우 이상치로 판단\n",
        "\n",
        "# 기존 이상치 수 계산\n",
        "total_existing_outliers = is_existing_outlier.sum()\n",
        "\n",
        "# 기존 이상치 비율 계산\n",
        "total_samples = len(df)\n",
        "existing_outlier_percentage = (total_existing_outliers / total_samples) * 100\n",
        "\n",
        "print(f\"기존 이상치 수: {total_existing_outliers}\")\n",
        "print(f\"기존 이상치 비율: {existing_outlier_percentage:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12ef8abd-8af0-4d47-98ca-5ac99f48b72e",
        "id": "GNTYqkpF1ZJ-"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "확장된 데이터셋 크기: (850, 18)\n",
            "확장된 타겟 크기: (850,)\n",
            "이상치가 포함된 데이터셋의 총 샘플 수: 850\n",
            "전체 이상치 비율: 3.06%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import load_iris\n",
        "from scipy.stats import zscore\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# 데이터셋 로딩\n",
        "data = fetch_openml(name='vehicle', version=1)\n",
        "X = data.data\n",
        "# Use LabelEncoder to convert categorical target to numerical\n",
        "encoder = LabelEncoder()\n",
        "y = encoder.fit_transform(data.target)\n",
        "df = pd.DataFrame(X, columns=data.feature_names)\n",
        "df['target'] = y  # 타겟 변수 추가\n",
        "\n",
        "\n",
        "# 데이터 정규화\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "df_scaled = pd.DataFrame(X_scaled, columns=data.feature_names)\n",
        "\n",
        "# 이상치 생성\n",
        "num_outliers = int(0.0048 * len(df))  # 이상치를 설정\n",
        "outliers = []\n",
        "\n",
        "for _ in range(num_outliers):\n",
        "    outlier_sample = df_scaled.sample(n=1).copy()  # 임의의 샘플 선택\n",
        "    for col in df_scaled.columns:\n",
        "        if np.random.rand() > 0.5:\n",
        "            # 최대값보다 큰 이상치 생성\n",
        "            outlier_sample[col] = df_scaled[col].max() + np.random.rand() * 3\n",
        "        else:\n",
        "            # 최소값보다 작은 이상치 생성\n",
        "            outlier_sample[col] = df_scaled[col].min() - np.random.rand() * 3\n",
        "    outliers.append(outlier_sample)\n",
        "\n",
        "outliers_df = pd.concat(outliers, ignore_index=True)\n",
        "\n",
        "# 이상치의 타겟 값 랜덤 할당\n",
        "outliers_df['target'] = np.random.choice(y, num_outliers)\n",
        "\n",
        "# 원본 데이터에 이상치 추가\n",
        "df_extended = pd.concat([df, outliers_df], ignore_index=True)\n",
        "\n",
        "# 이상치 추가 후 다시 정규화\n",
        "X_extended_scaled = scaler.fit_transform(df_extended.drop(columns=['target']))\n",
        "df_extended_scaled = pd.DataFrame(X_extended_scaled, columns=data.feature_names)\n",
        "\n",
        "# 새로운 X, y 정의\n",
        "X = df_extended_scaled.values  # NumPy 배열로 변환\n",
        "y = df_extended['target'].values  # 타겟 값 추출\n",
        "\n",
        "# 기존 이상치 식별\n",
        "df_zscores = df_scaled.apply(zscore)  # Z-score 계산\n",
        "is_existing_outlier = (df_zscores.abs() > 3).any(axis=1)  # 절대값이 3 이상인 경우 이상치로 판단\n",
        "\n",
        "total_existing_outliers = is_existing_outlier.sum()  # 기존 이상치 수\n",
        "total_outliers = total_existing_outliers + num_outliers  # 총 이상치 수\n",
        "total_samples = len(df_extended)  # 전체 샘플 수\n",
        "total_outlier_percentage = (total_outliers / total_samples) * 100  # 이상치 비율 계산\n",
        "\n",
        "# 데이터 확인\n",
        "print(f\"확장된 데이터셋 크기: {X.shape}\")\n",
        "print(f\"확장된 타겟 크기: {y.shape}\")\n",
        "print(f\"이상치가 포함된 데이터셋의 총 샘플 수: {len(X)}\")\n",
        "print(f\"전체 이상치 비율: {total_outlier_percentage:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b618394c-29ef-4a8a-86e6-f2ca8d86e99f",
        "id": "m12YFMRB1ZJ-"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "확장된 데이터셋 크기: (868, 18)\n",
            "확장된 타겟 크기: (868,)\n",
            "이상치가 포함된 데이터셋의 총 샘플 수: 868\n",
            "전체 이상치 비율: 5.07%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import load_iris\n",
        "from scipy.stats import zscore\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# 데이터셋 로딩\n",
        "data = fetch_openml(name='vehicle', version=1)\n",
        "X = data.data\n",
        "# Use LabelEncoder to convert categorical target to numerical\n",
        "encoder = LabelEncoder()\n",
        "y = encoder.fit_transform(data.target)\n",
        "df = pd.DataFrame(X, columns=data.feature_names)\n",
        "df['target'] = y  # 타겟 변수 추가\n",
        "\n",
        "\n",
        "# 데이터 정규화\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "df_scaled = pd.DataFrame(X_scaled, columns=data.feature_names)\n",
        "\n",
        "# 이상치 생성\n",
        "num_outliers = int(0.027 * len(df))  # 이상치를 설정\n",
        "outliers = []\n",
        "\n",
        "for _ in range(num_outliers):\n",
        "    outlier_sample = df_scaled.sample(n=1).copy()  # 임의의 샘플 선택\n",
        "    for col in df_scaled.columns:\n",
        "        if np.random.rand() > 0.5:\n",
        "            # 최대값보다 큰 이상치 생성\n",
        "            outlier_sample[col] = df_scaled[col].max() + np.random.rand() * 3\n",
        "        else:\n",
        "            # 최소값보다 작은 이상치 생성\n",
        "            outlier_sample[col] = df_scaled[col].min() - np.random.rand() * 3\n",
        "    outliers.append(outlier_sample)\n",
        "\n",
        "outliers_df = pd.concat(outliers, ignore_index=True)\n",
        "\n",
        "# 이상치의 타겟 값 랜덤 할당\n",
        "outliers_df['target'] = np.random.choice(y, num_outliers)\n",
        "\n",
        "# 원본 데이터에 이상치 추가\n",
        "df_extended = pd.concat([df, outliers_df], ignore_index=True)\n",
        "\n",
        "# 이상치 추가 후 다시 정규화\n",
        "X_extended_scaled = scaler.fit_transform(df_extended.drop(columns=['target']))\n",
        "df_extended_scaled = pd.DataFrame(X_extended_scaled, columns=data.feature_names)\n",
        "\n",
        "# 새로운 X, y 정의\n",
        "X = df_extended_scaled.values  # NumPy 배열로 변환\n",
        "y = df_extended['target'].values  # 타겟 값 추출\n",
        "\n",
        "# 기존 이상치 식별\n",
        "df_zscores = df_scaled.apply(zscore)  # Z-score 계산\n",
        "is_existing_outlier = (df_zscores.abs() > 3).any(axis=1)  # 절대값이 3 이상인 경우 이상치로 판단\n",
        "\n",
        "total_existing_outliers = is_existing_outlier.sum()  # 기존 이상치 수\n",
        "total_outliers = total_existing_outliers + num_outliers  # 총 이상치 수\n",
        "total_samples = len(df_extended)  # 전체 샘플 수\n",
        "total_outlier_percentage = (total_outliers / total_samples) * 100  # 이상치 비율 계산\n",
        "\n",
        "# 데이터 확인\n",
        "print(f\"확장된 데이터셋 크기: {X.shape}\")\n",
        "print(f\"확장된 타겟 크기: {y.shape}\")\n",
        "print(f\"이상치가 포함된 데이터셋의 총 샘플 수: {len(X)}\")\n",
        "print(f\"전체 이상치 비율: {total_outlier_percentage:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd7f4413-f966-4e4e-e5a2-c60140491c7c",
        "id": "zw8ZMpkO1ZJ_"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "확장된 데이터셋 크기: (916, 18)\n",
            "확장된 타겟 크기: (916,)\n",
            "이상치가 포함된 데이터셋의 총 샘플 수: 916\n",
            "전체 이상치 비율: 10.04%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import load_iris\n",
        "from scipy.stats import zscore\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# 데이터셋 로딩\n",
        "data = fetch_openml(name='vehicle', version=1)\n",
        "X = data.data\n",
        "# Use LabelEncoder to convert categorical target to numerical\n",
        "encoder = LabelEncoder()\n",
        "y = encoder.fit_transform(data.target)\n",
        "df = pd.DataFrame(X, columns=data.feature_names)\n",
        "df['target'] = y  # 타겟 변수 추가\n",
        "\n",
        "\n",
        "# 데이터 정규화\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "df_scaled = pd.DataFrame(X_scaled, columns=data.feature_names)\n",
        "\n",
        "# 이상치 생성\n",
        "num_outliers = int(0.083 * len(df))  # 이상치를 설정\n",
        "outliers = []\n",
        "\n",
        "for _ in range(num_outliers):\n",
        "    outlier_sample = df_scaled.sample(n=1).copy()  # 임의의 샘플 선택\n",
        "    for col in df_scaled.columns:\n",
        "        if np.random.rand() > 0.5:\n",
        "            # 최대값보다 큰 이상치 생성\n",
        "            outlier_sample[col] = df_scaled[col].max() + np.random.rand() * 3\n",
        "        else:\n",
        "            # 최소값보다 작은 이상치 생성\n",
        "            outlier_sample[col] = df_scaled[col].min() - np.random.rand() * 3\n",
        "    outliers.append(outlier_sample)\n",
        "\n",
        "outliers_df = pd.concat(outliers, ignore_index=True)\n",
        "\n",
        "# 이상치의 타겟 값 랜덤 할당\n",
        "outliers_df['target'] = np.random.choice(y, num_outliers)\n",
        "\n",
        "# 원본 데이터에 이상치 추가\n",
        "df_extended = pd.concat([df, outliers_df], ignore_index=True)\n",
        "\n",
        "# 이상치 추가 후 다시 정규화\n",
        "X_extended_scaled = scaler.fit_transform(df_extended.drop(columns=['target']))\n",
        "df_extended_scaled = pd.DataFrame(X_extended_scaled, columns=data.feature_names)\n",
        "\n",
        "# 새로운 X, y 정의\n",
        "X = df_extended_scaled.values  # NumPy 배열로 변환\n",
        "y = df_extended['target'].values  # 타겟 값 추출\n",
        "\n",
        "# 기존 이상치 식별\n",
        "df_zscores = df_scaled.apply(zscore)  # Z-score 계산\n",
        "is_existing_outlier = (df_zscores.abs() > 3).any(axis=1)  # 절대값이 3 이상인 경우 이상치로 판단\n",
        "\n",
        "total_existing_outliers = is_existing_outlier.sum()  # 기존 이상치 수\n",
        "total_outliers = total_existing_outliers + num_outliers  # 총 이상치 수\n",
        "total_samples = len(df_extended)  # 전체 샘플 수\n",
        "total_outlier_percentage = (total_outliers / total_samples) * 100  # 이상치 비율 계산\n",
        "\n",
        "# 데이터 확인\n",
        "print(f\"확장된 데이터셋 크기: {X.shape}\")\n",
        "print(f\"확장된 타겟 크기: {y.shape}\")\n",
        "print(f\"이상치가 포함된 데이터셋의 총 샘플 수: {len(X)}\")\n",
        "print(f\"전체 이상치 비율: {total_outlier_percentage:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1Y5HyHI1ZJ_",
        "outputId": "f99bd983-9cac-4618-910c-523d1c84dbb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[76.47, 72.16, 72.94, 71.76, 67.45, 68.24]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from itertools import combinations\n",
        "\n",
        "results = []\n",
        "\n",
        "# 데이터 전처리 및 분할\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# OvO 방식을 사용하여 각 SVM 유형에 대해 학습 및 평가\n",
        "for svm_type in ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']:\n",
        "    models_ovo = train_ovo(X_train, y_train, C=1.0, svm_type=svm_type)\n",
        "    y_pred_ovo = predict_ovo(X_test, models_ovo)\n",
        "\n",
        "    # y_pred_ovo 변수를 정의한 후에 데이터 타입 확인 및 변환\n",
        "    if y_test.dtype != np.int64:\n",
        "        y_test = y_test.astype(int)\n",
        "\n",
        "    if y_pred_ovo.dtype != np.int64:\n",
        "        y_pred_ovo = y_pred_ovo.astype(int)\n",
        "\n",
        "    accuracy_ovo = accuracy_score(y_test, y_pred_ovo)\n",
        "    results.append((svm_type, round(accuracy_ovo * 100, 2)))\n",
        "\n",
        "# 결과를 DataFrame으로 변환 및 출력\n",
        "results_df = pd.DataFrame(results, columns=['SVM Type', 'Accuracy'])\n",
        "print(results_df['Accuracy'].tolist())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "accuracy_data = {\n",
        "    '3.06%' : [71.37, 69.02, 70.2, 72.55, 71.37, 70.59],\n",
        "    '5.07%' : [73.56, 77.01, 73.95, 73.56, 73.18, 72.80],\n",
        "    '10.04%' : [70.18, 72.36, 70.91, 70.91, 65.82, 69.45]\n",
        "}\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Vehicle data (ACO)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)\n"
      ],
      "metadata": {
        "id": "PM6nUpoB2xU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "accuracy_data = {\n",
        "    '3.06%' : [25.49, 25.49, 78.43, 78.04, 78.82, 78.43],\n",
        "    '5.07%' : [21.07, 21.07, 78.54, 77.78, 77.78, 78.93],\n",
        "    '10.04%' : [70.18, 72.36, 70.91, 70.91, 65.82, 69.45]#하기\n",
        "}\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Vehicle data (L-BFGS-B)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)\n"
      ],
      "metadata": {
        "id": "P0NKKKS91ZKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "accuracy_data = {\n",
        "    '3.06%' : [76.86, 78.43, 77.65, 76.86, 72.55, 74.51],\n",
        "    '5.07%' : [73.56, 77.01, 73.95, 73.56, 73.18, 72.80],\n",
        "    '10.04%' : [70.18, 72.36, 70.91, 70.91, 65.82, 69.45]\n",
        "}\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Vehicle data (L-BFGS-B)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)\n"
      ],
      "metadata": {
        "id": "KA352w3l17as"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "accuracy_data = {\n",
        "    '3.06%' : [76.86, 78.43, 77.65, 76.86, 72.55, 74.51],\n",
        "    '5.07%' : [73.56, 77.01, 73.95, 73.56, 73.18, 72.80],\n",
        "    '10.04%' : [70.18, 72.36, 70.91, 70.91, 65.82, 69.45]\n",
        "}\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Vehicle data (HS)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)\n"
      ],
      "metadata": {
        "id": "9iFu2H8j174V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "accuracy_data = {\n",
        "    '3.06%' : [76.86, 78.43, 77.65, 76.86, 72.55, 74.51],\n",
        "    '5.07%' : [73.56, 77.01, 73.95, 73.56, 73.18, 72.80],\n",
        "    '10.04%' : [70.18, 72.36, 70.91, 70.91, 65.82, 69.45]\n",
        "}\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Vehicle data (SMO)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)\n"
      ],
      "metadata": {
        "id": "HJ2rcmKa18O2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "accuracy_data = {\n",
        "    '3.06%' : [76.86, 78.43, 77.65, 76.86, 72.55, 74.51],\n",
        "    '5.07%' : [73.56, 77.01, 73.95, 73.56, 73.18, 72.80],\n",
        "    '10.04%' : [70.18, 72.36, 70.91, 70.91, 65.82, 69.45]\n",
        "}\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Vehicle data (GA)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)\n"
      ],
      "metadata": {
        "id": "q8ZDpLfU18Wi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "accuracy_data = {\n",
        "    '3.06%' : [71.37, 69.02, 70.2, 72.55, 71.37, 70.59],\n",
        "    '5.07%' : [67.82, 65.52, 67.82, 66.28, 68.97, 73.95],\n",
        "    '10.04%' : [63.64, 65.09, 63.64, 67.64, 66.91, 65.82]\n",
        "}\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Vehicle data (PSO)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Rz6V3cU18EN",
        "outputId": "3ee26f14-b249-43e4-bdc6-22906a97966e"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                    3.06%  5.07%  10.04%\n",
            "Vehicle data (PSO)                      \n",
            "L1                  71.37  67.82   63.64\n",
            "L2                  69.02  65.52   65.09\n",
            "Fair                70.20  67.82   63.64\n",
            "Cauchy              72.55  66.28   67.64\n",
            "Welsch              71.37  68.97   66.91\n",
            "Geman-McClure       70.59  73.95   65.82\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CHq01pvGEfc"
      },
      "source": [
        "#**Direct with various outliers**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGrUQYooGEfd"
      },
      "source": [
        "###iris data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5l3Dd_plGEfe",
        "outputId": "d70182d8-061d-4e75-9068-0c1c3d4c1c99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "기존 이상치 수: 1\n",
            "기존 이상치 비율: 0.67%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.stats import zscore\n",
        "\n",
        "# 데이터셋 로딩\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "df = pd.DataFrame(X, columns=data.feature_names)\n",
        "df['target'] = y  # 타겟 변수 추가\n",
        "\n",
        "# 데이터 정규화\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "df_scaled = pd.DataFrame(X_scaled, columns=data.feature_names)\n",
        "\n",
        "# 기존 이상치 식별\n",
        "df_zscores = df_scaled.apply(zscore)  # Z-score 계산\n",
        "is_existing_outlier = (df_zscores.abs() > 3).any(axis=1)  # 절대값이 3 이상인 경우 이상치로 판단\n",
        "\n",
        "# 기존 이상치 수 계산\n",
        "total_existing_outliers = is_existing_outlier.sum()\n",
        "\n",
        "# 기존 이상치 비율 계산\n",
        "total_samples = len(df)\n",
        "existing_outlier_percentage = (total_existing_outliers / total_samples) * 100\n",
        "\n",
        "print(f\"기존 이상치 수: {total_existing_outliers}\")\n",
        "print(f\"기존 이상치 비율: {existing_outlier_percentage:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jfWH9vvMGEff",
        "outputId": "0cd9b0d0-3785-490d-cebe-4b93bc08a277"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "확장된 데이터셋 크기: (154, 4)\n",
            "확장된 타겟 크기: (154,)\n",
            "이상치가 포함된 데이터셋의 총 샘플 수: 154\n",
            "전체 이상치 비율: 3.25%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import load_iris\n",
        "from scipy.stats import zscore\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# 데이터셋 로딩\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "df = pd.DataFrame(X, columns=iris.feature_names)\n",
        "df['target'] = y  # 타겟 변수 추가\n",
        "\n",
        "# 데이터 정규화\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "df_scaled = pd.DataFrame(X_scaled, columns=iris.feature_names)\n",
        "\n",
        "# 이상치 생성\n",
        "num_outliers = int(0.03 * len(df))  # 이상치를 설정\n",
        "outliers = []\n",
        "\n",
        "for _ in range(num_outliers):\n",
        "    outlier_sample = df_scaled.sample(n=1).copy()  # 임의의 샘플 선택\n",
        "    for col in df_scaled.columns:\n",
        "        if np.random.rand() > 0.5:\n",
        "            # 최대값보다 큰 이상치 생성\n",
        "            outlier_sample[col] = df_scaled[col].max() + np.random.rand() * 3\n",
        "        else:\n",
        "            # 최소값보다 작은 이상치 생성\n",
        "            outlier_sample[col] = df_scaled[col].min() - np.random.rand() * 3\n",
        "    outliers.append(outlier_sample)\n",
        "\n",
        "outliers_df = pd.concat(outliers, ignore_index=True)\n",
        "\n",
        "# 이상치의 타겟 값 랜덤 할당\n",
        "outliers_df['target'] = np.random.choice(y, num_outliers)\n",
        "\n",
        "# 원본 데이터에 이상치 추가\n",
        "df_extended = pd.concat([df, outliers_df], ignore_index=True)\n",
        "\n",
        "# 이상치 추가 후 다시 정규화\n",
        "X_extended_scaled = scaler.fit_transform(df_extended.drop(columns=['target']))\n",
        "df_extended_scaled = pd.DataFrame(X_extended_scaled, columns=iris.feature_names)\n",
        "\n",
        "# 새로운 X, y 정의\n",
        "X = df_extended_scaled.values  # NumPy 배열로 변환\n",
        "y = df_extended['target'].values  # 타겟 값 추출\n",
        "\n",
        "# 기존 이상치 식별\n",
        "df_zscores = df_scaled.apply(zscore)  # Z-score 계산\n",
        "is_existing_outlier = (df_zscores.abs() > 3).any(axis=1)  # 절대값이 3 이상인 경우 이상치로 판단\n",
        "\n",
        "total_existing_outliers = is_existing_outlier.sum()  # 기존 이상치 수\n",
        "total_outliers = total_existing_outliers + num_outliers  # 총 이상치 수\n",
        "total_samples = len(df_extended)  # 전체 샘플 수\n",
        "total_outlier_percentage = (total_outliers / total_samples) * 100  # 이상치 비율 계산\n",
        "\n",
        "# 데이터 확인\n",
        "print(f\"확장된 데이터셋 크기: {X.shape}\")\n",
        "print(f\"확장된 타겟 크기: {y.shape}\")\n",
        "print(f\"이상치가 포함된 데이터셋의 총 샘플 수: {len(X)}\")\n",
        "print(f\"전체 이상치 비율: {total_outlier_percentage:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q21x3nUEGEff",
        "outputId": "dbebfd31-39aa-4077-82a4-ec90b48cc991"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "확장된 데이터셋 크기: (157, 4)\n",
            "확장된 타겟 크기: (157,)\n",
            "이상치가 포함된 데이터셋의 총 샘플 수: 157\n",
            "전체 이상치 비율: 5.10%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import load_iris\n",
        "from scipy.stats import zscore\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# 데이터셋 로딩\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "df = pd.DataFrame(X, columns=iris.feature_names)\n",
        "df['target'] = y  # 타겟 변수 추가\n",
        "\n",
        "# 데이터 정규화\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "df_scaled = pd.DataFrame(X_scaled, columns=iris.feature_names)\n",
        "\n",
        "# 이상치 생성\n",
        "num_outliers = int(0.05 * len(df))  # 이상치를 설정\n",
        "outliers = []\n",
        "\n",
        "for _ in range(num_outliers):\n",
        "    outlier_sample = df_scaled.sample(n=1).copy()  # 임의의 샘플 선택\n",
        "    for col in df_scaled.columns:\n",
        "        if np.random.rand() > 0.5:\n",
        "            # 최대값보다 큰 이상치 생성\n",
        "            outlier_sample[col] = df_scaled[col].max() + np.random.rand() * 3\n",
        "        else:\n",
        "            # 최소값보다 작은 이상치 생성\n",
        "            outlier_sample[col] = df_scaled[col].min() - np.random.rand() * 3\n",
        "    outliers.append(outlier_sample)\n",
        "\n",
        "outliers_df = pd.concat(outliers, ignore_index=True)\n",
        "\n",
        "# 이상치의 타겟 값 랜덤 할당\n",
        "outliers_df['target'] = np.random.choice(y, num_outliers)\n",
        "\n",
        "# 원본 데이터에 이상치 추가\n",
        "df_extended = pd.concat([df, outliers_df], ignore_index=True)\n",
        "\n",
        "# 이상치 추가 후 다시 정규화\n",
        "X_extended_scaled = scaler.fit_transform(df_extended.drop(columns=['target']))\n",
        "df_extended_scaled = pd.DataFrame(X_extended_scaled, columns=iris.feature_names)\n",
        "\n",
        "# 새로운 X, y 정의\n",
        "X = df_extended_scaled.values  # NumPy 배열로 변환\n",
        "y = df_extended['target'].values  # 타겟 값 추출\n",
        "\n",
        "# 기존 이상치 식별\n",
        "df_zscores = df_scaled.apply(zscore)  # Z-score 계산\n",
        "is_existing_outlier = (df_zscores.abs() > 3).any(axis=1)  # 절대값이 3 이상인 경우 이상치로 판단\n",
        "\n",
        "total_existing_outliers = is_existing_outlier.sum()  # 기존 이상치 수\n",
        "total_outliers = total_existing_outliers + num_outliers  # 총 이상치 수\n",
        "total_samples = len(df_extended)  # 전체 샘플 수\n",
        "total_outlier_percentage = (total_outliers / total_samples) * 100  # 이상치 비율 계산\n",
        "\n",
        "# 데이터 확인\n",
        "print(f\"확장된 데이터셋 크기: {X.shape}\")\n",
        "print(f\"확장된 타겟 크기: {y.shape}\")\n",
        "print(f\"이상치가 포함된 데이터셋의 총 샘플 수: {len(X)}\")\n",
        "print(f\"전체 이상치 비율: {total_outlier_percentage:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzDahwVhGEfg",
        "outputId": "1315a256-f491-44d4-e446-0e3502fd63c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "확장된 데이터셋 크기: (166, 4)\n",
            "확장된 타겟 크기: (166,)\n",
            "이상치가 포함된 데이터셋의 총 샘플 수: 166\n",
            "전체 이상치 비율: 10.24%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import load_iris\n",
        "from scipy.stats import zscore\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# 데이터셋 로딩\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "df = pd.DataFrame(X, columns=iris.feature_names)\n",
        "df['target'] = y  # 타겟 변수 추가\n",
        "\n",
        "# 데이터 정규화\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "df_scaled = pd.DataFrame(X_scaled, columns=iris.feature_names)\n",
        "\n",
        "# 이상치 생성\n",
        "num_outliers = int(0.11 * len(df))  # 이상치를 설정\n",
        "outliers = []\n",
        "\n",
        "for _ in range(num_outliers):\n",
        "    outlier_sample = df_scaled.sample(n=1).copy()  # 임의의 샘플 선택\n",
        "    for col in df_scaled.columns:\n",
        "        if np.random.rand() > 0.5:\n",
        "            # 최대값보다 큰 이상치 생성\n",
        "            outlier_sample[col] = df_scaled[col].max() + np.random.rand() * 3\n",
        "        else:\n",
        "            # 최소값보다 작은 이상치 생성\n",
        "            outlier_sample[col] = df_scaled[col].min() - np.random.rand() * 3\n",
        "    outliers.append(outlier_sample)\n",
        "\n",
        "outliers_df = pd.concat(outliers, ignore_index=True)\n",
        "\n",
        "# 이상치의 타겟 값 랜덤 할당\n",
        "outliers_df['target'] = np.random.choice(y, num_outliers)\n",
        "\n",
        "# 원본 데이터에 이상치 추가\n",
        "df_extended = pd.concat([df, outliers_df], ignore_index=True)\n",
        "\n",
        "# 이상치 추가 후 다시 정규화\n",
        "X_extended_scaled = scaler.fit_transform(df_extended.drop(columns=['target']))\n",
        "df_extended_scaled = pd.DataFrame(X_extended_scaled, columns=iris.feature_names)\n",
        "\n",
        "# 새로운 X, y 정의\n",
        "X = df_extended_scaled.values  # NumPy 배열로 변환\n",
        "y = df_extended['target'].values  # 타겟 값 추출\n",
        "\n",
        "# 기존 이상치 식별\n",
        "df_zscores = df_scaled.apply(zscore)  # Z-score 계산\n",
        "is_existing_outlier = (df_zscores.abs() > 3).any(axis=1)  # 절대값이 3 이상인 경우 이상치로 판단\n",
        "\n",
        "total_existing_outliers = is_existing_outlier.sum()  # 기존 이상치 수\n",
        "total_outliers = total_existing_outliers + num_outliers  # 총 이상치 수\n",
        "total_samples = len(df_extended)  # 전체 샘플 수\n",
        "total_outlier_percentage = (total_outliers / total_samples) * 100  # 이상치 비율 계산\n",
        "\n",
        "# 데이터 확인\n",
        "print(f\"확장된 데이터셋 크기: {X.shape}\")\n",
        "print(f\"확장된 타겟 크기: {y.shape}\")\n",
        "print(f\"이상치가 포함된 데이터셋의 총 샘플 수: {len(X)}\")\n",
        "print(f\"전체 이상치 비율: {total_outlier_percentage:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iu7k0voWGEfg",
        "outputId": "bebc5dd0-7785-43d0-bb98-d62d79a2d499"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "        SVM Type  Accuracy\n",
            "0             L1      84.0\n",
            "1             L2      18.0\n",
            "2           Fair      70.0\n",
            "3         Cauchy      74.0\n",
            "4         Welsch      66.0\n",
            "5  Geman-McClure      78.0\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from itertools import combinations\n",
        "\n",
        "results = []\n",
        "\n",
        "# 데이터 전처리 및 분할\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Direct 방식을 사용하여 각 SVM 유형에 대해 학습 및 평가\n",
        "for svm_type in ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']:\n",
        "    num_classes = len(np.unique(y))\n",
        "    weights, biases = train_direct(X_train, y_train, C=1.0, svm_type=svm_type, num_classes=num_classes)\n",
        "    y_pred_direct = predict_direct(X_test, weights, biases)\n",
        "\n",
        "\n",
        "    # y_pred_direct 변수를 정의한 후에 데이터 타입 확인 및 변환\n",
        "    if y_test.dtype != np.int64:\n",
        "        y_test = y_test.astype(int)\n",
        "\n",
        "    if y_pred_direct.dtype != np.int64:\n",
        "        y_pred_direct = y_pred_direct.astype(int)\n",
        "\n",
        "    accuracy_direct = accuracy_score(y_test, y_pred_direct)\n",
        "    results.append((svm_type, round(accuracy_direct * 100, 2)))\n",
        "\n",
        "# 결과를 DataFrame으로 변환 및 출력\n",
        "results_df = pd.DataFrame(results, columns=['SVM Type', 'Accuracy'])\n",
        "print(results_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5DFZWr9GEfh",
        "outputId": "b5b66b19-84c8-4ace-a3c4-7f6597351144"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                       3.25%  5.10%  10.24%\n",
            "Iris data (L-BFGS-B)                       \n",
            "L1                    100.00  91.67    64.0\n",
            "L2                     97.87  93.75    76.0\n",
            "Fair                   95.74  91.67    76.0\n",
            "Cauchy                 95.74  91.67    94.0\n",
            "Welsch                 95.74  91.67    92.0\n",
            "Geman-McClure          95.74  91.67    92.0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "accuracy_data = {\n",
        "    '3.25%' : [100.00, 97.87, 95.74, 95.74, 95.74, 95.74],\n",
        "    '5.10%' : [91.67, 93.75, 91.67, 91.67, 91.67, 91.67],\n",
        "    '10.24%' : [64.0, 76.0, 76.0, 94.0, 92.0, 92.0]\n",
        "}\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Iris data (L-BFGS-B)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GnzwgoZmGEfh",
        "outputId": "ecfd27c3-1ee6-47bd-8b37-51c28739f6ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                3.25%  5.10%  10.24%\n",
            "Iris data (GA)                      \n",
            "L1              87.23  70.83    70.0\n",
            "L2              78.72  41.67    64.0\n",
            "Fair            78.72  72.92    80.0\n",
            "Cauchy          78.72  81.25    68.0\n",
            "Welsch          74.47  70.83    72.0\n",
            "Geman-McClure   74.47  60.42    90.0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "outlier = ['3%', '5%', '10%']\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "\n",
        "accuracy_data = {\n",
        "    '3.25%' : [87.23, 78.72, 78.72, 78.72, 74.47, 74.47],\n",
        "    '5.10%' : [70.83, 41.67, 72.92, 81.25, 70.83, 60.42],\n",
        "    '10.24%' : [70.0, 64.0, 80.0, 68.0, 72.0, 90.0]\n",
        "}\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Iris data (GA)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKuEWMuRGEfi",
        "outputId": "9fa6f022-a082-4a69-eb7d-0a9d393cbc34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                 3.25%  5.10%  10.24%\n",
            "Iris data (SMO)                      \n",
            "L1               89.36  93.75    88.0\n",
            "L2               89.36  93.75    88.0\n",
            "Fair             87.23  89.58    94.0\n",
            "Cauchy           87.23  87.50    94.0\n",
            "Welsch           89.36  89.58    94.0\n",
            "Geman-McClure    89.36  89.58    94.0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "outlier = ['3%', '5%', '10%']\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "\n",
        "accuracy_data = {\n",
        "    '3.25%' : [89.36, 89.36, 87.23, 87.23, 89.36, 89.36],\n",
        "    '5.10%' : [93.75, 93.75, 89.58, 87.50, 89.58, 89.58],\n",
        "    '10.24%' : [88.0, 88.0, 94.0, 94.0, 94.0, 94.0]\n",
        "}\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Iris data (SMO)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcF4f3MfGEfi",
        "outputId": "d65f10c8-b400-4835-91e4-863ad8dce7af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                 3.25%  5.10%  10.24%\n",
            "Iris data (PSO)                      \n",
            "L1               91.49  91.67    68.0\n",
            "L2               95.74  93.75    64.0\n",
            "Fair             91.49  93.75    80.0\n",
            "Cauchy           97.87  89.58    92.0\n",
            "Welsch           89.36  91.67    64.0\n",
            "Geman-McClure    95.74  91.67    90.0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "outlier = ['3%', '5%', '10%']\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "\n",
        "accuracy_data = {\n",
        "    '3.25%' : [91.49, 95.74, 91.49, 97.87, 89.36, 95.74],\n",
        "    '5.10%' : [91.67, 93.75, 93.75, 89.58, 91.67, 91.67],\n",
        "    '10.24%' : [68.0, 64.0, 80.0, 92.0, 64.0, 90.0]\n",
        "}\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Iris data (PSO)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_BTYGh6yGEfi",
        "outputId": "674b8190-79e7-46fd-d649-277e5ff1bae5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                 3.25%  5.10%  10.24%\n",
            "Iris data (ACO)                      \n",
            "L1               82.98  87.50    66.0\n",
            "L2               91.49  93.75    64.0\n",
            "Fair             87.23  91.67    64.0\n",
            "Cauchy           85.11  89.58    80.0\n",
            "Welsch           74.47  66.67    64.0\n",
            "Geman-McClure    78.72  72.92    66.0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "outlier = ['3%', '5%', '10%']\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "\n",
        "accuracy_data = {\n",
        "    '3.25%' : [95.74, 97.87, 95.74, 97.87, 95.74, 95.74],\n",
        "    '5.10%' : [93.75, 93.75, 93.75, 91.67, 91.67, 91.67],\n",
        "    '10.24%' : [64.0, 86.0, 82.0, 94.0, 92.0, 92.0]\n",
        "}\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Iris data (ACO)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecwnwjtOGEfi",
        "outputId": "72de91d4-57d6-4364-c611-0afdc42a5ea2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                3.25%  5.10%  10.24%\n",
            "Iris data (HS)                      \n",
            "L1              78.72  66.67    84.0\n",
            "L2              87.23  68.75    18.0\n",
            "Fair            76.60  70.83    70.0\n",
            "Cauchy          89.36  75.00    74.0\n",
            "Welsch          74.47  81.25    66.0\n",
            "Geman-McClure   80.85  83.33    78.0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "accuracy_data = {\n",
        "    '3.25%' : [78.72, 87.23, 76.60, 89.36, 74.47, 80.85],\n",
        "    '5.10%' : [66.67, 68.75, 70.83, 75.00, 81.25, 83.33],\n",
        "    '10.24%' : [84.0, 18.0, 70.0, 74.0, 66.0, 78.0]\n",
        "}\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Iris data (HS)'\n",
        "\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFztGPM4tITa"
      },
      "source": [
        "### segment data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBva2Fc6tI1c",
        "outputId": "e3253056-a51d-4ab6-979b-a631b2d81e1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "기존 이상치 수: 175\n",
            "기존 이상치 비율: 7.58%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from scipy.stats import zscore\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# 데이터셋 로딩\n",
        "data = fetch_openml(name='segment', version=1)\n",
        "X = data.data\n",
        "# Use LabelEncoder to convert categorical target to numerical\n",
        "encoder = LabelEncoder()\n",
        "y = encoder.fit_transform(data.target)\n",
        "df = pd.DataFrame(X, columns=data.feature_names)\n",
        "df['target'] = y  # 타겟 변수 추가\n",
        "\n",
        "# 데이터 정규화\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "df_scaled = pd.DataFrame(X_scaled, columns=data.feature_names)\n",
        "\n",
        "# 기존 이상치 식별\n",
        "df_zscores = df_scaled.apply(zscore)  # Z-score 계산\n",
        "is_existing_outlier = (df_zscores.abs() > 3).any(axis=1)  # 절대값이 3 이상인 경우 이상치로 판단\n",
        "\n",
        "# 기존 이상치 수 계산\n",
        "total_existing_outliers = is_existing_outlier.sum()\n",
        "\n",
        "# 기존 이상치 비율 계산\n",
        "total_samples = len(df)\n",
        "existing_outlier_percentage = (total_existing_outliers / total_samples) * 100\n",
        "\n",
        "print(f\"기존 이상치 수: {total_existing_outliers}\")\n",
        "print(f\"기존 이상치 비율: {existing_outlier_percentage:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 184,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FnXeMi-NtI1c",
        "outputId": "d66e631c-827b-4c45-d4f9-9ac2ebb05bc8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "확장된 데이터셋 크기: (2374, 19)\n",
            "확장된 타겟 크기: (2374,)\n",
            "이상치가 포함된 데이터셋의 총 샘플 수: 2374\n",
            "전체 이상치 비율: 10.07%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from scipy.stats import zscore\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# 데이터셋 로딩\n",
        "data = fetch_openml(name='segment', version=1)\n",
        "X = data.data\n",
        "# Use LabelEncoder to convert categorical target to numerical\n",
        "encoder = LabelEncoder()\n",
        "y = encoder.fit_transform(data.target)\n",
        "df = pd.DataFrame(X, columns=data.feature_names)\n",
        "df['target'] = y  # 타겟 변수 추가\n",
        "\n",
        "# 데이터 정규화\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "df_scaled = pd.DataFrame(X_scaled, columns=data.feature_names)\n",
        "\n",
        "# 이상치 생성\n",
        "num_outliers = int(0.028 * len(df))  # 이상치를 설정\n",
        "outliers = []\n",
        "\n",
        "for _ in range(num_outliers):\n",
        "    outlier_sample = df_scaled.sample(n=1).copy()  # 임의의 샘플 선택\n",
        "    for col in df_scaled.columns:\n",
        "        if np.random.rand() > 0.5:\n",
        "            # 최대값보다 큰 이상치 생성\n",
        "            outlier_sample[col] = df_scaled[col].max() + np.random.rand() * 3\n",
        "        else:\n",
        "            # 최소값보다 작은 이상치 생성\n",
        "            outlier_sample[col] = df_scaled[col].min() - np.random.rand() * 3\n",
        "    outliers.append(outlier_sample)\n",
        "\n",
        "outliers_df = pd.concat(outliers, ignore_index=True)\n",
        "\n",
        "# 이상치의 타겟 값 랜덤 할당\n",
        "outliers_df['target'] = np.random.choice(y, num_outliers)\n",
        "\n",
        "# 원본 데이터에 이상치 추가\n",
        "df_extended = pd.concat([df, outliers_df], ignore_index=True)\n",
        "\n",
        "# 이상치 추가 후 다시 정규화\n",
        "X_extended_scaled = scaler.fit_transform(df_extended.drop(columns=['target']))\n",
        "df_extended_scaled = pd.DataFrame(X_extended_scaled, columns=data.feature_names)\n",
        "\n",
        "# 새로운 X, y 정의\n",
        "X = df_extended_scaled.values  # NumPy 배열로 변환\n",
        "y = df_extended['target'].values  # 타겟 값 추출\n",
        "\n",
        "# 기존 이상치 식별\n",
        "df_zscores = df_scaled.apply(zscore)  # Z-score 계산\n",
        "is_existing_outlier = (df_zscores.abs() > 3).any(axis=1)  # 절대값이 3 이상인 경우 이상치로 판단\n",
        "\n",
        "total_existing_outliers = is_existing_outlier.sum()  # 기존 이상치 수\n",
        "total_outliers = total_existing_outliers + num_outliers  # 총 이상치 수\n",
        "total_samples = len(df_extended)  # 전체 샘플 수\n",
        "total_outlier_percentage = (total_outliers / total_samples) * 100  # 이상치 비율 계산\n",
        "\n",
        "# 데이터 확인\n",
        "print(f\"확장된 데이터셋 크기: {X.shape}\")\n",
        "print(f\"확장된 타겟 크기: {y.shape}\")\n",
        "print(f\"이상치가 포함된 데이터셋의 총 샘플 수: {len(X)}\")\n",
        "print(f\"전체 이상치 비율: {total_outlier_percentage:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 187,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LsmZU_dptI1e",
        "outputId": "fba1b9ca-e6f2-4902-d9c8-03d04abc758e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        SVM Type  Accuracy\n",
            "0             L1     37.03\n",
            "1             L2     32.54\n",
            "2           Fair     54.84\n",
            "3         Cauchy     53.72\n",
            "4         Welsch     41.51\n",
            "5  Geman-McClure     40.25\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from itertools import combinations\n",
        "\n",
        "results = []\n",
        "\n",
        "# 데이터 전처리 및 분할\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Direct 방식을 사용하여 각 SVM 유형에 대해 학습 및 평가\n",
        "for svm_type in ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']:\n",
        "    num_classes = len(np.unique(y))\n",
        "    weights, biases = train_direct(X_train, y_train, C=1.0, svm_type=svm_type, num_classes=num_classes)\n",
        "    y_pred_direct = predict_direct(X_test, weights, biases)\n",
        "\n",
        "\n",
        "    # y_pred_direct 변수를 정의한 후에 데이터 타입 확인 및 변환\n",
        "    if y_test.dtype != np.int64:\n",
        "        y_test = y_test.astype(int)\n",
        "\n",
        "    if y_pred_direct.dtype != np.int64:\n",
        "        y_pred_direct = y_pred_direct.astype(int)\n",
        "\n",
        "    accuracy_direct = accuracy_score(y_test, y_pred_direct)\n",
        "    results.append((svm_type, round(accuracy_direct * 100, 2)))\n",
        "\n",
        "# 결과를 DataFrame으로 변환 및 출력\n",
        "results_df = pd.DataFrame(results, columns=['SVM Type', 'Accuracy'])\n",
        "print(results_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j15XWHdstpRt",
        "outputId": "cc288e63-7118-4d31-e8b4-d36d85b422aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                         7.58%  10.07%\n",
            "Segment data (L-BFGS-B)               \n",
            "L1                       21.79   19.07\n",
            "L2                       17.75   20.06\n",
            "Fair                     18.04   19.21\n",
            "Cauchy                   18.47   19.07\n",
            "Welsch                   19.91   19.50\n",
            "Geman-McClure            18.04   19.35\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "accuracy_data = {\n",
        "    '7.58%' : [21.79,17.75, 18.04, 18.47, 19.91, 18.04],\n",
        "    '10.07%' : [19.07, 20.06, 19.21, 19.07, 19.50, 19.35]\n",
        "}\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Segment data (L-BFGS-B)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zX4dexUytpRu",
        "outputId": "b6b0c3bd-be30-44d5-b34c-fea9d38dfdf4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   7.58%  10.07%\n",
            "Segment data (GA)               \n",
            "L1                 50.94   37.03\n",
            "L2                 37.81   32.54\n",
            "Fair               35.93   54.84\n",
            "Cauchy             34.92   53.72\n",
            "Welsch             40.26   41.51\n",
            "Geman-McClure      49.64   40.25\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "accuracy_data = {\n",
        "    '7.58%' : [ 50.94,37.81, 35.93, 34.92, 40.26, 49.64],\n",
        "    '10.07%' : [37.03, 32.54, 54.84, 53.72, 41.51, 40.25\n",
        "]\n",
        "}\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Segment data (GA)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yAcWJ85OtpRs",
        "outputId": "fd4fadba-fb30-4b99-8f8f-2b3d54d97913"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                    7.58%  10.07%\n",
            "Segment data (SMO)               \n",
            "L1                  85.86   84.99\n",
            "L2                  85.86   84.99\n",
            "Fair                12.41   14.87\n",
            "Cauchy              12.41   14.87\n",
            "Welsch              12.41   14.87\n",
            "Geman-McClure       12.41   14.87\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd #해야함\n",
        "\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "accuracy_data = {\n",
        "    '7.58%' : [ 85.86, 85.86, 12.41, 12.41, 12.41, 12.41],\n",
        "    '10.07%' : [84.99, 84.99, 14.87, 14.87, 14.87, 14.87]\n",
        "}\n",
        "\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Segment data (SMO)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WasNvHa5tpRu",
        "outputId": "ceb4e12c-2d85-49e6-8c8f-9b16ce93e3ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                    7.58%  10.07%\n",
            "Segment data (PSO)               \n",
            "L1                  48.34   44.74\n",
            "L2                  33.77   48.39\n",
            "Fair                68.83   60.73\n",
            "Cauchy              43.87   59.05\n",
            "Welsch              72.29   43.90\n",
            "Geman-McClure       49.35   59.47\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "accuracy_data = {\n",
        "    '7.58%' : [48.34, 33.77, 68.83, 43.87, 72.29, 49.35],\n",
        "    '10.07%' : [44.74, 48.39, 60.73, 59.05, 43.90, 59.47]\n",
        "}\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Segment data (PSO)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMmFEJuBtpRv",
        "outputId": "38be1fdd-bc05-4483-f052-a5543b3d6efe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                    7.58%  10.07%\n",
            "Segment data (ACO)               \n",
            "L1                  85.71   83.17\n",
            "L2                  86.72   67.04\n",
            "Fair                85.71   85.69\n",
            "Cauchy              87.01   85.13\n",
            "Welsch              82.97   84.85\n",
            "Geman-McClure       83.98   87.52\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "accuracy_data = {\n",
        "    '7.58%' : [85.71, 86.72, 85.71, 87.01, 82.97, 83.98],\n",
        "    '10.07%' : [83.17, 67.04, 85.69, 85.13, 84.85, 87.52]\n",
        "}\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Segment data (ACO)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Va9W_CYtpRv",
        "outputId": "0fd2442b-0ed1-4324-a666-56630e35299f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   7.58%  10.07%\n",
            "Segment data (HS)               \n",
            "L1                 38.82   53.16\n",
            "L2                 31.60   36.19\n",
            "Fair               56.71   38.99\n",
            "Cauchy             45.31   39.97\n",
            "Welsch             50.36   46.28\n",
            "Geman-McClure      40.40   52.45\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "accuracy_data = {\n",
        "    '7.58%' : [ 38.82, 31.60, 56.71, 45.31, 50.36, 40.40],\n",
        "    '10.07%' : [53.16, 36.19, 38.99, 39.97, 46.28, 52.45]\n",
        "}\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Segment data (HS)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AExVCWzu1ceU"
      },
      "source": [
        "### vehicle data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39783c43-d06b-4b6f-9dd7-16e174b41d61",
        "id": "4GirKKpo1ceV"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "기존 이상치 수: 22\n",
            "기존 이상치 비율: 2.60%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from scipy.stats import zscore\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# 데이터셋 로딩\n",
        "data = fetch_openml(name='vehicle', version=1)\n",
        "X = data.data\n",
        "encoder = LabelEncoder()\n",
        "y = encoder.fit_transform(data.target)\n",
        "df = pd.DataFrame(X, columns=data.feature_names)\n",
        "df['target'] = y  # 타겟 변수 추가\n",
        "\n",
        "# 데이터 정규화\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "df_scaled = pd.DataFrame(X_scaled, columns=data.feature_names)\n",
        "\n",
        "# 기존 이상치 식별\n",
        "df_zscores = df_scaled.apply(zscore)  # Z-score 계산\n",
        "is_existing_outlier = (df_zscores.abs() > 3).any(axis=1)  # 절대값이 3 이상인 경우 이상치로 판단\n",
        "\n",
        "# 기존 이상치 수 계산\n",
        "total_existing_outliers = is_existing_outlier.sum()\n",
        "\n",
        "# 기존 이상치 비율 계산\n",
        "total_samples = len(df)\n",
        "existing_outlier_percentage = (total_existing_outliers / total_samples) * 100\n",
        "\n",
        "print(f\"기존 이상치 수: {total_existing_outliers}\")\n",
        "print(f\"기존 이상치 비율: {existing_outlier_percentage:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b7ed294-d4c0-4ec1-ee18-a74c2f01600d",
        "id": "739MgnUm1ceW"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "확장된 데이터셋 크기: (850, 18)\n",
            "확장된 타겟 크기: (850,)\n",
            "이상치가 포함된 데이터셋의 총 샘플 수: 850\n",
            "전체 이상치 비율: 3.06%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import load_iris\n",
        "from scipy.stats import zscore\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# 데이터셋 로딩\n",
        "data = fetch_openml(name='vehicle', version=1)\n",
        "X = data.data\n",
        "# Use LabelEncoder to convert categorical target to numerical\n",
        "encoder = LabelEncoder()\n",
        "y = encoder.fit_transform(data.target)\n",
        "df = pd.DataFrame(X, columns=data.feature_names)\n",
        "df['target'] = y  # 타겟 변수 추가\n",
        "\n",
        "\n",
        "# 데이터 정규화\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "df_scaled = pd.DataFrame(X_scaled, columns=data.feature_names)\n",
        "\n",
        "# 이상치 생성\n",
        "num_outliers = int(0.0048 * len(df))  # 이상치를 설정\n",
        "outliers = []\n",
        "\n",
        "for _ in range(num_outliers):\n",
        "    outlier_sample = df_scaled.sample(n=1).copy()  # 임의의 샘플 선택\n",
        "    for col in df_scaled.columns:\n",
        "        if np.random.rand() > 0.5:\n",
        "            # 최대값보다 큰 이상치 생성\n",
        "            outlier_sample[col] = df_scaled[col].max() + np.random.rand() * 3\n",
        "        else:\n",
        "            # 최소값보다 작은 이상치 생성\n",
        "            outlier_sample[col] = df_scaled[col].min() - np.random.rand() * 3\n",
        "    outliers.append(outlier_sample)\n",
        "\n",
        "outliers_df = pd.concat(outliers, ignore_index=True)\n",
        "\n",
        "# 이상치의 타겟 값 랜덤 할당\n",
        "outliers_df['target'] = np.random.choice(y, num_outliers)\n",
        "\n",
        "# 원본 데이터에 이상치 추가\n",
        "df_extended = pd.concat([df, outliers_df], ignore_index=True)\n",
        "\n",
        "# 이상치 추가 후 다시 정규화\n",
        "X_extended_scaled = scaler.fit_transform(df_extended.drop(columns=['target']))\n",
        "df_extended_scaled = pd.DataFrame(X_extended_scaled, columns=data.feature_names)\n",
        "\n",
        "# 새로운 X, y 정의\n",
        "X = df_extended_scaled.values  # NumPy 배열로 변환\n",
        "y = df_extended['target'].values  # 타겟 값 추출\n",
        "\n",
        "# 기존 이상치 식별\n",
        "df_zscores = df_scaled.apply(zscore)  # Z-score 계산\n",
        "is_existing_outlier = (df_zscores.abs() > 3).any(axis=1)  # 절대값이 3 이상인 경우 이상치로 판단\n",
        "\n",
        "total_existing_outliers = is_existing_outlier.sum()  # 기존 이상치 수\n",
        "total_outliers = total_existing_outliers + num_outliers  # 총 이상치 수\n",
        "total_samples = len(df_extended)  # 전체 샘플 수\n",
        "total_outlier_percentage = (total_outliers / total_samples) * 100  # 이상치 비율 계산\n",
        "\n",
        "# 데이터 확인\n",
        "print(f\"확장된 데이터셋 크기: {X.shape}\")\n",
        "print(f\"확장된 타겟 크기: {y.shape}\")\n",
        "print(f\"이상치가 포함된 데이터셋의 총 샘플 수: {len(X)}\")\n",
        "print(f\"전체 이상치 비율: {total_outlier_percentage:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e1b908a-c58a-4a2f-c737-9c797dd8c239",
        "id": "xlb8XkcR1ceW"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "확장된 데이터셋 크기: (868, 18)\n",
            "확장된 타겟 크기: (868,)\n",
            "이상치가 포함된 데이터셋의 총 샘플 수: 868\n",
            "전체 이상치 비율: 5.07%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import load_iris\n",
        "from scipy.stats import zscore\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# 데이터셋 로딩\n",
        "data = fetch_openml(name='vehicle', version=1)\n",
        "X = data.data\n",
        "# Use LabelEncoder to convert categorical target to numerical\n",
        "encoder = LabelEncoder()\n",
        "y = encoder.fit_transform(data.target)\n",
        "df = pd.DataFrame(X, columns=data.feature_names)\n",
        "df['target'] = y  # 타겟 변수 추가\n",
        "\n",
        "\n",
        "# 데이터 정규화\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "df_scaled = pd.DataFrame(X_scaled, columns=data.feature_names)\n",
        "\n",
        "# 이상치 생성\n",
        "num_outliers = int(0.027 * len(df))  # 이상치를 설정\n",
        "outliers = []\n",
        "\n",
        "for _ in range(num_outliers):\n",
        "    outlier_sample = df_scaled.sample(n=1).copy()  # 임의의 샘플 선택\n",
        "    for col in df_scaled.columns:\n",
        "        if np.random.rand() > 0.5:\n",
        "            # 최대값보다 큰 이상치 생성\n",
        "            outlier_sample[col] = df_scaled[col].max() + np.random.rand() * 3\n",
        "        else:\n",
        "            # 최소값보다 작은 이상치 생성\n",
        "            outlier_sample[col] = df_scaled[col].min() - np.random.rand() * 3\n",
        "    outliers.append(outlier_sample)\n",
        "\n",
        "outliers_df = pd.concat(outliers, ignore_index=True)\n",
        "\n",
        "# 이상치의 타겟 값 랜덤 할당\n",
        "outliers_df['target'] = np.random.choice(y, num_outliers)\n",
        "\n",
        "# 원본 데이터에 이상치 추가\n",
        "df_extended = pd.concat([df, outliers_df], ignore_index=True)\n",
        "\n",
        "# 이상치 추가 후 다시 정규화\n",
        "X_extended_scaled = scaler.fit_transform(df_extended.drop(columns=['target']))\n",
        "df_extended_scaled = pd.DataFrame(X_extended_scaled, columns=data.feature_names)\n",
        "\n",
        "# 새로운 X, y 정의\n",
        "X = df_extended_scaled.values  # NumPy 배열로 변환\n",
        "y = df_extended['target'].values  # 타겟 값 추출\n",
        "\n",
        "# 기존 이상치 식별\n",
        "df_zscores = df_scaled.apply(zscore)  # Z-score 계산\n",
        "is_existing_outlier = (df_zscores.abs() > 3).any(axis=1)  # 절대값이 3 이상인 경우 이상치로 판단\n",
        "\n",
        "total_existing_outliers = is_existing_outlier.sum()  # 기존 이상치 수\n",
        "total_outliers = total_existing_outliers + num_outliers  # 총 이상치 수\n",
        "total_samples = len(df_extended)  # 전체 샘플 수\n",
        "total_outlier_percentage = (total_outliers / total_samples) * 100  # 이상치 비율 계산\n",
        "\n",
        "# 데이터 확인\n",
        "print(f\"확장된 데이터셋 크기: {X.shape}\")\n",
        "print(f\"확장된 타겟 크기: {y.shape}\")\n",
        "print(f\"이상치가 포함된 데이터셋의 총 샘플 수: {len(X)}\")\n",
        "print(f\"전체 이상치 비율: {total_outlier_percentage:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5c85968-3463-4b90-c8a1-4b67eec1caed",
        "id": "ftb54d--1ceX"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "확장된 데이터셋 크기: (916, 18)\n",
            "확장된 타겟 크기: (916,)\n",
            "이상치가 포함된 데이터셋의 총 샘플 수: 916\n",
            "전체 이상치 비율: 10.04%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import load_iris\n",
        "from scipy.stats import zscore\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# 데이터셋 로딩\n",
        "data = fetch_openml(name='vehicle', version=1)\n",
        "X = data.data\n",
        "# Use LabelEncoder to convert categorical target to numerical\n",
        "encoder = LabelEncoder()\n",
        "y = encoder.fit_transform(data.target)\n",
        "df = pd.DataFrame(X, columns=data.feature_names)\n",
        "df['target'] = y  # 타겟 변수 추가\n",
        "\n",
        "\n",
        "# 데이터 정규화\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "df_scaled = pd.DataFrame(X_scaled, columns=data.feature_names)\n",
        "\n",
        "# 이상치 생성\n",
        "num_outliers = int(0.083 * len(df))  # 이상치를 설정\n",
        "outliers = []\n",
        "\n",
        "for _ in range(num_outliers):\n",
        "    outlier_sample = df_scaled.sample(n=1).copy()  # 임의의 샘플 선택\n",
        "    for col in df_scaled.columns:\n",
        "        if np.random.rand() > 0.5:\n",
        "            # 최대값보다 큰 이상치 생성\n",
        "            outlier_sample[col] = df_scaled[col].max() + np.random.rand() * 3\n",
        "        else:\n",
        "            # 최소값보다 작은 이상치 생성\n",
        "            outlier_sample[col] = df_scaled[col].min() - np.random.rand() * 3\n",
        "    outliers.append(outlier_sample)\n",
        "\n",
        "outliers_df = pd.concat(outliers, ignore_index=True)\n",
        "\n",
        "# 이상치의 타겟 값 랜덤 할당\n",
        "outliers_df['target'] = np.random.choice(y, num_outliers)\n",
        "\n",
        "# 원본 데이터에 이상치 추가\n",
        "df_extended = pd.concat([df, outliers_df], ignore_index=True)\n",
        "\n",
        "# 이상치 추가 후 다시 정규화\n",
        "X_extended_scaled = scaler.fit_transform(df_extended.drop(columns=['target']))\n",
        "df_extended_scaled = pd.DataFrame(X_extended_scaled, columns=data.feature_names)\n",
        "\n",
        "# 새로운 X, y 정의\n",
        "X = df_extended_scaled.values  # NumPy 배열로 변환\n",
        "y = df_extended['target'].values  # 타겟 값 추출\n",
        "\n",
        "# 기존 이상치 식별\n",
        "df_zscores = df_scaled.apply(zscore)  # Z-score 계산\n",
        "is_existing_outlier = (df_zscores.abs() > 3).any(axis=1)  # 절대값이 3 이상인 경우 이상치로 판단\n",
        "\n",
        "total_existing_outliers = is_existing_outlier.sum()  # 기존 이상치 수\n",
        "total_outliers = total_existing_outliers + num_outliers  # 총 이상치 수\n",
        "total_samples = len(df_extended)  # 전체 샘플 수\n",
        "total_outlier_percentage = (total_outliers / total_samples) * 100  # 이상치 비율 계산\n",
        "\n",
        "# 데이터 확인\n",
        "print(f\"확장된 데이터셋 크기: {X.shape}\")\n",
        "print(f\"확장된 타겟 크기: {y.shape}\")\n",
        "print(f\"이상치가 포함된 데이터셋의 총 샘플 수: {len(X)}\")\n",
        "print(f\"전체 이상치 비율: {total_outlier_percentage:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AScVTdP01ceX"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from itertools import combinations\n",
        "\n",
        "results = []\n",
        "\n",
        "# 데이터 전처리 및 분할\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# OvO 방식을 사용하여 각 SVM 유형에 대해 학습 및 평가\n",
        "for svm_type in ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']:\n",
        "    models_ovo = train_ovo(X_train, y_train, C=1.0, svm_type=svm_type)\n",
        "    y_pred_ovo = predict_ovo(X_test, models_ovo)\n",
        "\n",
        "    # y_pred_ovo 변수를 정의한 후에 데이터 타입 확인 및 변환\n",
        "    if y_test.dtype != np.int64:\n",
        "        y_test = y_test.astype(int)\n",
        "\n",
        "    if y_pred_ovo.dtype != np.int64:\n",
        "        y_pred_ovo = y_pred_ovo.astype(int)\n",
        "\n",
        "    accuracy_ovo = accuracy_score(y_test, y_pred_ovo)\n",
        "    results.append((svm_type, round(accuracy_ovo * 100, 2)))\n",
        "\n",
        "# 결과를 DataFrame으로 변환 및 출력\n",
        "results_df = pd.DataFrame(results, columns=['SVM Type', 'Accuracy'])\n",
        "print(results_df['Accuracy'].tolist())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "accuracy_data = {\n",
        "    '3.06%' : [76.86, 78.43, 77.65, 76.86, 72.55, 74.51],\n",
        "    '5.07%' : [73.56, 77.01, 73.95, 73.56, 73.18, 72.80],\n",
        "    '10.04%' : [70.18, 72.36, 70.91, 70.91, 65.82, 69.45]\n",
        "}\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Vehicle data (L-BFGS-B)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)\n"
      ],
      "metadata": {
        "id": "NdH3SowD1ceY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "accuracy_data = {\n",
        "    '3.06%' : [76.86, 78.43, 77.65, 76.86, 72.55, 74.51],\n",
        "    '5.07%' : [73.56, 77.01, 73.95, 73.56, 73.18, 72.80],\n",
        "    '10.04%' : [70.18, 72.36, 70.91, 70.91, 65.82, 69.45]\n",
        "}\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Vehicle data (HS)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)\n"
      ],
      "metadata": {
        "id": "dI8C-Fmy2gsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "accuracy_data = {\n",
        "    '3.06%' : [76.86, 78.43, 77.65, 76.86, 72.55, 74.51],\n",
        "    '5.07%' : [73.56, 77.01, 73.95, 73.56, 73.18, 72.80],\n",
        "    '10.04%' : [70.18, 72.36, 70.91, 70.91, 65.82, 69.45]\n",
        "}\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Vehicle data (ACO)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)\n"
      ],
      "metadata": {
        "id": "iLIUsmCB2nTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "accuracy_data = {\n",
        "    '3.06%' : [76.86, 78.43, 77.65, 76.86, 72.55, 74.51],\n",
        "    '5.07%' : [73.56, 77.01, 73.95, 73.56, 73.18, 72.80],\n",
        "    '10.04%' : [70.18, 72.36, 70.91, 70.91, 65.82, 69.45]\n",
        "}\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Vehicle data (PSO)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)\n"
      ],
      "metadata": {
        "id": "Vi7AC1kr2gsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "accuracy_data = {\n",
        "    '3.06%' : [76.86, 78.43, 77.65, 76.86, 72.55, 74.51],\n",
        "    '5.07%' : [73.56, 77.01, 73.95, 73.56, 73.18, 72.80],\n",
        "    '10.04%' : [70.18, 72.36, 70.91, 70.91, 65.82, 69.45]\n",
        "}\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Vehicle data (SMO)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)\n"
      ],
      "metadata": {
        "id": "PEndMdYA2gsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "accuracy_data = {\n",
        "    '3.06%' : [76.86, 78.43, 77.65, 76.86, 72.55, 74.51],\n",
        "    '5.07%' : [73.56, 77.01, 73.95, 73.56, 73.18, 72.80],\n",
        "    '10.04%' : [70.18, 72.36, 70.91, 70.91, 65.82, 69.45]\n",
        "}\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Vehicle data (GA)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)\n"
      ],
      "metadata": {
        "id": "6zuAyFM72gsM"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}