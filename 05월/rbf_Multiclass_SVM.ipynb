{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### 2025.05.04"
      ],
      "metadata": {
        "id": "Ln0q6Mqz-wxI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#IRIS DATA\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.stats import zscore\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# 데이터셋 로딩\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "df = pd.DataFrame(X, columns=data.feature_names)\n",
        "df['target'] = y  # 타겟 변수 추가\n",
        "\n",
        "# 데이터 정규화\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "df_scaled = pd.DataFrame(X_scaled, columns=data.feature_names)\n",
        "\n",
        "# 기존 이상치 식별\n",
        "df_zscores = df_scaled.apply(zscore)  # Z-score 계산\n",
        "is_existing_outlier = (df_zscores.abs() > 3).any(axis=1)  # 절대값이 3 이상인 경우 이상치로 판단\n",
        "\n",
        "# 기존 이상치 수 계산\n",
        "total_existing_outliers = is_existing_outlier.sum()\n",
        "\n",
        "# 기존 이상치 비율 계산\n",
        "total_samples = len(df)\n",
        "existing_outlier_percentage = (total_existing_outliers / total_samples) * 100\n",
        "\n",
        "print(f\"기존 이상치 수: {total_existing_outliers}\")\n",
        "print(f\"기존 이상치 비율: {existing_outlier_percentage:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f90mjLw_-7nC",
        "outputId": "128ce397-90cc-402b-ac45-2206745529ab"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "기존 이상치 수: 1\n",
            "기존 이상치 비율: 0.67%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#SEGMENT DATA\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from scipy.stats import zscore\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# 데이터셋 로딩\n",
        "data = fetch_openml(name='segment', version=1)\n",
        "X = data.data\n",
        "# Use LabelEncoder to convert categorical target to numerical\n",
        "encoder = LabelEncoder()\n",
        "y = encoder.fit_transform(data.target)\n",
        "df = pd.DataFrame(X, columns=data.feature_names)\n",
        "df['target'] = y  # 타겟 변수 추가\n",
        "\n",
        "\n",
        "# 데이터 정규화\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "df_scaled = pd.DataFrame(X_scaled, columns=data.feature_names)\n",
        "\n",
        "# 이상치 생성\n",
        "num_outliers = int(0.028 * len(df))  # 이상치를 설정\n",
        "outliers = []\n",
        "\n",
        "for _ in range(num_outliers):\n",
        "    outlier_sample = df_scaled.sample(n=1).copy()  # 임의의 샘플 선택\n",
        "    for col in df_scaled.columns:\n",
        "        if np.random.rand() > 0.5:\n",
        "            # 최대값보다 큰 이상치 생성\n",
        "            outlier_sample[col] = df_scaled[col].max() + np.random.rand() * 3\n",
        "        else:\n",
        "            # 최소값보다 작은 이상치 생성\n",
        "            outlier_sample[col] = df_scaled[col].min() - np.random.rand() * 3\n",
        "    outliers.append(outlier_sample)\n",
        "\n",
        "outliers_df = pd.concat(outliers, ignore_index=True)\n",
        "\n",
        "# 이상치의 타겟 값 랜덤 할당\n",
        "outliers_df['target'] = np.random.choice(y, num_outliers)\n",
        "\n",
        "# 원본 데이터에 이상치 추가\n",
        "df_extended = pd.concat([df, outliers_df], ignore_index=True)\n",
        "\n",
        "# 이상치 추가 후 다시 정규화\n",
        "X_extended_scaled = scaler.fit_transform(df_extended.drop(columns=['target']))\n",
        "df_extended_scaled = pd.DataFrame(X_extended_scaled, columns=data.feature_names)\n",
        "\n",
        "# 새로운 X, y 정의\n",
        "X = df_extended_scaled.values  # NumPy 배열로 변환\n",
        "y = df_extended['target'].values  # 타겟 값 추출\n",
        "\n",
        "# 기존 이상치 식별\n",
        "df_zscores = df_scaled.apply(zscore)  # Z-score 계산\n",
        "is_existing_outlier = (df_zscores.abs() > 3).any(axis=1)  # 절대값이 3 이상인 경우 이상치로 판단\n",
        "\n",
        "total_existing_outliers = is_existing_outlier.sum()  # 기존 이상치 수\n",
        "total_outliers = total_existing_outliers + num_outliers  # 총 이상치 수\n",
        "total_samples = len(df_extended)  # 전체 샘플 수\n",
        "total_outlier_percentage = (total_outliers / total_samples) * 100  # 이상치 비율 계산\n",
        "\n",
        "# 데이터 확인\n",
        "print(f\"확장된 데이터셋 크기: {X.shape}\")\n",
        "print(f\"확장된 타겟 크기: {y.shape}\")\n",
        "print(f\"이상치가 포함된 데이터셋의 총 샘플 수: {len(X)}\")\n",
        "print(f\"전체 이상치 비율: {total_outlier_percentage:.2f}%\")\n",
        "print(f\"전체 이상치 개수: {total_outliers}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "offuyw-s_c-m",
        "outputId": "5a55c245-d1c7-4716-d68a-9239d99f4aac"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "확장된 데이터셋 크기: (2374, 19)\n",
            "확장된 타겟 크기: (2374,)\n",
            "이상치가 포함된 데이터셋의 총 샘플 수: 2374\n",
            "전체 이상치 비율: 10.07%\n",
            "전체 이상치 개수: 239\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#VEHICLE DATA\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from scipy.stats import zscore\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# 데이터셋 로딩\n",
        "data = fetch_openml(name='vehicle', version=1)\n",
        "X = data.data\n",
        "encoder = LabelEncoder()\n",
        "y = encoder.fit_transform(data.target)\n",
        "df = pd.DataFrame(X, columns=data.feature_names)\n",
        "df['target'] = y  # 타겟 변수 추가\n",
        "\n",
        "# 데이터 정규화\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "df_scaled = pd.DataFrame(X_scaled, columns=data.feature_names)\n",
        "\n",
        "# 기존 이상치 식별\n",
        "df_zscores = df_scaled.apply(zscore)  # Z-score 계산\n",
        "is_existing_outlier = (df_zscores.abs() > 3).any(axis=1)  # 절대값이 3 이상인 경우 이상치로 판단\n",
        "\n",
        "# 기존 이상치 수 계산\n",
        "total_existing_outliers = is_existing_outlier.sum()\n",
        "\n",
        "# 기존 이상치 비율 계산\n",
        "total_samples = len(df)\n",
        "existing_outlier_percentage = (total_existing_outliers / total_samples) * 100\n",
        "\n",
        "print(f\"기존 이상치 수: {total_existing_outliers}\")\n",
        "print(f\"기존 이상치 비율: {existing_outlier_percentage:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2kBdVXjvAYXY",
        "outputId": "31b2deff-5fcf-4a81-a003-ac30ddf86ae3"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "기존 이상치 수: 22\n",
            "기존 이상치 비율: 2.60%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "class LBFGS_SVM:\n",
        "    def __init__(self, C=0.01, svm_type='L2', c=1.0, gamma=0.1):\n",
        "        self.C = C\n",
        "        self.svm_type = svm_type\n",
        "        self.c = c\n",
        "        self.gamma = gamma\n",
        "        self.alpha = None\n",
        "        self.b_best = None\n",
        "        self.X_train = None\n",
        "        self.y_train = None\n",
        "\n",
        "    def rbf_kernel(self, X1, X2):\n",
        "        if X1.ndim == 1:\n",
        "            X1 = X1.reshape(1, -1)\n",
        "        if X2.ndim == 1:\n",
        "            X2 = X2.reshape(1, -1)\n",
        "        dists = np.sum((X1[:, np.newaxis] - X2[np.newaxis, :]) ** 2, axis=2)\n",
        "        return np.exp(-self.gamma * dists)\n",
        "\n",
        "    def compute_slack_term(self, xi):\n",
        "        if self.svm_type == 'L1':\n",
        "            return np.abs(xi)\n",
        "        elif self.svm_type == 'L2':\n",
        "            return xi ** 2\n",
        "        elif self.svm_type == 'Fair':\n",
        "            return np.sum(self.c**2 * ((xi / self.c) - np.log(1 + xi / self.c)))\n",
        "        elif self.svm_type == 'Cauchy':\n",
        "            return np.sum((self.c**2 / 2) * np.log(1 + (xi / self.c)**2))\n",
        "        elif self.svm_type == 'Welsch':\n",
        "            return np.sum((self.c**2 / 2) * (1 - np.exp(-(xi / self.c)**2)))\n",
        "        elif self.svm_type == 'Geman-McClure':\n",
        "            return np.sum(((xi**2)/2) / (1 + xi**2))\n",
        "        else:\n",
        "            raise ValueError(\"Unknown SVM type.\")\n",
        "\n",
        "    def fitness(self, alpha, K, y):\n",
        "        return 0.5 * np.sum(alpha[:, None] * alpha[None, :] * y[:, None] * y[None, :] * K) - np.sum(alpha)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.X_train = X\n",
        "        self.y_train = y\n",
        "        n_samples = X.shape[0]\n",
        "        K = self.rbf_kernel(X, X)\n",
        "\n",
        "        initial_alpha = np.zeros(n_samples)\n",
        "        bounds = [(0, self.C) for _ in range(n_samples)]\n",
        "        constraints = {'type': 'eq', 'fun': lambda alpha: np.dot(alpha, y)}\n",
        "\n",
        "        result = minimize(self.fitness, initial_alpha, args=(K, y), bounds=bounds, constraints=constraints)\n",
        "\n",
        "        if not result.success:\n",
        "            raise ValueError(\"Dual optimization failed: \" + result.message)\n",
        "\n",
        "        self.alpha = result.x\n",
        "        sv = self.alpha > 1e-5\n",
        "        self.alpha_sv = self.alpha[sv]\n",
        "        self.support_vectors_ = X[sv]\n",
        "        self.support_vector_labels_ = y[sv]\n",
        "        K_sv = self.rbf_kernel(self.support_vectors_, self.support_vectors_)\n",
        "        self.b_best = np.mean(\n",
        "            self.support_vector_labels_ - np.sum(self.alpha_sv * self.support_vector_labels_ * K_sv, axis=1)\n",
        "        )\n",
        "\n",
        "        # 슬랙 비용 계산 (분석용)\n",
        "        margins = y * self.decision_function(X)\n",
        "        slack = np.maximum(0, 1 - margins)\n",
        "        self.slack_cost = self.compute_slack_term(slack)\n",
        "\n",
        "    def decision_function(self, X):\n",
        "        K = self.rbf_kernel(X, self.support_vectors_)\n",
        "        return np.sum(self.alpha_sv * self.support_vector_labels_ * K, axis=1) + self.b_best\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.sign(self.decision_function(X))\n"
      ],
      "metadata": {
        "id": "hEn5_3mY-7p0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_ovo(X, y, C, svm_type, gamma=0.1):\n",
        "    classes = np.unique(y)\n",
        "    models = []\n",
        "    class_pairs = list(combinations(classes, 2))\n",
        "    for (cl1, cl2) in class_pairs:\n",
        "        mask = (y == cl1) | (y == cl2)\n",
        "        X_bin = X[mask]\n",
        "        y_bin = y[mask]\n",
        "        y_bin = (y_bin == cl1).astype(int) * 2 - 1\n",
        "        model = LBFGS_SVM(C=C, svm_type=svm_type, gamma=gamma)\n",
        "        model.fit(X_bin, y_bin)\n",
        "        models.append(((cl1, cl2), model))  # 모델 객체 자체 저장\n",
        "    return models\n",
        "\n",
        "def predict_ovo(X, models):\n",
        "    votes = np.zeros((X.shape[0], len(models)))\n",
        "    classes = np.unique(np.hstack([pair for pair, _ in models]))\n",
        "    for i, ((cl1, cl2), model) in enumerate(models):\n",
        "        preds = model.predict(X)  # 모델 객체의 predict 메서드 호출\n",
        "        votes[:, i] = np.where(preds == 1, cl1, cl2)\n",
        "    final_predictions = []\n",
        "    for j in range(votes.shape[0]):\n",
        "        valid_votes = votes[j][votes[j] >= 0]\n",
        "        if len(valid_votes) > 0:\n",
        "            bincount = np.bincount(valid_votes.astype(int))\n",
        "            final_predictions.append(bincount.argmax())\n",
        "        else:\n",
        "            final_predictions.append(np.random.choice(classes))\n",
        "    return np.array(final_predictions)"
      ],
      "metadata": {
        "id": "YjclbfY8-7tw"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from itertools import combinations\n",
        "\n",
        "results = []\n",
        "\n",
        "# 데이터 전처리 및 분할\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# OvO 방식을 사용하여 각 SVM 유형에 대해 학습 및 평가\n",
        "for svm_type in ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']:\n",
        "    models_ovo = train_ovo(X_train, y_train, C=1.0, svm_type=svm_type)\n",
        "    y_pred_ovo = predict_ovo(X_test, models_ovo)\n",
        "\n",
        "    # y_pred_ovo 변수를 정의한 후에 데이터 타입 확인 및 변환\n",
        "    if y_test.dtype != np.int64:\n",
        "        y_test = y_test.astype(int)\n",
        "\n",
        "    if y_pred_ovo.dtype != np.int64:\n",
        "        y_pred_ovo = y_pred_ovo.astype(int)\n",
        "\n",
        "    accuracy_ovo = accuracy_score(y_test, y_pred_ovo)\n",
        "    results.append((svm_type, round(accuracy_ovo * 100, 2)))\n",
        "\n",
        "# 결과를 DataFrame으로 변환 및 출력\n",
        "results_df = pd.DataFrame(results, columns=['SVM Type', 'Accuracy'])\n",
        "print(results_df['Accuracy'].tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hw-yhX68-7wW",
        "outputId": "fd2d452e-e2e5-4013-ef71-5f69e4d34d54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n",
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n",
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-34-4e1a0b476768>\", line 18, in <cell line: 0>\n",
            "    models_ovo = train_ovo(X_train, y_train, C=1.0, svm_type=svm_type)\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-32-4f19c4cb9438>\", line 13, in train_ovo\n",
            "    model.fit(X_bin, y_bin)\n",
            "  File \"<ipython-input-31-5cd0f25d0c27>\", line None, in fit\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/inspect.py\", line 1739, in getinnerframes\n",
            "    traceback_info = getframeinfo(tb, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/inspect.py\", line 1686, in getframeinfo\n",
            "    start = lineno - 1 - context//2\n",
            "            ~~~~~~~^~~\n",
            "TypeError: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-34-4e1a0b476768>\", line 18, in <cell line: 0>\n",
            "    models_ovo = train_ovo(X_train, y_train, C=1.0, svm_type=svm_type)\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-32-4f19c4cb9438>\", line 13, in train_ovo\n",
            "    model.fit(X_bin, y_bin)\n",
            "  File \"<ipython-input-31-5cd0f25d0c27>\", line None, in fit\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3575, in run_code\n",
            "    self.showtraceback(running_compiled_code=True)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2101, in showtraceback\n",
            "    stb = self.InteractiveTB.structured_traceback(etype,\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n",
            "    return FormattedTB.structured_traceback(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n",
            "    return VerboseTB.structured_traceback(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1124, in structured_traceback\n",
            "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n",
            "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
            "                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n",
            "    return len(records), 0\n",
            "           ^^^^^^^^^^^^\n",
            "TypeError: object of type 'NoneType' has no len()\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/inspect.py\", line 1739, in getinnerframes\n",
            "    traceback_info = getframeinfo(tb, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/inspect.py\", line 1686, in getframeinfo\n",
            "    start = lineno - 1 - context//2\n",
            "            ~~~~~~~^~~\n",
            "TypeError: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-34-4e1a0b476768>\", line 18, in <cell line: 0>\n",
            "    models_ovo = train_ovo(X_train, y_train, C=1.0, svm_type=svm_type)\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-32-4f19c4cb9438>\", line 13, in train_ovo\n",
            "    model.fit(X_bin, y_bin)\n",
            "  File \"<ipython-input-31-5cd0f25d0c27>\", line None, in fit\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3575, in run_code\n",
            "    self.showtraceback(running_compiled_code=True)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2101, in showtraceback\n",
            "    stb = self.InteractiveTB.structured_traceback(etype,\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n",
            "    return FormattedTB.structured_traceback(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n",
            "    return VerboseTB.structured_traceback(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1124, in structured_traceback\n",
            "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n",
            "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
            "                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n",
            "    return len(records), 0\n",
            "           ^^^^^^^^^^^^\n",
            "TypeError: object of type 'NoneType' has no len()\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3492, in run_ast_nodes\n",
            "    self.showtraceback()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2101, in showtraceback\n",
            "    stb = self.InteractiveTB.structured_traceback(etype,\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n",
            "    return FormattedTB.structured_traceback(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n",
            "    return VerboseTB.structured_traceback(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1142, in structured_traceback\n",
            "    formatted_exceptions += self.format_exception_as_a_whole(etype, evalue, etb, lines_of_context,\n",
            "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n",
            "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
            "                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n",
            "    return len(records), 0\n",
            "           ^^^^^^^^^^^^\n",
            "TypeError: object of type 'NoneType' has no len()\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/inspect.py\", line 1739, in getinnerframes\n",
            "    traceback_info = getframeinfo(tb, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/inspect.py\", line 1686, in getframeinfo\n",
            "    start = lineno - 1 - context//2\n",
            "            ~~~~~~~^~~\n",
            "TypeError: unsupported operand type(s) for -: 'NoneType' and 'int'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "IRIS는 데이터가 너무 간단해서 그래도 조금 기다리면 실행 가능.\n",
        "\n",
        "SEGMENT는 대부분의 코드 다 실행이 바로 됐는데 정말 오래 걸린다.\n",
        "\n",
        "(SEGMENT & M-estimator 6개 & OVO & L-BFGS-B & rbf kernel) 해서 이상치 별로 돌리기 => 원데이터 하나가 15분이 지나도 실행 완료 안됨.\n",
        "\n",
        "한 데이터당 6개 * 3개 * 최적화기법 5개 * 이상치 종류 3개 =>\n",
        "270번은 돌려야하는데 비효율적이어서 다른 대안이 필요할 것 같아요."
      ],
      "metadata": {
        "id": "Qz3YlQBsApXw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnXtrxuUswFU"
      },
      "source": [
        "#**Optimization & OvO**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsi4CNobs8U3",
        "outputId": "5e257ac8-08bb-42ac-e102-f7eb678b56cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ucimlrepo\n",
            "  Downloading ucimlrepo-0.0.7-py3-none-any.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ucimlrepo) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2020.12.5 in /usr/local/lib/python3.11/dist-packages (from ucimlrepo) (2025.4.26)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->ucimlrepo) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->ucimlrepo) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->ucimlrepo) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->ucimlrepo) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->ucimlrepo) (1.17.0)\n",
            "Downloading ucimlrepo-0.0.7-py3-none-any.whl (8.0 kB)\n",
            "Installing collected packages: ucimlrepo\n",
            "Successfully installed ucimlrepo-0.0.7\n"
          ]
        }
      ],
      "source": [
        "pip install ucimlrepo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZT47Jpt0tLI9"
      },
      "source": [
        "###L-BFGS-B (Limited-memory Broyden-Fletcher-Goldfarb-Shanno with Box constraints)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vVayD5ZkqfsS"
      },
      "outputs": [],
      "source": [
        "segment 원데이터 [14.87, 14.87, 92.01, 91.87, 92.01, 91.87]\n",
        "\n",
        "vehicle 원데이터 [22.83, 22.83, 73.23, 73.23, 73.23, 73.23]\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "class LBFGS_SVM:\n",
        "    def __init__(self, C=0.01, svm_type='L2', c=1.0, gamma=0.1):\n",
        "        self.C = C\n",
        "        self.svm_type = svm_type\n",
        "        self.c = c\n",
        "        self.gamma = gamma\n",
        "        self.alpha = None\n",
        "        self.b_best = None\n",
        "        self.X_train = None\n",
        "        self.y_train = None\n",
        "\n",
        "    def rbf_kernel(self, X1, X2):\n",
        "        if X1.ndim == 1:\n",
        "            X1 = X1.reshape(1, -1)\n",
        "        if X2.ndim == 1:\n",
        "            X2 = X2.reshape(1, -1)\n",
        "        dists = np.sum((X1[:, np.newaxis] - X2[np.newaxis, :]) ** 2, axis=2)\n",
        "        return np.exp(-self.gamma * dists)\n",
        "\n",
        "    def compute_slack_term(self, xi):\n",
        "        if self.svm_type == 'L1':\n",
        "            return np.abs(xi)\n",
        "        elif self.svm_type == 'L2':\n",
        "            return xi ** 2\n",
        "        elif self.svm_type == 'Fair':\n",
        "            return np.sum(self.c**2 * ((xi / self.c) - np.log(1 + xi / self.c)))\n",
        "        elif self.svm_type == 'Cauchy':\n",
        "            return np.sum((self.c**2 / 2) * np.log(1 + (xi / self.c)**2))\n",
        "        elif self.svm_type == 'Welsch':\n",
        "            return np.sum((self.c**2 / 2) * (1 - np.exp(-(xi / self.c)**2)))\n",
        "        elif self.svm_type == 'Geman-McClure':\n",
        "            return np.sum(((xi**2)/2) / (1 + xi**2))\n",
        "        else:\n",
        "            raise ValueError(\"Unknown SVM type.\")\n",
        "\n",
        "    def fitness(self, alpha, K, y):\n",
        "        return 0.5 * np.sum(alpha[:, None] * alpha[None, :] * y[:, None] * y[None, :] * K) - np.sum(alpha)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.X_train = X\n",
        "        self.y_train = y\n",
        "        n_samples = X.shape[0]\n",
        "        K = self.rbf_kernel(X, X)\n",
        "\n",
        "        initial_alpha = np.zeros(n_samples)\n",
        "        bounds = [(0, self.C) for _ in range(n_samples)]\n",
        "        constraints = {'type': 'eq', 'fun': lambda alpha: np.dot(alpha, y)}\n",
        "\n",
        "        result = minimize(self.fitness, initial_alpha, args=(K, y), bounds=bounds, constraints=constraints)\n",
        "\n",
        "        if not result.success:\n",
        "            raise ValueError(\"Dual optimization failed: \" + result.message)\n",
        "\n",
        "        self.alpha = result.x\n",
        "        sv = self.alpha > 1e-5\n",
        "        self.alpha_sv = self.alpha[sv]\n",
        "        self.support_vectors_ = X[sv]\n",
        "        self.support_vector_labels_ = y[sv]\n",
        "        K_sv = self.rbf_kernel(self.support_vectors_, self.support_vectors_)\n",
        "        self.b_best = np.mean(\n",
        "            self.support_vector_labels_ - np.sum(self.alpha_sv * self.support_vector_labels_ * K_sv, axis=1)\n",
        "        )\n",
        "\n",
        "        # 슬랙 비용 계산 (분석용)\n",
        "        margins = y * self.decision_function(X)\n",
        "        slack = np.maximum(0, 1 - margins)\n",
        "        self.slack_cost = self.compute_slack_term(slack)\n",
        "\n",
        "    def decision_function(self, X):\n",
        "        K = self.rbf_kernel(X, self.support_vectors_)\n",
        "        return np.sum(self.alpha_sv * self.support_vector_labels_ * K, axis=1) + self.b_best\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.sign(self.decision_function(X))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pkaTtUyutCrt"
      },
      "outputs": [],
      "source": [
        "def train_ovo(X, y, C, svm_type, gamma=0.1):\n",
        "    classes = np.unique(y)\n",
        "    models = []\n",
        "    class_pairs = list(combinations(classes, 2))\n",
        "    for (cl1, cl2) in class_pairs:\n",
        "        mask = (y == cl1) | (y == cl2)\n",
        "        X_bin = X[mask]\n",
        "        y_bin = y[mask]\n",
        "        y_bin = (y_bin == cl1).astype(int) * 2 - 1\n",
        "        model = LBFGS_SVM(C=C, svm_type=svm_type, gamma=gamma)\n",
        "        model.fit(X_bin, y_bin)\n",
        "        models.append(((cl1, cl2), model))  # 모델 객체 자체 저장\n",
        "    return models\n",
        "\n",
        "def predict_ovo(X, models):\n",
        "    votes = np.zeros((X.shape[0], len(models)))\n",
        "    classes = np.unique(np.hstack([pair for pair, _ in models]))\n",
        "    for i, ((cl1, cl2), model) in enumerate(models):\n",
        "        preds = model.predict(X)  # 모델 객체의 predict 메서드 호출\n",
        "        votes[:, i] = np.where(preds == 1, cl1, cl2)\n",
        "    final_predictions = []\n",
        "    for j in range(votes.shape[0]):\n",
        "        valid_votes = votes[j][votes[j] >= 0]\n",
        "        if len(valid_votes) > 0:\n",
        "            bincount = np.bincount(valid_votes.astype(int))\n",
        "            final_predictions.append(bincount.argmax())\n",
        "        else:\n",
        "            final_predictions.append(np.random.choice(classes))\n",
        "    return np.array(final_predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DYvMYSttRkI"
      },
      "source": [
        "###1. Genetic Algorithm (GA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xn8t5QNstTVR"
      },
      "outputs": [],
      "source": [
        "class GA_SVM:\n",
        "    def __init__(self, C=1.0, pop_size=20, max_iter=100, mutation_rate=0.1, crossover_rate=0.7, svm_type='L2', c=1.0):\n",
        "        self.C = C  # 정규화 상수\n",
        "        self.pop_size = pop_size  # 개체 수\n",
        "        self.max_iter = max_iter  # 세대 수\n",
        "        self.mutation_rate = mutation_rate  # 돌연변이 확률\n",
        "        self.crossover_rate = crossover_rate  # 교차 확률\n",
        "        self.svm_type = svm_type  # 'L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure'\n",
        "        self.w_best = None\n",
        "        self.b_best = None\n",
        "        self.c = c\n",
        "\n",
        "    def compute_slack_term(self, xi):\n",
        "        if self.svm_type == 'L1':\n",
        "            return np.abs(xi)\n",
        "        elif self.svm_type == 'L2':\n",
        "            return xi ** 2\n",
        "        elif self.svm_type == 'Fair':\n",
        "            return np.sum(self.c**2 * ((xi / self.c) - np.log(1 + xi / self.c)))\n",
        "        elif self.svm_type == 'Cauchy':\n",
        "            return np.sum((self.c**2 / 2) * np.log(1 + (xi / self.c)**2))\n",
        "        elif self.svm_type == 'Welsch':\n",
        "            return np.sum((self.c**2 / 2) * (1 - np.exp(-(xi / self.c)**2)))\n",
        "        elif self.svm_type == 'Geman-McClure':\n",
        "            return np.sum(((xi**2)/2) / (1 + xi**2))\n",
        "        else:\n",
        "            raise ValueError(\"Unknown SVM type. Choose from ['L1', 'L2', 'Cauchy', 'Fair', 'Welsch', 'Geman-McClure'].\")\n",
        "\n",
        "    def fitness(self, w, b, X, y):\n",
        "        margins = y * (np.dot(X, w) + b)\n",
        "        slack = np.maximum(0, 1 - margins)\n",
        "        slack_term = self.compute_slack_term(slack)\n",
        "        regularization = 0.5 * np.dot(w, w)\n",
        "        return regularization + self.C * np.sum(slack_term)\n",
        "\n",
        "    def initialize_population(self, n_features):\n",
        "        population = [(np.random.randn(n_features), np.random.randn()) for _ in range(self.pop_size)]\n",
        "        return population\n",
        "\n",
        "    def selection(self, population, fitness_values):\n",
        "        probabilities = 1 / (fitness_values + 1e-6)\n",
        "        probabilities /= probabilities.sum()\n",
        "        selected_indices = np.random.choice(len(population), size=len(population), p=probabilities)\n",
        "        return [population[i] for i in selected_indices]\n",
        "\n",
        "    def crossover(self, parent1, parent2):\n",
        "        w1, b1 = parent1\n",
        "        w2, b2 = parent2\n",
        "        if np.random.rand() < self.crossover_rate:\n",
        "            point = np.random.randint(len(w1))\n",
        "            new_w = np.concatenate((w1[:point], w2[point:]))\n",
        "            new_b = (b1 + b2) / 2\n",
        "        else:\n",
        "            new_w, new_b = w1.copy(), b1\n",
        "        return new_w, new_b\n",
        "\n",
        "    def mutation(self, w, b):\n",
        "        if np.random.rand() < self.mutation_rate:\n",
        "            mutation_vector = np.random.randn(*w.shape) * 0.1\n",
        "            w += mutation_vector\n",
        "            b += np.random.randn() * 0.1\n",
        "        return w, b\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        population = self.initialize_population(n_features)\n",
        "        fitness_values = np.array([self.fitness(w, b, X, y) for w, b in population])\n",
        "        np.random.seed(42)  # 시드 설정\n",
        "        for _ in range(self.max_iter):\n",
        "            selected_population = self.selection(population, fitness_values)\n",
        "            new_population = []\n",
        "            for i in range(0, len(selected_population), 2):\n",
        "                p1, p2 = selected_population[i], selected_population[(i + 1) % len(selected_population)]\n",
        "                offspring1 = self.crossover(p1, p2)\n",
        "                offspring2 = self.crossover(p2, p1)\n",
        "                new_population.append(self.mutation(*offspring1))\n",
        "                new_population.append(self.mutation(*offspring2))\n",
        "            population = new_population\n",
        "            fitness_values = np.array([self.fitness(w, b, X, y) for w, b in population])\n",
        "        best_idx = np.argmin(fitness_values)\n",
        "        self.w_best, self.b_best = population[best_idx]\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.sign(np.dot(X, self.w_best) + self.b_best)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AAk-jyeCtTYO"
      },
      "outputs": [],
      "source": [
        "def train_ovo(X, y, C, svm_type):\n",
        "    classes = np.unique(y)\n",
        "    models = []\n",
        "    class_pairs = list(combinations(classes, 2))\n",
        "    for (cl1, cl2) in class_pairs:\n",
        "        mask = (y == cl1) | (y == cl2)\n",
        "        X_bin = X[mask]\n",
        "        y_bin = y[mask]\n",
        "        y_bin = (y_bin == cl1).astype(int) * 2 - 1\n",
        "        model = GA_SVM(C=C, svm_type=svm_type)\n",
        "        model.fit(X_bin, y_bin)\n",
        "        w = model.w_best\n",
        "        b = model.b_best\n",
        "        models.append(((cl1, cl2), (w, b)))\n",
        "    return models\n",
        "\n",
        "def predict_ovo(X, models):\n",
        "    votes = np.zeros((X.shape[0], len(models)))\n",
        "    classes = np.unique(np.hstack([pair for pair, _ in models]))\n",
        "    for i, ((cl1, cl2), (w, b)) in enumerate(models):\n",
        "        preds = np.sign(X.dot(w) + b)\n",
        "        votes[:, i] = np.where(preds == 1, cl1, cl2)\n",
        "    final_predictions = []\n",
        "    for j in range(votes.shape[0]):\n",
        "        valid_votes = votes[j][votes[j] >= 0]\n",
        "        if len(valid_votes) > 0:\n",
        "            bincount = np.bincount(valid_votes.astype(int))\n",
        "            final_predictions.append(bincount.argmax())\n",
        "        else:\n",
        "            final_predictions.append(np.random.choice(classes))\n",
        "    return np.array(final_predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZD-fWtrtR1R"
      },
      "source": [
        "###SMO (Sequential Minial Optimization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "yoZ9zBsFtT6f"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class SMO_SVM:\n",
        "    def __init__(self, C=1.0, tol=1e-3, max_iter=1000, svm_type='L2', c=1.0, lr=0.01, gamma=None):\n",
        "        self.C = C\n",
        "        self.tol = tol\n",
        "        self.max_iter = max_iter\n",
        "        self.svm_type = svm_type\n",
        "        self.c = c\n",
        "        self.lr = lr\n",
        "        self.gamma = gamma  # gamma 추가\n",
        "        self.b_best = None\n",
        "\n",
        "    def kernel_function(self, X, Y):\n",
        "        gamma = self.gamma if self.gamma is not None else 1.0 / X.shape[1]\n",
        "        if X.ndim == 1:\n",
        "            X = X.reshape(1, -1)\n",
        "        if Y.ndim == 1:\n",
        "            Y = Y.reshape(1, -1)\n",
        "        K = np.zeros((X.shape[0], Y.shape[0]))\n",
        "        for i in range(X.shape[0]):\n",
        "            for j in range(Y.shape[0]):\n",
        "                diff = X[i] - Y[j]\n",
        "                K[i, j] = np.exp(-gamma * np.dot(diff, diff))\n",
        "        return K\n",
        "\n",
        "    def compute_slack_term(self, xi):\n",
        "        xi = np.maximum(xi, 1e-6)\n",
        "        if self.svm_type == 'Fair':\n",
        "            return np.sum(self.c**2 * ((xi / self.c) - np.log(1 + xi / self.c)))\n",
        "        elif self.svm_type == 'Cauchy':\n",
        "            return np.sum((self.c**2 / 2) * np.log(1 + (xi / self.c)**2))\n",
        "        elif self.svm_type == 'Welsch':\n",
        "            return np.sum((self.c**2 / 2) * (1 - np.exp(-(xi / self.c)**2)))\n",
        "        elif self.svm_type == 'Geman-McClure':\n",
        "            return np.sum(((xi**2)/2) / (1 + xi**2))\n",
        "        elif self.svm_type == 'L1':\n",
        "            return np.abs(xi)\n",
        "        elif self.svm_type == 'L2':\n",
        "            return xi ** 2\n",
        "        else:\n",
        "            raise ValueError(\"Unknown SVM type. Choose from ['L1', 'L2', 'Cauchy', 'Fair', 'Welsch', 'Geman-McClure'].\")\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples = X.shape[0]\n",
        "        self.alpha = np.zeros(n_samples)\n",
        "        self.b = 0\n",
        "        self.X_train = X\n",
        "        self.y_train = y\n",
        "\n",
        "        K = self.kernel_function(X, X)\n",
        "\n",
        "        for _ in range(self.max_iter):\n",
        "            for i in range(n_samples):\n",
        "                margin = y[i] * (np.sum(self.alpha * y * K[:, i]) + self.b)\n",
        "                slack = max(0, 1 - margin)\n",
        "                slack_term = self.compute_slack_term(slack)\n",
        "\n",
        "                if slack > 0:\n",
        "                    delta_alpha = self.C * (1 - margin) - slack_term\n",
        "                    self.alpha[i] = np.clip(self.alpha[i] + self.lr * delta_alpha, 0, self.C)\n",
        "                    self.b += self.lr * self.alpha[i] * y[i]\n",
        "\n",
        "        self.b_best = self.b\n",
        "\n",
        "    def predict(self, X):\n",
        "        K = self.kernel_function(X, self.X_train)\n",
        "        decision = np.dot(K, self.alpha * self.y_train) + self.b_best\n",
        "        return np.sign(decision)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "mXqyiWhRtT9w"
      },
      "outputs": [],
      "source": [
        "def train_ovo(X, y, C, svm_type):\n",
        "    classes = np.unique(y)\n",
        "    models = []\n",
        "    class_pairs = list(combinations(classes, 2))\n",
        "    for (cl1, cl2) in class_pairs:\n",
        "        mask = (y == cl1) | (y == cl2)\n",
        "        X_bin = X[mask]\n",
        "        y_bin = y[mask]\n",
        "        y_bin = (y_bin == cl1).astype(int) * 2 - 1\n",
        "        X_bin = X_bin.to_numpy() if isinstance(X_bin, pd.Series) else X_bin\n",
        "        y_bin = y_bin.to_numpy() if isinstance(y_bin, pd.Series) else y_bin\n",
        "        model = SMO_SVM(C=C, svm_type=svm_type)\n",
        "        model.fit(X_bin, y_bin)\n",
        "        models.append(((cl1, cl2), model))  # 모델 전체 저장\n",
        "    return models\n",
        "\n",
        "\n",
        "def predict_ovo(X, models):\n",
        "    votes = np.zeros((X.shape[0], len(models)))\n",
        "    classes = np.unique(np.hstack([pair for pair, _ in models]))\n",
        "    for i, ((cl1, cl2), model) in enumerate(models):\n",
        "        preds = model.predict(X)\n",
        "        votes[:, i] = np.where(preds == 1, cl1, cl2)\n",
        "    final_predictions = []\n",
        "    for j in range(votes.shape[0]):\n",
        "        valid_votes = votes[j][votes[j] >= 0]\n",
        "        if len(valid_votes) > 0:\n",
        "            bincount = np.bincount(valid_votes.astype(int))\n",
        "            final_predictions.append(bincount.argmax())\n",
        "        else:\n",
        "            final_predictions.append(np.random.choice(classes))\n",
        "    return np.array(final_predictions)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pc7uliY_tSKB"
      },
      "source": [
        "###PSO (Particle Swarm Optimization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xe9iCzf4tUxE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from itertools import combinations\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "\n",
        "class PSO_SVM:\n",
        "    def __init__(self, C=1.0, num_particles=30, max_iter=100, svm_type='L2', c=1.0):\n",
        "        self.C = C  # 정규화 상수\n",
        "        self.num_particles = num_particles  # PSO 입자 개수\n",
        "        self.max_iter = max_iter  # PSO 반복 횟수\n",
        "        self.svm_type = svm_type  # L1 또는 L2\n",
        "        self.w_best = None\n",
        "        self.b_best = None\n",
        "        self.c = c\n",
        "\n",
        "    def compute_slack_term(self, xi):\n",
        "        \"\"\" SVM 타입별 슬랙 변수 비용 계산 \"\"\"\n",
        "        if self.svm_type == 'L1':\n",
        "            return np.abs(xi)\n",
        "        elif self.svm_type == 'L2':\n",
        "            return xi ** 2\n",
        "        elif self.svm_type == 'Fair':\n",
        "            return np.sum(self.c**2 * ((xi / self.c) - np.log(1 + xi / self.c)))\n",
        "        elif self.svm_type == 'Cauchy':\n",
        "            return np.sum((self.c**2 / 2) * np.log(1 + (xi / self.c)**2))\n",
        "        elif self.svm_type == 'Welsch':\n",
        "            return np.sum((self.c**2 / 2) * (1 - np.exp(-(xi / self.c)**2)))\n",
        "        elif self.svm_type == 'Geman-McClure':\n",
        "            return np.sum(((xi**2)/2) / (1 + xi**2))\n",
        "        else:\n",
        "            raise ValueError(\"Unknown SVM type. Choose from ['L1', 'L2', 'Cauchy', 'Fair', 'Welsch', 'Geman-McClure'].\")\n",
        "\n",
        "    def fitness(self, w, b, X, y):\n",
        "        \"\"\" 손실 함수 계산 (Hinge Loss 기반) \"\"\"\n",
        "        margins = y * (np.dot(X, w) + b)\n",
        "        slack = np.maximum(0, 1 - margins)  # Hinge Loss 적용\n",
        "        slack_term = self.compute_slack_term(slack)\n",
        "        regularization = 0.5 * np.dot(w, w)  # L2 Regularization\n",
        "        return regularization + self.C * np.sum(slack_term)  # 최소화할 손실 값\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # PSO 초기화\n",
        "        w_particles = np.random.randn(self.num_particles, n_features)  # 초기 w 값\n",
        "        b_particles = np.random.randn(self.num_particles)  # 초기 b 값\n",
        "        velocities_w = np.random.randn(self.num_particles, n_features) * 0.1  # 속도 초기화\n",
        "        velocities_b = np.random.randn(self.num_particles) * 0.1\n",
        "\n",
        "        # 개별 최적 및 전역 최적 초기화\n",
        "        p_best_w = np.copy(w_particles)\n",
        "        p_best_b = np.copy(b_particles)\n",
        "        p_best_scores = np.array([self.fitness(w, b, X, y) for w, b in zip(w_particles, b_particles)])\n",
        "\n",
        "        g_best_index = np.argmin(p_best_scores)\n",
        "        g_best_w = p_best_w[g_best_index]\n",
        "        g_best_b = p_best_b[g_best_index]\n",
        "\n",
        "        # PSO 학습 진행\n",
        "        w_inertia = 0.7  # 관성 계수\n",
        "        c1 = 1.5  # 개인 최적화 계수\n",
        "        c2 = 1.5  # 글로벌 최적화 계수\n",
        "\n",
        "        for _ in range(self.max_iter):\n",
        "            for i in range(self.num_particles):\n",
        "                r1, r2 = np.random.rand(), np.random.rand()\n",
        "                velocities_w[i] = (w_inertia * velocities_w[i] +\n",
        "                                   c1 * r1 * (p_best_w[i] - w_particles[i]) +\n",
        "                                   c2 * r2 * (g_best_w - w_particles[i]))\n",
        "                velocities_b[i] = (w_inertia * velocities_b[i] +\n",
        "                                   c1 * r1 * (p_best_b[i] - b_particles[i]) +\n",
        "                                   c2 * r2 * (g_best_b - b_particles[i]))\n",
        "\n",
        "                # 업데이트된 위치\n",
        "                w_particles[i] += velocities_w[i]\n",
        "                b_particles[i] += velocities_b[i]\n",
        "\n",
        "                # 새로운 피트니스 값 계산\n",
        "                new_fitness = self.fitness(w_particles[i], b_particles[i], X, y)\n",
        "\n",
        "                # 최적값 갱신\n",
        "                if new_fitness < p_best_scores[i]:\n",
        "                    p_best_w[i] = w_particles[i]\n",
        "                    p_best_b[i] = b_particles[i]\n",
        "                    p_best_scores[i] = new_fitness\n",
        "\n",
        "            # 전체 최적 갱신\n",
        "            g_best_index = np.argmin(p_best_scores)\n",
        "            g_best_w = p_best_w[g_best_index]\n",
        "            g_best_b = p_best_b[g_best_index]\n",
        "\n",
        "        self.w_best = g_best_w\n",
        "        self.b_best = g_best_b\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.sign(np.dot(X, self.w_best) + self.b_best)\n",
        "def train_ovo(X, y, C, svm_type):\n",
        "    classes = np.unique(y)\n",
        "    models = []\n",
        "    class_pairs = list(combinations(classes, 2))\n",
        "    for (cl1, cl2) in class_pairs:\n",
        "        mask = (y == cl1) | (y == cl2)\n",
        "        X_bin = X[mask]\n",
        "        y_bin = y[mask]\n",
        "        y_bin = (y_bin == cl1).astype(int) * 2 - 1\n",
        "        model = PSO_SVM(C=C, svm_type=svm_type)\n",
        "        model.fit(X_bin, y_bin)\n",
        "        w = model.w_best\n",
        "        b = model.b_best\n",
        "        models.append(((cl1, cl2), (w, b)))\n",
        "    return models\n",
        "\n",
        "def predict_ovo(X, models):\n",
        "    votes = np.zeros((X.shape[0], len(models)))\n",
        "    classes = np.unique(np.hstack([pair for pair, _ in models]))\n",
        "    for i, ((cl1, cl2), (w, b)) in enumerate(models):\n",
        "        preds = np.sign(X.dot(w) + b)\n",
        "        votes[:, i] = np.where(preds == 1, cl1, cl2)\n",
        "    final_predictions = []\n",
        "    for j in range(votes.shape[0]):\n",
        "        valid_votes = votes[j][votes[j] >= 0]\n",
        "        if len(valid_votes) > 0:\n",
        "            bincount = np.bincount(valid_votes.astype(int))\n",
        "            final_predictions.append(bincount.argmax())\n",
        "        else:\n",
        "            final_predictions.append(np.random.choice(classes))\n",
        "    return np.array(final_predictions)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gdfPzADCtUz8"
      },
      "outputs": [],
      "source": [
        "def train_ovo(X, y, C, svm_type):\n",
        "    classes = np.unique(y)\n",
        "    models = []\n",
        "    class_pairs = list(combinations(classes, 2))\n",
        "    for (cl1, cl2) in class_pairs:\n",
        "        mask = (y == cl1) | (y == cl2)\n",
        "        X_bin = X[mask]\n",
        "        y_bin = y[mask]\n",
        "        y_bin = (y_bin == cl1).astype(int) * 2 - 1\n",
        "        model = PSO_SVM(C=C, svm_type=svm_type)\n",
        "        model.fit(X_bin, y_bin)\n",
        "        w = model.w_best\n",
        "        b = model.b_best\n",
        "        models.append(((cl1, cl2), (w, b)))\n",
        "    return models\n",
        "\n",
        "def predict_ovo(X, models):\n",
        "    votes = np.zeros((X.shape[0], len(models)))\n",
        "    classes = np.unique(np.hstack([pair for pair, _ in models]))\n",
        "    for i, ((cl1, cl2), (w, b)) in enumerate(models):\n",
        "        preds = np.sign(X.dot(w) + b)\n",
        "        votes[:, i] = np.where(preds == 1, cl1, cl2)\n",
        "    final_predictions = []\n",
        "    for j in range(votes.shape[0]):\n",
        "        valid_votes = votes[j][votes[j] >= 0]\n",
        "        if len(valid_votes) > 0:\n",
        "            bincount = np.bincount(valid_votes.astype(int))\n",
        "            final_predictions.append(bincount.argmax())\n",
        "        else:\n",
        "            final_predictions.append(np.random.choice(classes))\n",
        "    return np.array(final_predictions)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-d6Hn0gRtSOB"
      },
      "source": [
        "###ACO (Ant Colony Optimization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1N4-N5sztVhc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from itertools import combinations\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "class ACO_SVM:\n",
        "    def __init__(self, C=1.0, num_ants=30, max_iter=100, decay=0.5, alpha=1, beta=2, svm_type='L2', c=1.0):\n",
        "        self.C = C  # 정규화 상수\n",
        "        self.num_ants = num_ants  # 개미 개수\n",
        "        self.max_iter = max_iter  # 반복 횟수\n",
        "        self.decay = decay  # 페로몬 증발 계수\n",
        "        self.alpha = alpha  # 페로몬 영향도\n",
        "        self.beta = beta  # 휴리스틱 정보 영향도\n",
        "        self.svm_type = svm_type  # L1 또는 L2\n",
        "        self.w_best = None\n",
        "        self.b_best = None\n",
        "        self.c = c\n",
        "\n",
        "    def compute_slack_term(self, xi):\n",
        "        \"\"\" SVM 타입별 슬랙 변수 비용 계산 \"\"\"\n",
        "        if self.svm_type == 'L1':\n",
        "            return np.abs(xi)\n",
        "        elif self.svm_type == 'L2':\n",
        "            return xi ** 2\n",
        "        elif self.svm_type == 'Fair':\n",
        "            return np.sum(self.c**2 * ((xi / self.c) - np.log(1 + xi / self.c)))\n",
        "        elif self.svm_type == 'Cauchy':\n",
        "            return np.sum((self.c**2 / 2) * np.log(1 + (xi / self.c)**2))\n",
        "        elif self.svm_type == 'Welsch':\n",
        "            return np.sum((self.c**2 / 2) * (1 - np.exp(-(xi / self.c)**2)))\n",
        "        elif self.svm_type == 'Geman-McClure':\n",
        "            return np.sum(((xi**2)/2) / (1 + xi**2))\n",
        "        else:\n",
        "            raise ValueError(\"Unknown SVM type. Choose from ['L1', 'L2', 'Cauchy', 'Fair', 'Welsch', 'Geman-McClure'].\")\n",
        "\n",
        "    def fitness(self, w, b, X, y):\n",
        "        \"\"\" 손실 함수 계산 (Hinge Loss 기반) \"\"\"\n",
        "        margins = y * (np.dot(X, w) + b)\n",
        "        slack = np.maximum(0, 1 - margins)  # Hinge Loss 적용\n",
        "        slack_term = self.compute_slack_term(slack)\n",
        "        regularization = 0.5 * np.dot(w, w)  # L2 Regularization\n",
        "        return regularization + self.C * np.sum(slack_term)  # 최소화할 손실 값\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # 개미들의 초기 해 (랜덤 가중치 및 바이어스)\n",
        "        w_ants = np.random.randn(self.num_ants, n_features)\n",
        "        b_ants = np.random.randn(self.num_ants)\n",
        "        pheromones = np.ones(self.num_ants)  # 초기 페로몬 값 동일\n",
        "\n",
        "        best_fitness = float('inf')\n",
        "        g_best_w, g_best_b = None, None\n",
        "\n",
        "        for _ in range(self.max_iter):\n",
        "            fitness_values = np.array([self.fitness(w, b, X, y) for w, b in zip(w_ants, b_ants)])\n",
        "\n",
        "            # 가장 좋은 개미 선택\n",
        "            best_index = np.argmin(fitness_values)\n",
        "            if fitness_values[best_index] < best_fitness:\n",
        "                best_fitness = fitness_values[best_index]\n",
        "                g_best_w, g_best_b = w_ants[best_index], b_ants[best_index]\n",
        "\n",
        "            # 페로몬 업데이트 (좋은 해에 페로몬 증가)\n",
        "            pheromones = (1 - self.decay) * pheromones  # 증발 적용\n",
        "            pheromones[best_index] += 1 / (1 + best_fitness)  # 좋은 해 강화\n",
        "\n",
        "            # 개미들의 새로운 탐색 방향 선택\n",
        "            probabilities = (pheromones ** self.alpha) * ((1 / (1 + fitness_values)) ** self.beta)\n",
        "            probabilities /= np.sum(probabilities)\n",
        "\n",
        "            selected_indices = np.random.choice(self.num_ants, size=self.num_ants, p=probabilities)\n",
        "            w_ants = w_ants[selected_indices] + np.random.randn(self.num_ants, n_features) * 0.1\n",
        "            b_ants = b_ants[selected_indices] + np.random.randn(self.num_ants) * 0.1\n",
        "\n",
        "        self.w_best = g_best_w\n",
        "        self.b_best = g_best_b\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.sign(np.dot(X, self.w_best) + self.b_best)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Ax8GcdPtVkk"
      },
      "outputs": [],
      "source": [
        "def train_ovo(X, y, C, svm_type):\n",
        "    classes = np.unique(y)\n",
        "    models = []\n",
        "    class_pairs = list(combinations(classes, 2))\n",
        "    for (cl1, cl2) in class_pairs:\n",
        "        mask = (y == cl1) | (y == cl2)\n",
        "        X_bin = X[mask]\n",
        "        y_bin = y[mask]\n",
        "        y_bin = (y_bin == cl1).astype(int) * 2 - 1\n",
        "        model = ACO_SVM(C=C, svm_type=svm_type)\n",
        "        model.fit(X_bin, y_bin)\n",
        "        w = model.w_best\n",
        "        b = model.b_best\n",
        "        models.append(((cl1, cl2), (w, b)))\n",
        "    return models\n",
        "\n",
        "def predict_ovo(X, models):\n",
        "    votes = np.zeros((X.shape[0], len(models)))\n",
        "    classes = np.unique(np.hstack([pair for pair, _ in models]))\n",
        "    for i, ((cl1, cl2), (w, b)) in enumerate(models):\n",
        "        preds = np.sign(X.dot(w) + b)\n",
        "        votes[:, i] = np.where(preds == 1, cl1, cl2)\n",
        "    final_predictions = []\n",
        "    for j in range(votes.shape[0]):\n",
        "        valid_votes = votes[j][votes[j] >= 0]\n",
        "        if len(valid_votes) > 0:\n",
        "            bincount = np.bincount(valid_votes.astype(int))\n",
        "            final_predictions.append(bincount.argmax())\n",
        "        else:\n",
        "            final_predictions.append(np.random.choice(classes))\n",
        "    return np.array(final_predictions)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXCVw0KOtSSg"
      },
      "source": [
        "###HS (Harmony Search)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5FD1H62tWe7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from itertools import combinations\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "class HS_SVM:\n",
        "    def __init__(self, C=1.0, hm_size=20, max_iter=100, hmcr=0.9, par=0.3, bw=0.1, svm_type='L2', c=1.0):\n",
        "        self.C = C  # 정규화 상수\n",
        "        self.hm_size = hm_size  # 하모니 메모리 크기\n",
        "        self.max_iter = max_iter  # 반복 횟수\n",
        "        self.hmcr = hmcr  # 하모니 메모리 고려율 (기존 해를 선택할 확률)\n",
        "        self.par = par  # 피치 조정 비율 (기존 해를 변형할 확률)\n",
        "        self.bw = bw  # 변형 크기\n",
        "        self.svm_type = svm_type  # 'L1' 또는 'L2'\n",
        "        self.w_best = None\n",
        "        self.b_best = None\n",
        "        self.c = c\n",
        "\n",
        "    def compute_slack_term(self, xi):\n",
        "        \"\"\" SVM 타입별 슬랙 변수 비용 계산 \"\"\"\n",
        "        if self.svm_type == 'L1':\n",
        "            return np.abs(xi)\n",
        "        elif self.svm_type == 'L2':\n",
        "            return xi ** 2\n",
        "        elif self.svm_type == 'Fair':\n",
        "            return np.sum(self.c**2 * ((xi / self.c) - np.log(1 + xi / self.c)))\n",
        "        elif self.svm_type == 'Cauchy':\n",
        "            return np.sum((self.c**2 / 2) * np.log(1 + (xi / self.c)**2))\n",
        "        elif self.svm_type == 'Welsch':\n",
        "            return np.sum((self.c**2 / 2) * (1 - np.exp(-(xi / self.c)**2)))\n",
        "        elif self.svm_type == 'Geman-McClure':\n",
        "            return np.sum(((xi**2)/2) / (1 + xi**2))\n",
        "        else:\n",
        "            raise ValueError(\"Unknown SVM type. Choose from ['L1', 'L2', 'Cauchy', 'Fair', 'Welsch', 'Geman-McClure'].\")\n",
        "\n",
        "    def fitness(self, w, b, X, y):\n",
        "        \"\"\" 손실 함수 계산 (Hinge Loss 기반) \"\"\"\n",
        "        margins = y * (np.dot(X, w) + b)\n",
        "        slack = np.maximum(0, 1 - margins)  # Hinge Loss 적용\n",
        "        slack_term = self.compute_slack_term(slack)\n",
        "\n",
        "        regularization = 0.5 * np.dot(w, w)  # L2 Regularization\n",
        "        return regularization + self.C * np.sum(slack_term)  # 최소화할 손실 값\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # 초기 하모니 메모리 (랜덤 가중치 및 바이어스)\n",
        "        harmony_memory = [(np.random.randn(n_features), np.random.randn()) for _ in range(self.hm_size)]\n",
        "        fitness_values = np.array([self.fitness(w, b, X, y) for w, b in harmony_memory])\n",
        "\n",
        "        for _ in range(self.max_iter):\n",
        "            # 새로운 해 생성\n",
        "            new_w, new_b = np.zeros(n_features), 0\n",
        "\n",
        "            for j in range(n_features):\n",
        "                if np.random.rand() < self.hmcr:  # 기존 해에서 선택할 확률\n",
        "                    new_w[j] = harmony_memory[np.random.randint(self.hm_size)][0][j]\n",
        "                    if np.random.rand() < self.par:  # 피치 조정\n",
        "                        new_w[j] += np.random.uniform(-self.bw, self.bw)\n",
        "                else:  # 랜덤 탐색\n",
        "                    new_w[j] = np.random.randn()\n",
        "\n",
        "            if np.random.rand() < self.hmcr:\n",
        "                new_b = harmony_memory[np.random.randint(self.hm_size)][1]\n",
        "                if np.random.rand() < self.par:\n",
        "                    new_b += np.random.uniform(-self.bw, self.bw)\n",
        "            else:\n",
        "                new_b = np.random.randn()\n",
        "\n",
        "            # 새로운 해의 적합도 평가\n",
        "            new_fitness = self.fitness(new_w, new_b, X, y)\n",
        "\n",
        "            # 기존 해 중 최악의 해와 비교 후 교체\n",
        "            worst_idx = np.argmax(fitness_values)\n",
        "            if new_fitness < fitness_values[worst_idx]:\n",
        "                harmony_memory[worst_idx] = (new_w, new_b)\n",
        "                fitness_values[worst_idx] = new_fitness\n",
        "\n",
        "        # 최적 해 선택\n",
        "        best_idx = np.argmin(fitness_values)\n",
        "        self.w_best, self.b_best = harmony_memory[best_idx]\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.sign(np.dot(X, self.w_best) + self.b_best)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I3aET8k4tWh7"
      },
      "outputs": [],
      "source": [
        "def train_ovo(X, y, C, svm_type):\n",
        "    classes = np.unique(y)\n",
        "    models = []\n",
        "    class_pairs = list(combinations(classes, 2))\n",
        "    for (cl1, cl2) in class_pairs:\n",
        "        mask = (y == cl1) | (y == cl2)\n",
        "        X_bin = X[mask]\n",
        "        y_bin = y[mask]\n",
        "        y_bin = (y_bin == cl1).astype(int) * 2 - 1\n",
        "        model = HS_SVM(C=C, hm_size=20, max_iter=100, hmcr=0.9, par=0.3, bw=0.1, svm_type=svm_type)\n",
        "        model.fit(X_bin, y_bin)\n",
        "        w = model.w_best\n",
        "        b = model.b_best\n",
        "        models.append(((cl1, cl2), (w, b)))\n",
        "    return models\n",
        "\n",
        "def predict_ovo(X, models):\n",
        "    votes = np.zeros((X.shape[0], len(models)))\n",
        "    classes = np.unique(np.hstack([pair for pair, _ in models]))\n",
        "    for i, ((cl1, cl2), (w, b)) in enumerate(models):\n",
        "        preds = np.sign(X.dot(w) + b)\n",
        "        votes[:, i] = np.where(preds == 1, cl1, cl2)\n",
        "    final_predictions = []\n",
        "    for j in range(votes.shape[0]):\n",
        "        valid_votes = votes[j][votes[j] >= 0]\n",
        "        if len(valid_votes) > 0:\n",
        "            bincount = np.bincount(valid_votes.astype(int))\n",
        "            final_predictions.append(bincount.argmax())\n",
        "        else:\n",
        "            final_predictions.append(np.random.choice(classes))\n",
        "    return np.array(final_predictions)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-VWWHbTv9hM"
      },
      "source": [
        "#**Optimization & OvR**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqdxKtiowkJK"
      },
      "source": [
        "###L-BFGS-B (Limited-memory Broyden-Fletcher-Goldfarb-Shanno with Box constraints)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BKoEJzniwkJK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "class LBFGS_SVM:\n",
        "    def __init__(self, C=0.01, svm_type='L2', c=1.0, gamma=0.1):\n",
        "        self.C = C\n",
        "        self.svm_type = svm_type\n",
        "        self.c = c\n",
        "        self.gamma = gamma\n",
        "        self.alpha = None\n",
        "        self.b_best = None\n",
        "        self.X_train = None\n",
        "        self.y_train = None\n",
        "\n",
        "    def rbf_kernel(self, X1, X2):\n",
        "        if X1.ndim == 1:\n",
        "            X1 = X1.reshape(1, -1)\n",
        "        if X2.ndim == 1:\n",
        "            X2 = X2.reshape(1, -1)\n",
        "        dists = np.sum((X1[:, np.newaxis] - X2[np.newaxis, :]) ** 2, axis=2)\n",
        "        return np.exp(-self.gamma * dists)\n",
        "\n",
        "    def compute_slack_term(self, xi):\n",
        "        if self.svm_type == 'L1':\n",
        "            return np.abs(xi)\n",
        "        elif self.svm_type == 'L2':\n",
        "            return xi ** 2\n",
        "        elif self.svm_type == 'Fair':\n",
        "            return np.sum(self.c**2 * ((xi / self.c) - np.log(1 + xi / self.c)))\n",
        "        elif self.svm_type == 'Cauchy':\n",
        "            return np.sum((self.c**2 / 2) * np.log(1 + (xi / self.c)**2))\n",
        "        elif self.svm_type == 'Welsch':\n",
        "            return np.sum((self.c**2 / 2) * (1 - np.exp(-(xi / self.c)**2)))\n",
        "        elif self.svm_type == 'Geman-McClure':\n",
        "            return np.sum(((xi**2)/2) / (1 + xi**2))\n",
        "        else:\n",
        "            raise ValueError(\"Unknown SVM type.\")\n",
        "\n",
        "    def fitness(self, alpha, K, y):\n",
        "        return 0.5 * np.sum(alpha[:, None] * alpha[None, :] * y[:, None] * y[None, :] * K) - np.sum(alpha)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.X_train = X\n",
        "        self.y_train = y\n",
        "        n_samples = X.shape[0]\n",
        "        K = self.rbf_kernel(X, X)\n",
        "\n",
        "        initial_alpha = np.zeros(n_samples)\n",
        "        bounds = [(0, self.C) for _ in range(n_samples)]\n",
        "        constraints = {'type': 'eq', 'fun': lambda alpha: np.dot(alpha, y)}\n",
        "\n",
        "        result = minimize(self.fitness, initial_alpha, args=(K, y), bounds=bounds, constraints=constraints)\n",
        "\n",
        "        if not result.success:\n",
        "            raise ValueError(\"Dual optimization failed: \" + result.message)\n",
        "\n",
        "        self.alpha = result.x\n",
        "        sv = self.alpha > 1e-5\n",
        "        self.alpha_sv = self.alpha[sv]\n",
        "        self.support_vectors_ = X[sv]\n",
        "        self.support_vector_labels_ = y[sv]\n",
        "        K_sv = self.rbf_kernel(self.support_vectors_, self.support_vectors_)\n",
        "        self.b_best = np.mean(\n",
        "            self.support_vector_labels_ - np.sum(self.alpha_sv * self.support_vector_labels_ * K_sv, axis=1)\n",
        "        )\n",
        "\n",
        "        # 슬랙 비용 계산 (분석용)\n",
        "        margins = y * self.decision_function(X)\n",
        "        slack = np.maximum(0, 1 - margins)\n",
        "        self.slack_cost = self.compute_slack_term(slack)\n",
        "\n",
        "    def decision_function(self, X):\n",
        "        K = self.rbf_kernel(X, self.support_vectors_)\n",
        "        return np.sum(self.alpha_sv * self.support_vector_labels_ * K, axis=1) + self.b_best\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.sign(self.decision_function(X))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dboqMjDhwkJL"
      },
      "outputs": [],
      "source": [
        "# OvR training function using LBFGS_SVM (RBF 커널 기반)\n",
        "def train_ovr(X, y, C, svm_type, gamma=0.1):\n",
        "    classes = np.unique(y)\n",
        "    models = []\n",
        "\n",
        "    for cl in classes:\n",
        "        y_bin = (y == cl).astype(int) * 2 - 1\n",
        "        model = LBFGS_SVM(C=C, svm_type=svm_type, gamma=gamma)\n",
        "        model.fit(X, y_bin)\n",
        "        models.append(model)  # 모델 객체 자체 저장\n",
        "\n",
        "    return models\n",
        "\n",
        "def predict_ovr(X, models):\n",
        "    # 각 모델의 decision_function 값을 모아서 가장 큰 값을 가진 클래스 선택\n",
        "    predictions = np.array([model.decision_function(X) for model in models])\n",
        "    return np.argmax(predictions, axis=0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYVLte-IwkJL"
      },
      "source": [
        "###1. Genetic Algorithm (GA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYBf_swpwkJL"
      },
      "outputs": [],
      "source": [
        "class GA_SVM:\n",
        "    def __init__(self, C=1.0, pop_size=20, max_iter=100, mutation_rate=0.1, crossover_rate=0.7, svm_type='L2', c=1.0):\n",
        "        self.C = C  # 정규화 상수\n",
        "        self.pop_size = pop_size  # 개체 수\n",
        "        self.max_iter = max_iter  # 세대 수\n",
        "        self.mutation_rate = mutation_rate  # 돌연변이 확률\n",
        "        self.crossover_rate = crossover_rate  # 교차 확률\n",
        "        self.svm_type = svm_type  # 'L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure'\n",
        "        self.w_best = None\n",
        "        self.b_best = None\n",
        "        self.c = c\n",
        "\n",
        "    def compute_slack_term(self, xi):\n",
        "        if self.svm_type == 'L1':\n",
        "            return np.abs(xi)\n",
        "        elif self.svm_type == 'L2':\n",
        "            return xi ** 2\n",
        "        elif self.svm_type == 'Fair':\n",
        "            return np.sum(self.c**2 * ((xi / self.c) - np.log(1 + xi / self.c)))\n",
        "        elif self.svm_type == 'Cauchy':\n",
        "            return np.sum((self.c**2 / 2) * np.log(1 + (xi / self.c)**2))\n",
        "        elif self.svm_type == 'Welsch':\n",
        "            return np.sum((self.c**2 / 2) * (1 - np.exp(-(xi / self.c)**2)))\n",
        "        elif self.svm_type == 'Geman-McClure':\n",
        "            return np.sum(((xi**2)/2) / (1 + xi**2))\n",
        "        else:\n",
        "            raise ValueError(\"Unknown SVM type. Choose from ['L1', 'L2', 'Cauchy', 'Fair', 'Welsch', 'Geman-McClure'].\")\n",
        "\n",
        "    def fitness(self, w, b, X, y):\n",
        "        margins = y * (np.dot(X, w) + b)\n",
        "        slack = np.maximum(0, 1 - margins)\n",
        "        slack_term = self.compute_slack_term(slack)\n",
        "        regularization = 0.5 * np.dot(w, w)\n",
        "        return regularization + self.C * np.sum(slack_term)\n",
        "\n",
        "    def initialize_population(self, n_features):\n",
        "        population = [(np.random.randn(n_features), np.random.randn()) for _ in range(self.pop_size)]\n",
        "        return population\n",
        "\n",
        "    def selection(self, population, fitness_values):\n",
        "        probabilities = 1 / (fitness_values + 1e-6)\n",
        "        probabilities /= probabilities.sum()\n",
        "        selected_indices = np.random.choice(len(population), size=len(population), p=probabilities)\n",
        "        return [population[i] for i in selected_indices]\n",
        "\n",
        "    def crossover(self, parent1, parent2):\n",
        "        w1, b1 = parent1\n",
        "        w2, b2 = parent2\n",
        "        if np.random.rand() < self.crossover_rate:\n",
        "            point = np.random.randint(len(w1))\n",
        "            new_w = np.concatenate((w1[:point], w2[point:]))\n",
        "            new_b = (b1 + b2) / 2\n",
        "        else:\n",
        "            new_w, new_b = w1.copy(), b1\n",
        "        return new_w, new_b\n",
        "\n",
        "    def mutation(self, w, b):\n",
        "        if np.random.rand() < self.mutation_rate:\n",
        "            mutation_vector = np.random.randn(*w.shape) * 0.1\n",
        "            w += mutation_vector\n",
        "            b += np.random.randn() * 0.1\n",
        "        return w, b\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        population = self.initialize_population(n_features)\n",
        "        fitness_values = np.array([self.fitness(w, b, X, y) for w, b in population])\n",
        "        np.random.seed(42)  # 시드 설정\n",
        "        for _ in range(self.max_iter):\n",
        "            selected_population = self.selection(population, fitness_values)\n",
        "            new_population = []\n",
        "            for i in range(0, len(selected_population), 2):\n",
        "                p1, p2 = selected_population[i], selected_population[(i + 1) % len(selected_population)]\n",
        "                offspring1 = self.crossover(p1, p2)\n",
        "                offspring2 = self.crossover(p2, p1)\n",
        "                new_population.append(self.mutation(*offspring1))\n",
        "                new_population.append(self.mutation(*offspring2))\n",
        "            population = new_population\n",
        "            fitness_values = np.array([self.fitness(w, b, X, y) for w, b in population])\n",
        "        best_idx = np.argmin(fitness_values)\n",
        "        self.w_best, self.b_best = population[best_idx]\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.sign(np.dot(X, self.w_best) + self.b_best)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4xDnM5d2wkJL"
      },
      "outputs": [],
      "source": [
        "# OvR training function\n",
        "def train_ovr(X, y, C, svm_type):\n",
        "    classes = np.unique(y)  # 모든 클래스 목록을 가져옴\n",
        "    models = []  # 학습된 모델을 저장할 리스트\n",
        "\n",
        "    for cl in classes:\n",
        "        y_bin = (y == cl).astype(int) * 2 - 1  # 현재 클래스는 +1, 나머지는 -1로 변환\n",
        "        model = GA_SVM(C=C, svm_type=svm_type)  # SVM 모델 생성\n",
        "        model.fit(X, y_bin)  # 모델 학습\n",
        "\n",
        "        w = model.w_best  # 최적화된 가중치 벡터\n",
        "        b = model.b_best  # 최적화된 편향\n",
        "\n",
        "        models.append((w, b))  # 학습된 모델을 리스트에 저장\n",
        "\n",
        "    return models  # 모든 클래스별 학습된 모델 반환\n",
        "\n",
        "def predict_ovr(X, models):\n",
        "    predictions = [X.dot(w) + b for w, b in models]  # 각 클래스별 마진 계산\n",
        "    return np.argmax(predictions, axis=0)  # 가장 높은 마진 값을 가진 클래스를 예측\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dvc7x7owkJL"
      },
      "source": [
        "###SMO (Sequential Minial Optimization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v909xZA5wkJL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "from itertools import combinations\n",
        "\n",
        "class SMO_SVM:\n",
        "    def __init__(self, C=1.0, kernel='linear', tol=1e-3, max_iter=1000, svm_type='L2', c=1.0, lr=0.01):\n",
        "        self.C = C\n",
        "        self.kernel = kernel\n",
        "        self.tol = tol\n",
        "        self.max_iter = max_iter\n",
        "        self.svm_type = svm_type\n",
        "        self.c = c\n",
        "        self.lr = lr\n",
        "        self.w_best = None  # 최적 가중치 벡터 추가\n",
        "        self.b_best = None  # 최적 편향 값 추가\n",
        "\n",
        "    def kernel_function(self, X, Y):\n",
        "        if self.kernel == 'linear':\n",
        "            return np.dot(X, Y.T)\n",
        "        else:\n",
        "            raise ValueError(\"Only linear kernel is supported in this implementation.\")\n",
        "\n",
        "    def compute_slack_term(self, xi):\n",
        "        xi = np.maximum(xi, 1e-6)\n",
        "\n",
        "        if self.svm_type == 'Fair':\n",
        "            return np.sum(self.c**2 * ((xi / self.c) - np.log(1 + xi / self.c)))\n",
        "        elif self.svm_type == 'Cauchy':\n",
        "            return np.sum((self.c**2 / 2) * np.log(1 + (xi / self.c)**2))\n",
        "        elif self.svm_type == 'Welsch':\n",
        "            return np.sum((self.c**2 / 2) * (1 - np.exp(-(xi / self.c)**2)))\n",
        "        elif self.svm_type == 'Geman-McClure':\n",
        "            return np.sum(((xi**2)/2) / (1 + xi**2))\n",
        "        elif self.svm_type == 'L1':\n",
        "            return np.abs(xi)\n",
        "        elif self.svm_type == 'L2':\n",
        "            return xi ** 2\n",
        "        else:\n",
        "            raise ValueError(\"Unknown SVM type. Choose from ['L1', 'L2', 'Cauchy', 'Fair', 'Welsch', 'Geman-McClure'].\")\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        self.alpha = np.zeros(n_samples)\n",
        "        self.b = 0\n",
        "        self.w = np.zeros(n_features)\n",
        "\n",
        "        for _ in range(self.max_iter):\n",
        "            for i in range(n_samples):\n",
        "                xi, yi = X[i], y[i]\n",
        "                margin = yi * (np.dot(self.w, xi) + self.b)\n",
        "                slack = max(0, 1 - margin)\n",
        "                slack_term = self.compute_slack_term(slack)\n",
        "\n",
        "                if slack > 0:\n",
        "                    delta_alpha = self.C * (1 - margin) - slack_term\n",
        "                    self.alpha[i] = np.clip(self.alpha[i] + self.lr * delta_alpha, 0, self.C)\n",
        "                    self.w += self.lr * self.alpha[i] * yi * xi\n",
        "                    self.b += self.lr * self.alpha[i] * yi\n",
        "\n",
        "        self.w_best = self.w  # 최적의 가중치 저장\n",
        "        self.b_best = self.b  # 최적의 편향 값 저장\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.sign(np.dot(X, self.w_best) + self.b_best)  # b 대신 b_best 사용\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IncXMFZcwkJL"
      },
      "outputs": [],
      "source": [
        "# OvR training function using LBFGS_SVM\n",
        "def train_ovr(X, y, C, svm_type):\n",
        "    classes = np.unique(y)  # 모든 클래스 목록을 가져옴\n",
        "    models = []  # 학습된 모델을 저장할 리스트\n",
        "\n",
        "    for cl in classes:\n",
        "        y_bin = (y == cl).astype(int) * 2 - 1  # 현재 클래스는 +1, 나머지는 -1로 변환\n",
        "        model = SMO_SVM(C=C, svm_type=svm_type)  # SVM 모델 생성\n",
        "        model.fit(X, y_bin)  # 모델 학습\n",
        "\n",
        "        w = model.w_best  # 최적화된 가중치 벡터\n",
        "        b = model.b_best  # 최적화된 편향\n",
        "\n",
        "        models.append((w, b))  # 학습된 모델을 리스트에 저장\n",
        "\n",
        "    return models  # 모든 클래스별 학습된 모델 반환\n",
        "\n",
        "def predict_ovr(X, models):\n",
        "    predictions = [X.dot(w) + b for w, b in models]  # 각 클래스별 마진 계산\n",
        "    return np.argmax(predictions, axis=0)  # 가장 높은 마진 값을 가진 클래스를 예측\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70WCb2PFwkJL"
      },
      "source": [
        "###PSO (Particle Swarm Optimization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qn52YMR8wkJL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from itertools import combination\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "class PSO_SVM:\n",
        "    def __init__(self, C=1.0, num_particles=30, max_iter=100, svm_type='L2', c=1.0):\n",
        "        self.C = C  # 정규화 상수\n",
        "        self.num_particles = num_particles  # PSO 입자 개수\n",
        "        self.max_iter = max_iter  # PSO 반복 횟수\n",
        "        self.svm_type = svm_type  # L1 또는 L2\n",
        "        self.w_best = None\n",
        "        self.b_best = None\n",
        "        self.c = c\n",
        "\n",
        "    def compute_slack_term(self, xi):\n",
        "        \"\"\" SVM 타입별 슬랙 변수 비용 계산 \"\"\"\n",
        "        if self.svm_type == 'L1':\n",
        "            return np.abs(xi)\n",
        "        elif self.svm_type == 'L2':\n",
        "            return xi ** 2\n",
        "        elif self.svm_type == 'Fair':\n",
        "            return np.sum(self.c**2 * ((xi / self.c) - np.log(1 + xi / self.c)))\n",
        "        elif self.svm_type == 'Cauchy':\n",
        "            return np.sum((self.c**2 / 2) * np.log(1 + (xi / self.c)**2))\n",
        "        elif self.svm_type == 'Welsch':\n",
        "            return np.sum((self.c**2 / 2) * (1 - np.exp(-(xi / self.c)**2)))\n",
        "        elif self.svm_type == 'Geman-McClure':\n",
        "            return np.sum(((xi**2)/2) / (1 + xi**2))\n",
        "        else:\n",
        "            raise ValueError(\"Unknown SVM type. Choose from ['L1', 'L2', 'Cauchy', 'Fair', 'Welsch', 'Geman-McClure'].\")\n",
        "\n",
        "    def fitness(self, w, b, X, y):\n",
        "        \"\"\" 손실 함수 계산 (Hinge Loss 기반) \"\"\"\n",
        "        margins = y * (np.dot(X, w) + b)\n",
        "        slack = np.maximum(0, 1 - margins)  # Hinge Loss 적용\n",
        "        slack_term = self.compute_slack_term(slack)\n",
        "        regularization = 0.5 * np.dot(w, w)  # L2 Regularization\n",
        "        return regularization + self.C * np.sum(slack_term)  # 최소화할 손실 값\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # PSO 초기화\n",
        "        w_particles = np.random.randn(self.num_particles, n_features)  # 초기 w 값\n",
        "        b_particles = np.random.randn(self.num_particles)  # 초기 b 값\n",
        "        velocities_w = np.random.randn(self.num_particles, n_features) * 0.1  # 속도 초기화\n",
        "        velocities_b = np.random.randn(self.num_particles) * 0.1\n",
        "\n",
        "        # 개별 최적 및 전역 최적 초기화\n",
        "        p_best_w = np.copy(w_particles)\n",
        "        p_best_b = np.copy(b_particles)\n",
        "        p_best_scores = np.array([self.fitness(w, b, X, y) for w, b in zip(w_particles, b_particles)])\n",
        "\n",
        "        g_best_index = np.argmin(p_best_scores)\n",
        "        g_best_w = p_best_w[g_best_index]\n",
        "        g_best_b = p_best_b[g_best_index]\n",
        "\n",
        "        # PSO 학습 진행\n",
        "        w_inertia = 0.7  # 관성 계수\n",
        "        c1 = 1.5  # 개인 최적화 계수\n",
        "        c2 = 1.5  # 글로벌 최적화 계수\n",
        "\n",
        "        for _ in range(self.max_iter):\n",
        "            for i in range(self.num_particles):\n",
        "                r1, r2 = np.random.rand(), np.random.rand()\n",
        "                velocities_w[i] = (w_inertia * velocities_w[i] +\n",
        "                                   c1 * r1 * (p_best_w[i] - w_particles[i]) +\n",
        "                                   c2 * r2 * (g_best_w - w_particles[i]))\n",
        "                velocities_b[i] = (w_inertia * velocities_b[i] +\n",
        "                                   c1 * r1 * (p_best_b[i] - b_particles[i]) +\n",
        "                                   c2 * r2 * (g_best_b - b_particles[i]))\n",
        "\n",
        "                # 업데이트된 위치\n",
        "                w_particles[i] += velocities_w[i]\n",
        "                b_particles[i] += velocities_b[i]\n",
        "\n",
        "                # 새로운 피트니스 값 계산\n",
        "                new_fitness = self.fitness(w_particles[i], b_particles[i], X, y)\n",
        "\n",
        "                # 최적값 갱신\n",
        "                if new_fitness < p_best_scores[i]:\n",
        "                    p_best_w[i] = w_particles[i]\n",
        "                    p_best_b[i] = b_particles[i]\n",
        "                    p_best_scores[i] = new_fitness\n",
        "\n",
        "            # 전체 최적 갱신\n",
        "            g_best_index = np.argmin(p_best_scores)\n",
        "            g_best_w = p_best_w[g_best_index]\n",
        "            g_best_b = p_best_b[g_best_index]\n",
        "\n",
        "        self.w_best = g_best_w\n",
        "        self.b_best = g_best_b\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.sign(np.dot(X, self.w_best) + self.b_best)\n",
        "def train_ovo(X, y, C, svm_type):\n",
        "    classes = np.unique(y)\n",
        "    models = []\n",
        "    class_pairs = list(combinations(classes, 2))\n",
        "    for (cl1, cl2) in class_pairs:\n",
        "        mask = (y == cl1) | (y == cl2)\n",
        "        X_bin = X[mask]\n",
        "        y_bin = y[mask]\n",
        "        y_bin = (y_bin == cl1).astype(int) * 2 - 1\n",
        "        model = PSO_SVM(C=C, svm_type=svm_type)\n",
        "        model.fit(X_bin, y_bin)\n",
        "        w = model.w_best\n",
        "        b = model.b_best\n",
        "        models.append(((cl1, cl2), (w, b)))\n",
        "    return models\n",
        "\n",
        "def predict_ovo(X, models):\n",
        "    votes = np.zeros((X.shape[0], len(models)))\n",
        "    classes = np.unique(np.hstack([pair for pair, _ in models]))\n",
        "    for i, ((cl1, cl2), (w, b)) in enumerate(models):\n",
        "        preds = np.sign(X.dot(w) + b)\n",
        "        votes[:, i] = np.where(preds == 1, cl1, cl2)\n",
        "    final_predictions = []\n",
        "    for j in range(votes.shape[0]):\n",
        "        valid_votes = votes[j][votes[j] >= 0]\n",
        "        if len(valid_votes) > 0:\n",
        "            bincount = np.bincount(valid_votes.astype(int))\n",
        "            final_predictions.append(bincount.argmax())\n",
        "        else:\n",
        "            final_predictions.append(np.random.choice(classes))\n",
        "    return np.array(final_predictions)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDTWEliuwkJM"
      },
      "outputs": [],
      "source": [
        "# OvR training function\n",
        "def train_ovr(X, y, C, svm_type):\n",
        "    classes = np.unique(y)  # 모든 클래스 목록을 가져옴\n",
        "    models = []  # 학습된 모델을 저장할 리스트\n",
        "\n",
        "    for cl in classes:\n",
        "        y_bin = (y == cl).astype(int) * 2 - 1  # 현재 클래스는 +1, 나머지는 -1로 변환\n",
        "        model = PSO_SVM(C=C, svm_type=svm_type)  # SVM 모델 생성\n",
        "        model.fit(X, y_bin)  # 모델 학습\n",
        "\n",
        "        w = model.w_best  # 최적화된 가중치 벡터\n",
        "        b = model.b_best  # 최적화된 편향\n",
        "\n",
        "        models.append((w, b))  # 학습된 모델을 리스트에 저장\n",
        "\n",
        "    return models  # 모든 클래스별 학습된 모델 반환\n",
        "\n",
        "def predict_ovr(X, models):\n",
        "    predictions = [X.dot(w) + b for w, b in models]  # 각 클래스별 마진 계산\n",
        "    return np.argmax(predictions, axis=0)  # 가장 높은 마진 값을 가진 클래스를 예측\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KnIM7dGwkJM"
      },
      "source": [
        "###ACO (Ant Colony Optimization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zE_YlB1WwkJM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from itertools import combinations\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "class ACO_SVM:\n",
        "    def __init__(self, C=1.0, num_ants=30, max_iter=100, decay=0.5, alpha=1, beta=2, svm_type='L2', c=1.0):\n",
        "        self.C = C  # 정규화 상수\n",
        "        self.num_ants = num_ants  # 개미 개수\n",
        "        self.max_iter = max_iter  # 반복 횟수\n",
        "        self.decay = decay  # 페로몬 증발 계수\n",
        "        self.alpha = alpha  # 페로몬 영향도\n",
        "        self.beta = beta  # 휴리스틱 정보 영향도\n",
        "        self.svm_type = svm_type  # L1 또는 L2\n",
        "        self.w_best = None\n",
        "        self.b_best = None\n",
        "        self.c = c\n",
        "\n",
        "    def compute_slack_term(self, xi):\n",
        "        \"\"\" SVM 타입별 슬랙 변수 비용 계산 \"\"\"\n",
        "        if self.svm_type == 'L1':\n",
        "            return np.abs(xi)\n",
        "        elif self.svm_type == 'L2':\n",
        "            return xi ** 2\n",
        "        elif self.svm_type == 'Fair':\n",
        "            return np.sum(self.c**2 * ((xi / self.c) - np.log(1 + xi / self.c)))\n",
        "        elif self.svm_type == 'Cauchy':\n",
        "            return np.sum((self.c**2 / 2) * np.log(1 + (xi / self.c)**2))\n",
        "        elif self.svm_type == 'Welsch':\n",
        "            return np.sum((self.c**2 / 2) * (1 - np.exp(-(xi / self.c)**2)))\n",
        "        elif self.svm_type == 'Geman-McClure':\n",
        "            return np.sum(((xi**2)/2) / (1 + xi**2))\n",
        "        else:\n",
        "            raise ValueError(\"Unknown SVM type. Choose from ['L1', 'L2', 'Cauchy', 'Fair', 'Welsch', 'Geman-McClure'].\")\n",
        "\n",
        "    def fitness(self, w, b, X, y):\n",
        "        \"\"\" 손실 함수 계산 (Hinge Loss 기반) \"\"\"\n",
        "        margins = y * (np.dot(X, w) + b)\n",
        "        slack = np.maximum(0, 1 - margins)  # Hinge Loss 적용\n",
        "        slack_term = self.compute_slack_term(slack)\n",
        "        regularization = 0.5 * np.dot(w, w)  # L2 Regularization\n",
        "        return regularization + self.C * np.sum(slack_term)  # 최소화할 손실 값\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # 개미들의 초기 해 (랜덤 가중치 및 바이어스)\n",
        "        w_ants = np.random.randn(self.num_ants, n_features)\n",
        "        b_ants = np.random.randn(self.num_ants)\n",
        "        pheromones = np.ones(self.num_ants)  # 초기 페로몬 값 동일\n",
        "\n",
        "        best_fitness = float('inf')\n",
        "        g_best_w, g_best_b = None, None\n",
        "\n",
        "        for _ in range(self.max_iter):\n",
        "            fitness_values = np.array([self.fitness(w, b, X, y) for w, b in zip(w_ants, b_ants)])\n",
        "\n",
        "            # 가장 좋은 개미 선택\n",
        "            best_index = np.argmin(fitness_values)\n",
        "            if fitness_values[best_index] < best_fitness:\n",
        "                best_fitness = fitness_values[best_index]\n",
        "                g_best_w, g_best_b = w_ants[best_index], b_ants[best_index]\n",
        "\n",
        "            # 페로몬 업데이트 (좋은 해에 페로몬 증가)\n",
        "            pheromones = (1 - self.decay) * pheromones  # 증발 적용\n",
        "            pheromones[best_index] += 1 / (1 + best_fitness)  # 좋은 해 강화\n",
        "\n",
        "            # 개미들의 새로운 탐색 방향 선택\n",
        "            probabilities = (pheromones ** self.alpha) * ((1 / (1 + fitness_values)) ** self.beta)\n",
        "            probabilities /= np.sum(probabilities)\n",
        "\n",
        "            selected_indices = np.random.choice(self.num_ants, size=self.num_ants, p=probabilities)\n",
        "            w_ants = w_ants[selected_indices] + np.random.randn(self.num_ants, n_features) * 0.1\n",
        "            b_ants = b_ants[selected_indices] + np.random.randn(self.num_ants) * 0.1\n",
        "\n",
        "        self.w_best = g_best_w\n",
        "        self.b_best = g_best_b\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.sign(np.dot(X, self.w_best) + self.b_best)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KIvGptOkwkJM"
      },
      "outputs": [],
      "source": [
        "# OvR training function\n",
        "def train_ovr(X, y, C, svm_type):\n",
        "    classes = np.unique(y)  # 모든 클래스 목록을 가져옴\n",
        "    models = []  # 학습된 모델을 저장할 리스트\n",
        "\n",
        "    for cl in classes:\n",
        "        y_bin = (y == cl).astype(int) * 2 - 1  # 현재 클래스는 +1, 나머지는 -1로 변환\n",
        "        model = ACO_SVM(C=C, svm_type=svm_type)  # SVM 모델 생성\n",
        "        model.fit(X, y_bin)  # 모델 학습\n",
        "\n",
        "        w = model.w_best  # 최적화된 가중치 벡터\n",
        "        b = model.b_best  # 최적화된 편향\n",
        "\n",
        "        models.append((w, b))  # 학습된 모델을 리스트에 저장\n",
        "\n",
        "    return models  # 모든 클래스별 학습된 모델 반환\n",
        "\n",
        "def predict_ovr(X, models):\n",
        "    predictions = [X.dot(w) + b for w, b in models]  # 각 클래스별 마진 계산\n",
        "    return np.argmax(predictions, axis=0)  # 가장 높은 마진 값을 가진 클래스를 예측\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpVjPY_IwkJM"
      },
      "source": [
        "###HS (Harmony Search)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Seoc-s5RwkJM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from itertools import combinations\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "class HS_SVM:\n",
        "    def __init__(self, C=1.0, hm_size=20, max_iter=100, hmcr=0.9, par=0.3, bw=0.1, svm_type='L2', c=1.0):\n",
        "        self.C = C  # 정규화 상수\n",
        "        self.hm_size = hm_size  # 하모니 메모리 크기\n",
        "        self.max_iter = max_iter  # 반복 횟수\n",
        "        self.hmcr = hmcr  # 하모니 메모리 고려율 (기존 해를 선택할 확률)\n",
        "        self.par = par  # 피치 조정 비율 (기존 해를 변형할 확률)\n",
        "        self.bw = bw  # 변형 크기\n",
        "        self.svm_type = svm_type  # 'L1' 또는 'L2'\n",
        "        self.w_best = None\n",
        "        self.b_best = None\n",
        "        self.c = c\n",
        "\n",
        "    def compute_slack_term(self, xi):\n",
        "        \"\"\" SVM 타입별 슬랙 변수 비용 계산 \"\"\"\n",
        "        if self.svm_type == 'L1':\n",
        "            return np.abs(xi)\n",
        "        elif self.svm_type == 'L2':\n",
        "            return xi ** 2\n",
        "        elif self.svm_type == 'Fair':\n",
        "            return np.sum(self.c**2 * ((xi / self.c) - np.log(1 + xi / self.c)))\n",
        "        elif self.svm_type == 'Cauchy':\n",
        "            return np.sum((self.c**2 / 2) * np.log(1 + (xi / self.c)**2))\n",
        "        elif self.svm_type == 'Welsch':\n",
        "            return np.sum((self.c**2 / 2) * (1 - np.exp(-(xi / self.c)**2)))\n",
        "        elif self.svm_type == 'Geman-McClure':\n",
        "            return np.sum(((xi**2)/2) / (1 + xi**2))\n",
        "        else:\n",
        "            raise ValueError(\"Unknown SVM type. Choose from ['L1', 'L2', 'Cauchy', 'Fair', 'Welsch', 'Geman-McClure'].\")\n",
        "\n",
        "    def fitness(self, w, b, X, y):\n",
        "        \"\"\" 손실 함수 계산 (Hinge Loss 기반) \"\"\"\n",
        "        margins = y * (np.dot(X, w) + b)\n",
        "        slack = np.maximum(0, 1 - margins)  # Hinge Loss 적용\n",
        "        slack_term = self.compute_slack_term(slack)\n",
        "\n",
        "        regularization = 0.5 * np.dot(w, w)  # L2 Regularization\n",
        "        return regularization + self.C * np.sum(slack_term)  # 최소화할 손실 값\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # 초기 하모니 메모리 (랜덤 가중치 및 바이어스)\n",
        "        harmony_memory = [(np.random.randn(n_features), np.random.randn()) for _ in range(self.hm_size)]\n",
        "        fitness_values = np.array([self.fitness(w, b, X, y) for w, b in harmony_memory])\n",
        "\n",
        "        for _ in range(self.max_iter):\n",
        "            # 새로운 해 생성\n",
        "            new_w, new_b = np.zeros(n_features), 0\n",
        "\n",
        "            for j in range(n_features):\n",
        "                if np.random.rand() < self.hmcr:  # 기존 해에서 선택할 확률\n",
        "                    new_w[j] = harmony_memory[np.random.randint(self.hm_size)][0][j]\n",
        "                    if np.random.rand() < self.par:  # 피치 조정\n",
        "                        new_w[j] += np.random.uniform(-self.bw, self.bw)\n",
        "                else:  # 랜덤 탐색\n",
        "                    new_w[j] = np.random.randn()\n",
        "\n",
        "            if np.random.rand() < self.hmcr:\n",
        "                new_b = harmony_memory[np.random.randint(self.hm_size)][1]\n",
        "                if np.random.rand() < self.par:\n",
        "                    new_b += np.random.uniform(-self.bw, self.bw)\n",
        "            else:\n",
        "                new_b = np.random.randn()\n",
        "\n",
        "            # 새로운 해의 적합도 평가\n",
        "            new_fitness = self.fitness(new_w, new_b, X, y)\n",
        "\n",
        "            # 기존 해 중 최악의 해와 비교 후 교체\n",
        "            worst_idx = np.argmax(fitness_values)\n",
        "            if new_fitness < fitness_values[worst_idx]:\n",
        "                harmony_memory[worst_idx] = (new_w, new_b)\n",
        "                fitness_values[worst_idx] = new_fitness\n",
        "\n",
        "        # 최적 해 선택\n",
        "        best_idx = np.argmin(fitness_values)\n",
        "        self.w_best, self.b_best = harmony_memory[best_idx]\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.sign(np.dot(X, self.w_best) + self.b_best)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bFD60ucuwkJM"
      },
      "outputs": [],
      "source": [
        "# OvR training function\n",
        "def train_ovr(X, y, C, svm_type):\n",
        "    classes = np.unique(y)  # 모든 클래스 목록을 가져옴\n",
        "    models = []  # 학습된 모델을 저장할 리스트\n",
        "\n",
        "    for cl in classes:\n",
        "        y_bin = (y == cl).astype(int) * 2 - 1  # 현재 클래스는 +1, 나머지는 -1로 변환\n",
        "        model = HS_SVM(C=C, svm_type=svm_type)  # SVM 모델 생성\n",
        "        model.fit(X, y_bin)  # 모델 학습\n",
        "\n",
        "        w = model.w_best  # 최적화된 가중치 벡터\n",
        "        b = model.b_best  # 최적화된 편향\n",
        "\n",
        "        models.append((w, b))  # 학습된 모델을 리스트에 저장\n",
        "\n",
        "    return models  # 모든 클래스별 학습된 모델 반환\n",
        "\n",
        "def predict_ovr(X, models):\n",
        "    predictions = [X.dot(w) + b for w, b in models]  # 각 클래스별 마진 계산\n",
        "    return np.argmax(predictions, axis=0)  # 가장 높은 마진 값을 가진 클래스를 예측\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8r4n9wcG5AH"
      },
      "source": [
        "#**Optimization & Direct**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AbWid8vG5AI"
      },
      "source": [
        "###L-BFGS-B (Limited-memory Broyden-Fletcher-Goldfarb-Shanno with Box constraints)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r9W8szaZ542r"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.optimize import minimize\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from itertools import combinations\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "np.random.seed(42)  # 시드 설정\n",
        "\n",
        "class LBFGS_SVM:\n",
        "    def __init__(self, C=0.01, svm_type='L2', c=1.0, num_classes=3):\n",
        "        self.C = C  # 정규화 상수\n",
        "        self.svm_type = svm_type  # L1 또는 L2\n",
        "        self.num_classes = num_classes  # 다중 클래스 수\n",
        "        self.W_best = None\n",
        "        self.b_best = None\n",
        "        self.c = c\n",
        "\n",
        "    def compute_slack_term(self, xi):\n",
        "        \"\"\" SVM 타입별 슬랙 변수 비용 계산 \"\"\"\n",
        "        if self.svm_type == 'L1':\n",
        "            return np.abs(xi)\n",
        "        elif self.svm_type == 'L2':\n",
        "            return xi ** 2\n",
        "        elif self.svm_type == 'Fair':\n",
        "            return np.sum(self.c**2 * ((xi / self.c) - np.log(1 + xi / self.c)))\n",
        "        elif self.svm_type == 'Cauchy':\n",
        "            return np.sum((self.c**2 / 2) * np.log(1 + (xi / self.c)**2))\n",
        "        elif self.svm_type == 'Welsch':\n",
        "            return np.sum((self.c**2 / 2) * (1 - np.exp(-(xi / self.c)**2)))\n",
        "        elif self.svm_type == 'Geman-McClure':\n",
        "            return np.sum(((xi**2)/2) / (1 + xi**2))\n",
        "        else:\n",
        "            raise ValueError(\"Unknown SVM type. Choose from ['L1', 'L2', 'Cauchy', 'Fair', 'Welsch', 'Geman-McClure'].\")\n",
        "\n",
        "    def fitness(self, params, X, y):\n",
        "        \"\"\" 다중 클래스 SVM 손실 함수 \"\"\"\n",
        "        n_features = X.shape[1]\n",
        "        W = params[:-self.num_classes].reshape((self.num_classes, n_features))\n",
        "        b = params[-self.num_classes:]\n",
        "\n",
        "        scores = X.dot(W.T) + b\n",
        "        margins = np.maximum(0, 1 - (scores[np.arange(X.shape[0]), y] - scores.T).T)\n",
        "        margins[np.arange(X.shape[0]), y] = 0  # 정답 클래스의 손실 제거\n",
        "\n",
        "        slack_term = self.compute_slack_term(margins)\n",
        "        regularization = np.sum(W**2) / 2  # L2 Regularization\n",
        "        return regularization + self.C * np.sum(slack_term)  # 최소화할 손실 값\n",
        "\n",
        "    def fitness(self, params, X, y):\n",
        "        \"\"\" 손실 함수 계산 (Hinge Loss 기반) \"\"\"\n",
        "        n_features = X.shape[1]\n",
        "        w = params[:n_features]\n",
        "        b = params[n_features]\n",
        "        margins = y * (np.dot(X, w) + b)\n",
        "        slack = np.maximum(0, 1 - margins)  # Hinge Loss 적용\n",
        "        slack_term = self.compute_slack_term(slack)\n",
        "        regularization = 0.5 * np.dot(w, w)  # L2 Regularization\n",
        "        return regularization + self.C * np.sum(slack_term)  # 최소화할 손실 값\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        initial_params = np.zeros(n_features + 1)\n",
        "        bounds = [(None, None)] * n_features + [(None, None)]  # 경계 조건 추가\n",
        "        options = {'disp': True, 'maxiter': 5000, 'ftol': 1e-9}  # 최적화 옵션 추가: maxiter 증가, ftol 추가\n",
        "        result = minimize(self.fitness, initial_params, args=(X, y), method='L-BFGS-B', bounds=bounds, options=options)\n",
        "        # If optimization fails, try SLSQP solver\n",
        "        if not result.success:\n",
        "            result = minimize(self.fitness, initial_params, args=(X, y), method='SLSQP', bounds=bounds, options=options)\n",
        "\n",
        "        if result.success:\n",
        "            self.w_best = result.x[:n_features]\n",
        "            self.b_best = result.x[n_features]\n",
        "        else:\n",
        "            raise ValueError(\"Optimization failed: \" + result.message)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        initial_params = np.zeros((self.num_classes * n_features) + self.num_classes)\n",
        "        bounds = [(None, None)] * len(initial_params)\n",
        "        options = {'disp': True, 'maxiter': 5000, 'ftol': 1e-9}  # 최적화 옵션 추가\n",
        "        result = minimize(self.fitness, initial_params, args=(X, y), method='L-BFGS-B', bounds=bounds, options=options)\n",
        "\n",
        "        if result.success:\n",
        "            self.W_best = result.x[:-self.num_classes].reshape((self.num_classes, n_features))\n",
        "            self.b_best = result.x[-self.num_classes:]\n",
        "        else:\n",
        "            raise ValueError(\"Optimization failed: \" + result.message)\n",
        "\n",
        "    def predict(self, X):\n",
        "        scores = X.dot(self.W_best.T) + self.b_best\n",
        "        return np.argmax(scores, axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sVpl6ACIG5AJ"
      },
      "outputs": [],
      "source": [
        "# Direct SVM multiclass training function\n",
        "def train_direct(X, y, C, num_classes, svm_type):\n",
        "    model = LBFGS_SVM(C=C, svm_type=svm_type, num_classes=num_classes)\n",
        "    model.fit(X, y)\n",
        "    return model.W_best, model.b_best  # 모델의 가중치와 바이어스를 반환\n",
        "\n",
        "# Multiclass prediction function\n",
        "def predict_direct(X, weights, biases):\n",
        "    scores = X.dot(weights.T) + biases\n",
        "    return np.argmax(scores, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTyXn81TG5AK"
      },
      "source": [
        "###1. Genetic Algorithm (GA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XzMm6avNG5AK"
      },
      "outputs": [],
      "source": [
        "class GA_SVM:\n",
        "  def __init__(self, C=1.0, pop_size=20, max_iter=100, mutation_rate=0.1, crossover_rate=0.7, svm_type='L2', c=1.0, num_classes=3):\n",
        "      self.C = C  # 정규화 상수\n",
        "      self.pop_size = pop_size  # 개체 수\n",
        "      self.max_iter = max_iter  # 세대 수\n",
        "      self.mutation_rate = mutation_rate  # 돌연변이 확률\n",
        "      self.crossover_rate = crossover_rate  # 교차 확률\n",
        "      self.svm_type = svm_type  # 'L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure'\n",
        "      self.W_best = None\n",
        "      self.b_best = None\n",
        "      self.c = c\n",
        "      self.num_classes = num_classes\n",
        "\n",
        "  def compute_slack_term(self, xi):\n",
        "      if self.svm_type == 'L1':\n",
        "          return np.abs(xi)\n",
        "      elif self.svm_type == 'L2':\n",
        "          return xi ** 2\n",
        "      elif self.svm_type == 'Fair':\n",
        "          return np.sum(self.c**2 * ((xi / self.c) - np.log(1 + xi / self.c)))\n",
        "      elif self.svm_type == 'Cauchy':\n",
        "          return np.sum((self.c**2 / 2) * np.log(1 + (xi / self.c)**2))\n",
        "      elif self.svm_type == 'Welsch':\n",
        "          return np.sum((self.c**2 / 2) * (1 - np.exp(-(xi / self.c)**2)))\n",
        "      elif self.svm_type == 'Geman-McClure':\n",
        "          return np.sum(((xi**2)/2) / (1 + xi**2))\n",
        "      else:\n",
        "          raise ValueError(\"Unknown SVM type. Choose from ['L1', 'L2', 'Cauchy', 'Fair', 'Welsch', 'Geman-McClure'].\")\n",
        "\n",
        "  def fitness(self, W, b, X, y):\n",
        "      scores = X.dot(W.T) + b\n",
        "      margins = np.maximum(0, 1 - (scores[np.arange(X.shape[0]), y] - scores.T).T)\n",
        "      margins[np.arange(X.shape[0]), y] = 0  # 정답 클래스의 손실 제거\n",
        "      slack_term = self.compute_slack_term(margins)\n",
        "      regularization = np.sum(W**2) / 2  # L2 Regularization\n",
        "      return regularization + self.C * np.sum(slack_term)\n",
        "\n",
        "  def initialize_population(self, n_features):\n",
        "      population = [(np.random.randn(self.num_classes, n_features), np.random.randn(self.num_classes)) for _ in range(self.pop_size)]\n",
        "      return population\n",
        "\n",
        "  def selection(self, population, fitness_values):\n",
        "      probabilities = 1 / (fitness_values + 1e-6)\n",
        "      probabilities /= probabilities.sum()\n",
        "      selected_indices = np.random.choice(len(population), size=len(population), p=probabilities)\n",
        "      return [population[i] for i in selected_indices]\n",
        "\n",
        "  def crossover(self, parent1, parent2):\n",
        "      W1, b1 = parent1\n",
        "      W2, b2 = parent2\n",
        "      if np.random.rand() < self.crossover_rate:\n",
        "          point = np.random.randint(W1.shape[1])\n",
        "          new_W = np.hstack((W1[:, :point], W2[:, point:]))\n",
        "          new_b = (b1 + b2) / 2\n",
        "      else:\n",
        "          new_W, new_b = W1.copy(), b1\n",
        "      return new_W, new_b\n",
        "\n",
        "  def mutation(self, W, b):\n",
        "      if np.random.rand() < self.mutation_rate:\n",
        "          mutation_matrix = np.random.randn(*W.shape) * 0.1\n",
        "          W += mutation_matrix\n",
        "          b += np.random.randn(*b.shape) * 0.1\n",
        "      return W, b\n",
        "\n",
        "  def fit(self, X, y):\n",
        "      n_samples, n_features = X.shape\n",
        "      population = self.initialize_population(n_features)\n",
        "      fitness_values = np.array([self.fitness(W, b, X, y) for W, b in population])\n",
        "      np.random.seed(42)  # 시드 설정\n",
        "      for _ in range(self.max_iter):\n",
        "          selected_population = self.selection(population, fitness_values)\n",
        "          new_population = []\n",
        "          for i in range(0, len(selected_population), 2):\n",
        "              p1, p2 = selected_population[i], selected_population[(i + 1) % len(selected_population)]\n",
        "              offspring1 = self.crossover(p1, p2)\n",
        "              offspring2 = self.crossover(p2, p1)\n",
        "              new_population.append(self.mutation(*offspring1))\n",
        "              new_population.append(self.mutation(*offspring2))\n",
        "          population = new_population\n",
        "          fitness_values = np.array([self.fitness(W, b, X, y) for W, b in population])\n",
        "      best_idx = np.argmin(fitness_values)\n",
        "      self.W_best, self.b_best = population[best_idx]\n",
        "\n",
        "  def predict(self, X):\n",
        "      scores = X.dot(self.W_best.T) + self.b_best\n",
        "      return np.argmax(scores, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3VNGv9G1G5AK"
      },
      "outputs": [],
      "source": [
        "# Direct SVM multiclass training function\n",
        "def train_direct(X, y, C, num_classes, svm_type):\n",
        "    model = GA_SVM(C=C, svm_type=svm_type, num_classes=num_classes)\n",
        "    model.fit(X, y)\n",
        "    return model.W_best, model.b_best  # 모델의 가중치와 바이어스를 반환\n",
        "\n",
        "# Multiclass prediction function\n",
        "def predict_direct(X, weights, biases):\n",
        "    scores = X.dot(weights.T) + biases\n",
        "    return np.argmax(scores, axis=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stUCqlMaG5AL"
      },
      "source": [
        "###SMO (Sequential Minial Optimization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4VSsVm2tG5AL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "from itertools import combinations\n",
        "\n",
        "class SMO_SVM:\n",
        "    def __init__(self, C=1.0, kernel='linear', tol=1e-3, max_iter=1000, svm_type='L2', c=1.0, lr=0.01, num_classes=3):\n",
        "      self.C = C\n",
        "      self.kernel = kernel\n",
        "      self.tol = tol\n",
        "      self.max_iter = max_iter\n",
        "      self.svm_type = svm_type\n",
        "      self.c = c\n",
        "      self.lr = lr\n",
        "      self.num_classes = num_classes\n",
        "      self.W_best = None  # 최적 가중치 행렬 추가\n",
        "      self.b_best = None  # 최적 편향 벡터 추가\n",
        "    def kernel_function(self, X, Y):\n",
        "      if self.kernel == 'linear':\n",
        "          return np.dot(X, Y.T)\n",
        "      else:\n",
        "          raise ValueError(\"Only linear kernel is supported in this implementation.\")\n",
        "\n",
        "    def compute_slack_term(self, xi):\n",
        "        xi = np.maximum(xi, 1e-6)\n",
        "        if self.svm_type == 'Fair':\n",
        "            return np.sum(self.c**2 * ((xi / self.c) - np.log(1 + xi / self.c)))\n",
        "        elif self.svm_type == 'Cauchy':\n",
        "            return np.sum((self.c**2 / 2) * np.log(1 + (xi / self.c)**2))\n",
        "        elif self.svm_type == 'Welsch':\n",
        "            return np.sum((self.c**2 / 2) * (1 - np.exp(-(xi / self.c)**2)))\n",
        "        elif self.svm_type == 'Geman-McClure':\n",
        "            return np.sum(((xi**2)/2) / (1 + xi**2))\n",
        "        elif self.svm_type == 'L1':\n",
        "            return np.abs(xi)\n",
        "        elif self.svm_type == 'L2':\n",
        "            return xi ** 2\n",
        "        else:\n",
        "            raise ValueError(\"Unknown SVM type. Choose from ['L1', 'L2', 'Cauchy', 'Fair', 'Welsch', 'Geman-McClure'].\")\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        self.alpha = np.zeros((n_samples, self.num_classes))\n",
        "        self.b = np.zeros(self.num_classes)\n",
        "        self.W = np.zeros((self.num_classes, n_features))\n",
        "\n",
        "        for _ in range(self.max_iter):\n",
        "            for i in range(n_samples):\n",
        "                xi, yi = X[i], y[i]\n",
        "                margins = self.W.dot(xi) + self.b\n",
        "                correct_class_margin = margins[yi]\n",
        "                margin_diff = 1 - (correct_class_margin - margins)\n",
        "                margin_diff[yi] = 0  # 정답 클래스의 손실 제거\n",
        "                slack = np.maximum(0, margin_diff)\n",
        "                slack_term = self.compute_slack_term(slack)\n",
        "\n",
        "                if np.any(slack > 0):\n",
        "                    delta_alpha = self.C * (1 - margin_diff) - slack_term\n",
        "                    self.alpha[i, yi] = np.clip(self.alpha[i, yi] + self.lr * delta_alpha[yi], 0, self.C)\n",
        "                    self.W[yi] += self.lr * self.alpha[i, yi] * xi\n",
        "                    self.b[yi] += self.lr * self.alpha[i, yi]\n",
        "\n",
        "        self.W_best = self.W  # 최적의 가중치 행렬 저장\n",
        "        self.b_best = self.b  # 최적의 편향 벡터 저장\n",
        "\n",
        "    def predict(self, X):\n",
        "        scores = X.dot(self.W_best.T) + self.b_best\n",
        "        return np.argmax(scores, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LmkLwi-8G5AL"
      },
      "outputs": [],
      "source": [
        "# Direct SVM multiclass training function\n",
        "def train_direct(X, y, C, num_classes, svm_type):\n",
        "    model = SMO_SVM(C=C, svm_type=svm_type, num_classes=num_classes)\n",
        "    model.fit(X, y)\n",
        "    return model.W_best, model.b_best  # 모델의 가중치와 바이어스를 반환\n",
        "\n",
        "# Multiclass prediction function\n",
        "def predict_direct(X, weights, biases):\n",
        "    scores = X.dot(weights.T) + biases\n",
        "    return np.argmax(scores, axis=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNeDbczSG5AM"
      },
      "source": [
        "###PSO (Particle Swarm Optimization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFM9Ig5XG5AM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from itertools import combinations\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "class PSO_SVM:\n",
        "    def __init__(self, C=1.0, num_particles=30, max_iter=100, svm_type='L2', c=1.0, num_classes=3):\n",
        "        self.C = C  # 정규화 상수\n",
        "        self.num_particles = num_particles  # PSO 입자 개수\n",
        "        self.max_iter = max_iter  # PSO 반복 횟수\n",
        "        self.svm_type = svm_type  # L1 또는 L2\n",
        "        self.num_classes = num_classes\n",
        "        self.W_best = None\n",
        "        self.b_best = None\n",
        "        self.c = c\n",
        "    def compute_slack_term(self, xi):\n",
        "        if self.svm_type == 'L1':\n",
        "            return np.abs(xi)\n",
        "        elif self.svm_type == 'L2':\n",
        "            return xi ** 2\n",
        "        elif self.svm_type == 'Fair':\n",
        "            return np.sum(self.c**2 * ((xi / self.c) - np.log(1 + xi / self.c)))\n",
        "        elif self.svm_type == 'Cauchy':\n",
        "            return np.sum((self.c**2 / 2) * np.log(1 + (xi / self.c)**2))\n",
        "        elif self.svm_type == 'Welsch':\n",
        "            return np.sum((self.c**2 / 2) * (1 - np.exp(-(xi / self.c)**2)))\n",
        "        elif self.svm_type == 'Geman-McClure':\n",
        "            return np.sum(((xi**2)/2) / (1 + xi**2))\n",
        "        else:\n",
        "            raise ValueError(\"Unknown SVM type. Choose from ['L1', 'L2', 'Cauchy', 'Fair', 'Welsch', 'Geman-McClure'].\")\n",
        "\n",
        "    def fitness(self, W, b, X, y):\n",
        "        scores = X.dot(W.T) + b\n",
        "        margins = np.maximum(0, 1 - (scores[np.arange(X.shape[0]), y] - scores.T).T)\n",
        "        margins[np.arange(X.shape[0]), y] = 0\n",
        "        slack_term = self.compute_slack_term(margins)\n",
        "        regularization = np.sum(W**2) / 2\n",
        "        return regularization + self.C * np.sum(slack_term)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        W_particles = np.random.randn(self.num_particles, self.num_classes, n_features)\n",
        "        b_particles = np.random.randn(self.num_particles, self.num_classes)\n",
        "        velocities_W = np.random.randn(self.num_particles, self.num_classes, n_features) * 0.1\n",
        "        velocities_b = np.random.randn(self.num_particles, self.num_classes) * 0.1\n",
        "\n",
        "        p_best_W = np.copy(W_particles)\n",
        "        p_best_b = np.copy(b_particles)\n",
        "        p_best_scores = np.array([self.fitness(W, b, X, y) for W, b in zip(W_particles, b_particles)])\n",
        "\n",
        "        g_best_index = np.argmin(p_best_scores)\n",
        "        g_best_W = p_best_W[g_best_index]\n",
        "        g_best_b = p_best_b[g_best_index]\n",
        "\n",
        "        w_inertia = 0.7\n",
        "        c1, c2 = 1.5, 1.5\n",
        "\n",
        "        for _ in range(self.max_iter):\n",
        "            for i in range(self.num_particles):\n",
        "                r1, r2 = np.random.rand(), np.random.rand()\n",
        "                velocities_W[i] = (w_inertia * velocities_W[i] +\n",
        "                                  c1 * r1 * (p_best_W[i] - W_particles[i]) +\n",
        "                                  c2 * r2 * (g_best_W - W_particles[i]))\n",
        "                velocities_b[i] = (w_inertia * velocities_b[i] +\n",
        "                                  c1 * r1 * (p_best_b[i] - b_particles[i]) +\n",
        "                                  c2 * r2 * (g_best_b - b_particles[i]))\n",
        "\n",
        "                W_particles[i] += velocities_W[i]\n",
        "                b_particles[i] += velocities_b[i]\n",
        "                new_fitness = self.fitness(W_particles[i], b_particles[i], X, y)\n",
        "\n",
        "                if new_fitness < p_best_scores[i]:\n",
        "                    p_best_W[i] = W_particles[i]\n",
        "                    p_best_b[i] = b_particles[i]\n",
        "                    p_best_scores[i] = new_fitness\n",
        "\n",
        "            g_best_index = np.argmin(p_best_scores)\n",
        "            g_best_W = p_best_W[g_best_index]\n",
        "            g_best_b = p_best_b[g_best_index]\n",
        "\n",
        "        self.W_best = g_best_W\n",
        "        self.b_best = g_best_b\n",
        "\n",
        "    def predict(self, X):\n",
        "        scores = X.dot(self.W_best.T) + self.b_best\n",
        "        return np.argmax(scores, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uX8h5qlPG5AM"
      },
      "outputs": [],
      "source": [
        "# Direct SVM multiclass training function\n",
        "def train_direct(X, y, C, num_classes, svm_type):\n",
        "    model = PSO_SVM(C=C, svm_type=svm_type, num_classes=num_classes)\n",
        "    model.fit(X, y)\n",
        "    return model.W_best, model.b_best  # 모델의 가중치와 바이어스를 반환\n",
        "\n",
        "# Multiclass prediction function\n",
        "def predict_direct(X, weights, biases):\n",
        "    scores = X.dot(weights.T) + biases\n",
        "    return np.argmax(scores, axis=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhdUf7pDG5AN"
      },
      "source": [
        "###ACO (Ant Colony Optimization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sWaq6jD9G5AN"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from itertools import combinations\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "class ACO_SVM:\n",
        "    def __init__(self, C=1.0, num_ants=30, max_iter=100, decay=0.5, alpha=1, beta=2, svm_type='L2', c=1.0, num_classes=3):\n",
        "        self.C = C  # 정규화 상수\n",
        "        self.num_ants = num_ants  # 개미 개수\n",
        "        self.max_iter = max_iter  # 반복 횟수\n",
        "        self.decay = decay  # 페로몬 증발 계수\n",
        "        self.alpha = alpha  # 페로몬 영향도\n",
        "        self.beta = beta  # 휴리스틱 정보 영향도\n",
        "        self.svm_type = svm_type  # L1 또는 L2\n",
        "        self.num_classes = num_classes\n",
        "        self.W_best = None\n",
        "        self.b_best = None\n",
        "        self.c = c\n",
        "    def compute_slack_term(self, xi):\n",
        "        if self.svm_type == 'L1':\n",
        "            return np.abs(xi)\n",
        "        elif self.svm_type == 'L2':\n",
        "            return xi ** 2\n",
        "        elif self.svm_type == 'Fair':\n",
        "            return np.sum(self.c**2 * ((xi / self.c) - np.log(1 + xi / self.c)))\n",
        "        elif self.svm_type == 'Cauchy':\n",
        "            return np.sum((self.c**2 / 2) * np.log(1 + (xi / self.c)**2))\n",
        "        elif self.svm_type == 'Welsch':\n",
        "            return np.sum((self.c**2 / 2) * (1 - np.exp(-(xi / self.c)**2)))\n",
        "        elif self.svm_type == 'Geman-McClure':\n",
        "            return np.sum(((xi**2)/2) / (1 + xi**2))\n",
        "        else:\n",
        "            raise ValueError(\"Unknown SVM type. Choose from ['L1', 'L2', 'Cauchy', 'Fair', 'Welsch', 'Geman-McClure'].\")\n",
        "\n",
        "    def fitness(self, W, b, X, y):\n",
        "        scores = X.dot(W.T) + b\n",
        "        margins = np.maximum(0, 1 - (scores[np.arange(X.shape[0]), y] - scores.T).T)\n",
        "        margins[np.arange(X.shape[0]), y] = 0\n",
        "        slack_term = self.compute_slack_term(margins)\n",
        "        regularization = np.sum(W**2) / 2\n",
        "        return regularization + self.C * np.sum(slack_term)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        W_ants = np.random.randn(self.num_ants, self.num_classes, n_features)\n",
        "        b_ants = np.random.randn(self.num_ants, self.num_classes)\n",
        "        pheromones = np.ones(self.num_ants)\n",
        "\n",
        "        best_fitness = float('inf')\n",
        "        g_best_W, g_best_b = None, None\n",
        "\n",
        "        for _ in range(self.max_iter):\n",
        "            fitness_values = np.array([self.fitness(W, b, X, y) for W, b in zip(W_ants, b_ants)])\n",
        "            best_index = np.argmin(fitness_values)\n",
        "            if fitness_values[best_index] < best_fitness:\n",
        "                best_fitness = fitness_values[best_index]\n",
        "                g_best_W, g_best_b = W_ants[best_index], b_ants[best_index]\n",
        "\n",
        "            pheromones = (1 - self.decay) * pheromones\n",
        "            pheromones[best_index] += 1 / (1 + best_fitness)\n",
        "\n",
        "            probabilities = (pheromones ** self.alpha) * ((1 / (1 + fitness_values)) ** self.beta)\n",
        "            probabilities /= np.sum(probabilities)\n",
        "\n",
        "            selected_indices = np.random.choice(self.num_ants, size=self.num_ants, p=probabilities)\n",
        "            W_ants = W_ants[selected_indices] + np.random.randn(self.num_ants, self.num_classes, n_features) * 0.1\n",
        "            b_ants = b_ants[selected_indices] + np.random.randn(self.num_ants, self.num_classes) * 0.1\n",
        "\n",
        "        self.W_best = g_best_W\n",
        "        self.b_best = g_best_b\n",
        "\n",
        "    def predict(self, X):\n",
        "        scores = X.dot(self.W_best.T) + self.b_best\n",
        "        return np.argmax(scores, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JFZtLGCeG5AN"
      },
      "outputs": [],
      "source": [
        "# Direct SVM multiclass training function\n",
        "def train_direct(X, y, C, num_classes, svm_type):\n",
        "    model = ACO_SVM(C=C, svm_type=svm_type, num_classes=num_classes)\n",
        "    model.fit(X, y)\n",
        "    return model.W_best, model.b_best  # 모델의 가중치와 바이어스를 반환\n",
        "\n",
        "# Multiclass prediction function\n",
        "def predict_direct(X, weights, biases):\n",
        "    scores = X.dot(weights.T) + biases\n",
        "    return np.argmax(scores, axis=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvqcqw92G5AO"
      },
      "source": [
        "###HS (Harmony Search)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_9YRW6KG5AO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from itertools import combinations\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "class HS_SVM:\n",
        "    def __init__(self, C=1.0, hm_size=20, max_iter=100, hmcr=0.9, par=0.3, bw=0.1, svm_type='L2', c=1.0, num_classes=3):\n",
        "        self.C = C  # 정규화 상수\n",
        "        self.hm_size = hm_size  # 하모니 메모리 크기\n",
        "        self.max_iter = max_iter  # 반복 횟수\n",
        "        self.hmcr = hmcr  # 하모니 메모리 고려율 (기존 해를 선택할 확률)\n",
        "        self.par = par  # 피치 조정 비율 (기존 해를 변형할 확률)\n",
        "        self.bw = bw  # 변형 크기\n",
        "        self.svm_type = svm_type  # 'L1' 또는 'L2'\n",
        "        self.num_classes = num_classes\n",
        "        self.W_best = None\n",
        "        self.b_best = None\n",
        "        self.c = c\n",
        "\n",
        "    def compute_slack_term(self, xi):\n",
        "        if self.svm_type == 'L1':\n",
        "            return np.abs(xi)\n",
        "        elif self.svm_type == 'L2':\n",
        "            return xi ** 2\n",
        "        elif self.svm_type == 'Fair':\n",
        "            return np.sum(self.c**2 * ((xi / self.c) - np.log(1 + xi / self.c)))\n",
        "        elif self.svm_type == 'Cauchy':\n",
        "            return np.sum((self.c**2 / 2) * np.log(1 + (xi / self.c)**2))\n",
        "        elif self.svm_type == 'Welsch':\n",
        "            return np.sum((self.c**2 / 2) * (1 - np.exp(-(xi / self.c)**2)))\n",
        "        elif self.svm_type == 'Geman-McClure':\n",
        "            return np.sum(((xi**2)/2) / (1 + xi**2))\n",
        "        else:\n",
        "            raise ValueError(\"Unknown SVM type. Choose from ['L1', 'L2', 'Cauchy', 'Fair', 'Welsch', 'Geman-McClure'].\")\n",
        "\n",
        "    def fitness(self, W, b, X, y):\n",
        "        scores = X.dot(W.T) + b\n",
        "        margins = np.maximum(0, 1 - (scores[np.arange(X.shape[0]), y] - scores.T).T)\n",
        "        margins[np.arange(X.shape[0]), y] = 0\n",
        "        slack_term = self.compute_slack_term(margins)\n",
        "        regularization = np.sum(W**2) / 2\n",
        "        return regularization + self.C * np.sum(slack_term)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        harmony_memory = [(np.random.randn(self.num_classes, n_features), np.random.randn(self.num_classes)) for _ in range(self.hm_size)]\n",
        "        fitness_values = np.array([self.fitness(W, b, X, y) for W, b in harmony_memory])\n",
        "\n",
        "        for _ in range(self.max_iter):\n",
        "            new_W, new_b = np.zeros((self.num_classes, n_features)), np.zeros(self.num_classes)\n",
        "            for j in range(n_features):\n",
        "                if np.random.rand() < self.hmcr:\n",
        "                    new_W[:, j] = harmony_memory[np.random.randint(self.hm_size)][0][:, j]\n",
        "                    if np.random.rand() < self.par:\n",
        "                        new_W[:, j] += np.random.uniform(-self.bw, self.bw, self.num_classes)\n",
        "                else:\n",
        "                    new_W[:, j] = np.random.randn(self.num_classes)\n",
        "\n",
        "            if np.random.rand() < self.hmcr:\n",
        "                new_b = harmony_memory[np.random.randint(self.hm_size)][1]\n",
        "                if np.random.rand() < self.par:\n",
        "                    new_b += np.random.uniform(-self.bw, self.bw, self.num_classes)\n",
        "            else:\n",
        "                new_b = np.random.randn(self.num_classes)\n",
        "\n",
        "            new_fitness = self.fitness(new_W, new_b, X, y)\n",
        "            worst_idx = np.argmax(fitness_values)\n",
        "            if new_fitness < fitness_values[worst_idx]:\n",
        "                harmony_memory[worst_idx] = (new_W, new_b)\n",
        "                fitness_values[worst_idx] = new_fitness\n",
        "\n",
        "        best_idx = np.argmin(fitness_values)\n",
        "        self.W_best, self.b_best = harmony_memory[best_idx]\n",
        "\n",
        "    def predict(self, X):\n",
        "        scores = X.dot(self.W_best.T) + self.b_best\n",
        "        return np.argmax(scores, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVRY1K_IG5AO"
      },
      "outputs": [],
      "source": [
        "# Direct SVM multiclass training function\n",
        "def train_direct(X, y, C, num_classes, svm_type):\n",
        "    model = HS_SVM(C=C, svm_type=svm_type, num_classes=num_classes)\n",
        "    model.fit(X, y)\n",
        "    return model.W_best, model.b_best  # 모델의 가중치와 바이어스를 반환\n",
        "\n",
        "# Multiclass prediction function\n",
        "def predict_direct(X, weights, biases):\n",
        "    scores = X.dot(weights.T) + biases\n",
        "    return np.argmax(scores, axis=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWUPoG7VoeVW"
      },
      "source": [
        "#**OvO with various outliers**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4BXtOi4o98n"
      },
      "source": [
        "###iris data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fK51E0X_pmvP",
        "outputId": "c76db6c5-753a-415f-cb06-cac7719db36d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "기존 이상치 수: 1\n",
            "기존 이상치 비율: 0.67%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.stats import zscore\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# 데이터셋 로딩\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "df = pd.DataFrame(X, columns=data.feature_names)\n",
        "df['target'] = y  # 타겟 변수 추가\n",
        "\n",
        "# 데이터 정규화\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "df_scaled = pd.DataFrame(X_scaled, columns=data.feature_names)\n",
        "\n",
        "# 기존 이상치 식별\n",
        "df_zscores = df_scaled.apply(zscore)  # Z-score 계산\n",
        "is_existing_outlier = (df_zscores.abs() > 3).any(axis=1)  # 절대값이 3 이상인 경우 이상치로 판단\n",
        "\n",
        "# 기존 이상치 수 계산\n",
        "total_existing_outliers = is_existing_outlier.sum()\n",
        "\n",
        "# 기존 이상치 비율 계산\n",
        "total_samples = len(df)\n",
        "existing_outlier_percentage = (total_existing_outliers / total_samples) * 100\n",
        "\n",
        "print(f\"기존 이상치 수: {total_existing_outliers}\")\n",
        "print(f\"기존 이상치 비율: {existing_outlier_percentage:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_und-Y6EnrCp",
        "outputId": "84e5b7d9-eb97-4c82-80aa-17ff819d70ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "확장된 데이터셋 크기: (154, 4)\n",
            "확장된 타겟 크기: (154,)\n",
            "이상치가 포함된 데이터셋의 총 샘플 수: 154\n",
            "전체 이상치 비율: 3.25%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import load_iris\n",
        "from scipy.stats import zscore\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# 데이터셋 로딩\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "df = pd.DataFrame(X, columns=iris.feature_names)\n",
        "df['target'] = y  # 타겟 변수 추가\n",
        "\n",
        "# 데이터 정규화\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "df_scaled = pd.DataFrame(X_scaled, columns=iris.feature_names)\n",
        "\n",
        "# 이상치 생성\n",
        "num_outliers = int(0.03 * len(df))  # 이상치를 설정\n",
        "outliers = []\n",
        "\n",
        "for _ in range(num_outliers):\n",
        "    outlier_sample = df_scaled.sample(n=1).copy()  # 임의의 샘플 선택\n",
        "    for col in df_scaled.columns:\n",
        "        if np.random.rand() > 0.5:\n",
        "            # 최대값보다 큰 이상치 생성\n",
        "            outlier_sample[col] = df_scaled[col].max() + np.random.rand() * 3\n",
        "        else:\n",
        "            # 최소값보다 작은 이상치 생성\n",
        "            outlier_sample[col] = df_scaled[col].min() - np.random.rand() * 3\n",
        "    outliers.append(outlier_sample)\n",
        "\n",
        "outliers_df = pd.concat(outliers, ignore_index=True)\n",
        "\n",
        "# 이상치의 타겟 값 랜덤 할당\n",
        "outliers_df['target'] = np.random.choice(y, num_outliers)\n",
        "\n",
        "# 원본 데이터에 이상치 추가\n",
        "df_extended = pd.concat([df, outliers_df], ignore_index=True)\n",
        "\n",
        "# 이상치 추가 후 다시 정규화\n",
        "X_extended_scaled = scaler.fit_transform(df_extended.drop(columns=['target']))\n",
        "df_extended_scaled = pd.DataFrame(X_extended_scaled, columns=iris.feature_names)\n",
        "\n",
        "# 새로운 X, y 정의\n",
        "X = df_extended_scaled.values  # NumPy 배열로 변환\n",
        "y = df_extended['target'].values  # 타겟 값 추출\n",
        "\n",
        "# 기존 이상치 식별\n",
        "df_zscores = df_scaled.apply(zscore)  # Z-score 계산\n",
        "is_existing_outlier = (df_zscores.abs() > 3).any(axis=1)  # 절대값이 3 이상인 경우 이상치로 판단\n",
        "\n",
        "total_existing_outliers = is_existing_outlier.sum()  # 기존 이상치 수\n",
        "total_outliers = total_existing_outliers + num_outliers  # 총 이상치 수\n",
        "total_samples = len(df_extended)  # 전체 샘플 수\n",
        "total_outlier_percentage = (total_outliers / total_samples) * 100  # 이상치 비율 계산\n",
        "\n",
        "# 데이터 확인\n",
        "print(f\"확장된 데이터셋 크기: {X.shape}\")\n",
        "print(f\"확장된 타겟 크기: {y.shape}\")\n",
        "print(f\"이상치가 포함된 데이터셋의 총 샘플 수: {len(X)}\")\n",
        "print(f\"전체 이상치 비율: {total_outlier_percentage:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHMPkItdpNyJ",
        "outputId": "e7335b04-f6fb-4d20-830d-7955cc36d005"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "확장된 데이터셋 크기: (157, 4)\n",
            "확장된 타겟 크기: (157,)\n",
            "이상치가 포함된 데이터셋의 총 샘플 수: 157\n",
            "전체 이상치 비율: 5.10%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import load_iris\n",
        "from scipy.stats import zscore\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# 데이터셋 로딩\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "df = pd.DataFrame(X, columns=iris.feature_names)\n",
        "df['target'] = y  # 타겟 변수 추가\n",
        "\n",
        "# 데이터 정규화\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "df_scaled = pd.DataFrame(X_scaled, columns=iris.feature_names)\n",
        "\n",
        "# 이상치 생성\n",
        "num_outliers = int(0.05 * len(df))  # 이상치를 설정\n",
        "outliers = []\n",
        "\n",
        "for _ in range(num_outliers):\n",
        "    outlier_sample = df_scaled.sample(n=1).copy()  # 임의의 샘플 선택\n",
        "    for col in df_scaled.columns:\n",
        "        if np.random.rand() > 0.5:\n",
        "            # 최대값보다 큰 이상치 생성\n",
        "            outlier_sample[col] = df_scaled[col].max() + np.random.rand() * 3\n",
        "        else:\n",
        "            # 최소값보다 작은 이상치 생성\n",
        "            outlier_sample[col] = df_scaled[col].min() - np.random.rand() * 3\n",
        "    outliers.append(outlier_sample)\n",
        "\n",
        "outliers_df = pd.concat(outliers, ignore_index=True)\n",
        "\n",
        "# 이상치의 타겟 값 랜덤 할당\n",
        "outliers_df['target'] = np.random.choice(y, num_outliers)\n",
        "\n",
        "# 원본 데이터에 이상치 추가\n",
        "df_extended = pd.concat([df, outliers_df], ignore_index=True)\n",
        "\n",
        "# 이상치 추가 후 다시 정규화\n",
        "X_extended_scaled = scaler.fit_transform(df_extended.drop(columns=['target']))\n",
        "df_extended_scaled = pd.DataFrame(X_extended_scaled, columns=iris.feature_names)\n",
        "\n",
        "# 새로운 X, y 정의\n",
        "X = df_extended_scaled.values  # NumPy 배열로 변환\n",
        "y = df_extended['target'].values  # 타겟 값 추출\n",
        "\n",
        "# 기존 이상치 식별\n",
        "df_zscores = df_scaled.apply(zscore)  # Z-score 계산\n",
        "is_existing_outlier = (df_zscores.abs() > 3).any(axis=1)  # 절대값이 3 이상인 경우 이상치로 판단\n",
        "\n",
        "total_existing_outliers = is_existing_outlier.sum()  # 기존 이상치 수\n",
        "total_outliers = total_existing_outliers + num_outliers  # 총 이상치 수\n",
        "total_samples = len(df_extended)  # 전체 샘플 수\n",
        "total_outlier_percentage = (total_outliers / total_samples) * 100  # 이상치 비율 계산\n",
        "\n",
        "# 데이터 확인\n",
        "print(f\"확장된 데이터셋 크기: {X.shape}\")\n",
        "print(f\"확장된 타겟 크기: {y.shape}\")\n",
        "print(f\"이상치가 포함된 데이터셋의 총 샘플 수: {len(X)}\")\n",
        "print(f\"전체 이상치 비율: {total_outlier_percentage:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtQip-vfpN0D",
        "outputId": "4c36a51e-3cee-4505-b2db-6cfb9701f1bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "확장된 데이터셋 크기: (156, 4)\n",
            "확장된 타겟 크기: (156,)\n",
            "이상치가 포함된 데이터셋의 총 샘플 수: 156\n",
            "전체 이상치 비율: 4.49%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import load_iris\n",
        "from scipy.stats import zscore\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# 데이터셋 로딩\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "df = pd.DataFrame(X, columns=iris.feature_names)\n",
        "df['target'] = y  # 타겟 변수 추가\n",
        "\n",
        "# 데이터 정규화\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "df_scaled = pd.DataFrame(X_scaled, columns=iris.feature_names)\n",
        "\n",
        "# 이상치 생성\n",
        "num_outliers = int(0.04 * len(df))  # 이상치를 설정\n",
        "outliers = []\n",
        "\n",
        "for _ in range(num_outliers):\n",
        "    outlier_sample = df_scaled.sample(n=1).copy()  # 임의의 샘플 선택\n",
        "    for col in df_scaled.columns:\n",
        "        if np.random.rand() > 0.5:\n",
        "            # 최대값보다 큰 이상치 생성\n",
        "            outlier_sample[col] = df_scaled[col].max() + np.random.rand() * 3\n",
        "        else:\n",
        "            # 최소값보다 작은 이상치 생성\n",
        "            outlier_sample[col] = df_scaled[col].min() - np.random.rand() * 3\n",
        "    outliers.append(outlier_sample)\n",
        "\n",
        "outliers_df = pd.concat(outliers, ignore_index=True)\n",
        "\n",
        "# 이상치의 타겟 값 랜덤 할당\n",
        "outliers_df['target'] = np.random.choice(y, num_outliers)\n",
        "\n",
        "# 원본 데이터에 이상치 추가\n",
        "df_extended = pd.concat([df, outliers_df], ignore_index=True)\n",
        "\n",
        "# 이상치 추가 후 다시 정규화\n",
        "X_extended_scaled = scaler.fit_transform(df_extended.drop(columns=['target']))\n",
        "df_extended_scaled = pd.DataFrame(X_extended_scaled, columns=iris.feature_names)\n",
        "\n",
        "# 새로운 X, y 정의\n",
        "X = df_extended_scaled.values  # NumPy 배열로 변환\n",
        "y = df_extended['target'].values  # 타겟 값 추출\n",
        "\n",
        "# 기존 이상치 식별\n",
        "df_zscores = df_scaled.apply(zscore)  # Z-score 계산\n",
        "is_existing_outlier = (df_zscores.abs() > 3).any(axis=1)  # 절대값이 3 이상인 경우 이상치로 판단\n",
        "\n",
        "total_existing_outliers = is_existing_outlier.sum()  # 기존 이상치 수\n",
        "total_outliers = total_existing_outliers + num_outliers  # 총 이상치 수\n",
        "total_samples = len(df_extended)  # 전체 샘플 수\n",
        "total_outlier_percentage = (total_outliers / total_samples) * 100  # 이상치 비율 계산\n",
        "\n",
        "# 데이터 확인\n",
        "print(f\"확장된 데이터셋 크기: {X.shape}\")\n",
        "print(f\"확장된 타겟 크기: {y.shape}\")\n",
        "print(f\"이상치가 포함된 데이터셋의 총 샘플 수: {len(X)}\")\n",
        "print(f\"전체 이상치 비율: {total_outlier_percentage:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "id": "pJGd9cJBpN2O",
        "outputId": "f928b08a-533e-4db3-f926-2a46117c387b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-e17aa612f33f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# OvO 방식을 사용하여 각 SVM 유형에 대해 학습 및 평가\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msvm_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'L1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'L2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Fair'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Cauchy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Welsch'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Geman-McClure'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mmodels_ovo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_ovo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msvm_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msvm_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0my_pred_ovo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_ovo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels_ovo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-e05b4a6fa682>\u001b[0m in \u001b[0;36mtrain_ovo\u001b[0;34m(X, y, C, svm_type, gamma)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0my_bin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_bin\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mcl1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLBFGS_SVM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msvm_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msvm_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_bin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_bin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcl1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcl2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 모델 객체 자체 저장\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-03770742de03>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mconstraints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'type'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'eq'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fun'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfitness\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbounds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstraints\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/optimize/_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    748\u001b[0m                                **options)\n\u001b[1;32m    749\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'slsqp'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 750\u001b[0;31m         res = _minimize_slsqp(fun, x0, args, jac, bounds,\n\u001b[0m\u001b[1;32m    751\u001b[0m                               constraints, callback=callback, **options)\n\u001b[1;32m    752\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'trust-constr'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/optimize/_slsqp_py.py\u001b[0m in \u001b[0;36m_minimize_slsqp\u001b[0;34m(func, x0, args, jac, bounds, constraints, maxiter, ftol, iprint, disp, eps, callback, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0;31m# Call SLSQP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m         slsqp(m, meq, x, xl, xu, fx, c, g, a, acc, majiter, mode, w, jw,\n\u001b[0m\u001b[1;32m    430\u001b[0m               \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m               \u001b[0miexact\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincons\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mireset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitermx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from itertools import combinations\n",
        "\n",
        "results = []\n",
        "\n",
        "# 데이터 전처리 및 분할\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# OvO 방식을 사용하여 각 SVM 유형에 대해 학습 및 평가\n",
        "for svm_type in ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']:\n",
        "    models_ovo = train_ovo(X_train, y_train, C=1.0, svm_type=svm_type, gamma=0.1)\n",
        "    y_pred_ovo = predict_ovo(X_test, models_ovo)\n",
        "\n",
        "    # y_pred_ovo 변수를 정의한 후에 데이터 타입 확인 및 변환\n",
        "    if y_test.dtype != np.int64:\n",
        "        y_test = y_test.astype(int)\n",
        "\n",
        "    if y_pred_ovo.dtype != np.int64:\n",
        "        y_pred_ovo = y_pred_ovo.astype(int)\n",
        "\n",
        "    accuracy_ovo = accuracy_score(y_test, y_pred_ovo)\n",
        "    results.append((svm_type, round(accuracy_ovo * 100, 2)))\n",
        "\n",
        "# 결과를 DataFrame으로 변환 및 출력\n",
        "results_df = pd.DataFrame(results, columns=['SVM Type', 'Accuracy'])\n",
        "print(results_df['Accuracy'].tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vCwdZcbpN4n",
        "outputId": "8b18cec7-002d-4990-8b35-482f6f677da8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                      3.25%  5.10%  10.24%\n",
            "Iris data (L-BFGS-B)                      \n",
            "L1                    97.87  95.83    92.0\n",
            "L2                    97.87  93.75    76.0\n",
            "Fair                  95.74  93.75    84.0\n",
            "Cauchy                97.87  93.75    94.0\n",
            "Welsch                95.74  91.67    94.0\n",
            "Geman-McClure         95.74  91.67    94.0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "#0.67% (L-BFGS-B,PSO,HS)\n",
        "[97.78, 97.78, 97.78, 97.78, 97.78, 97.78]\n",
        "[97.87, 97.87, 95.74, 97.87, 95.74, 95.74]\n",
        "[88.89, 93.33, 100.0, 91.11, 97.78, 88.89]\n",
        "\n",
        "#3.25%\n",
        "[97.87, 97.87, 95.74, 97.87, 95.74, 95.74]\n",
        "[97.87, 97.87, 95.74, 97.87, 95.74, 95.74]\n",
        "[91.49, 87.23, 95.74, 95.74, 91.49, 89.36]\n",
        "\n",
        "#4.49%\n",
        "[97.87, 76.6, 80.85, 97.87, 97.87, 97.87]\n",
        "[97.87, 76.6, 80.85, 97.87, 97.87, 97.87]\n",
        "[82.98, 55.32, 87.23, 87.23, 91.49, 82.98]\n",
        "\n",
        "\n",
        "\n",
        "#3.25%\n",
        "[97.87, 97.87, 95.74, 97.87, 95.74, 95.74]\n",
        "[85.11, 91.49, 80.85, 87.23, 80.85, 85.11]\n",
        "[97.87, 97.87, 95.74, 97.87, 95.74, 95.74]\n",
        "[97.78, 97.78, 97.78, 97.78, 97.78, 97.78]\n",
        "[91.49, 87.23, 95.74, 95.74, 91.49, 89.36]\n",
        "#3.87%\n",
        "[97.87, 89.36, 95.74, 95.74, 95.74, 95.74]\n",
        "[76.6, 70.21, 85.11, 82.98, 76.6, 74.47]\n",
        "[97.87, 89.36, 95.74, 95.74, 95.74, 95.74]\n",
        "[97.87, 89.36, 95.74, 95.74, 95.74, 95.74]\n",
        "[82.98, 91.49, 85.11, 89.36, 93.62, 89.36]\n",
        "#4.49%\n",
        "[97.87, 76.6, 80.85, 97.87, 97.87, 97.87]\n",
        "[68.09, 63.83, 87.23, 78.72, 82.98, 85.11]\n",
        "[97.87, 76.6, 80.85, 97.87, 97.87, 97.87]\n",
        "[97.87, 72.34, 85.11, 100.0, 100.0, 97.87]\n",
        "[82.98, 55.32, 87.23, 87.23, 91.49, 82.98]\n",
        "#5.8%\n",
        "[87.5, 79.17, 83.33, 91.67, 91.67, 91.67]\n",
        "[75.0, 77.08, 81.25, 85.42, 75.0, 81.25]\n",
        "[87.5, 79.17, 83.33, 91.67, 91.67, 91.67]\n",
        "[87.5, 79.17, 83.33, 91.67, 91.67, 91.67]\n",
        "[89.58, 79.17, 77.08, 87.5, 91.67, 91.67]\n",
        "#6.2%\n",
        "[93.75, 81.25, 89.58, 93.75, 97.92, 93.75]\n",
        "[68.75, 81.25, 89.58, 81.25, 70.83, 72.92]\n",
        "[93.75, 81.25, 89.58, 93.75, 97.92, 93.75]\n",
        "[93.75, 81.25, 89.58, 93.75, 93.75, 93.75]\n",
        "[60.42, 81.25, 77.08, 93.75, 95.83, 93.75]\n",
        "#6.8%\n",
        "[100.0, 100.0, 100.0, 100.0, 100.0, 100.0]\n",
        "[97.92, 77.08, 100.0, 89.58, 81.25, 77.08]\n",
        "[100.0, 100.0, 100.0, 100.0, 100.0, 100.0]\n",
        "[100.0, 100.0, 100.0, 100.0, 100.0, 97.92]\n",
        "[97.92, 93.75, 91.67, 91.67, 97.92, 100.0]\n",
        "#7.45%\n",
        "[97.96, 97.96, 97.96, 97.96, 97.96, 97.96]\n",
        "[71.43, 95.92, 87.76, 79.59, 77.55, 87.76]\n",
        "[97.96, 97.96, 97.96, 97.96, 97.96, 97.96]\n",
        "[97.96, 97.96, 97.96, 97.96, 97.96, 97.96]\n",
        "[93.88, 87.76, 81.63, 95.92, 97.96, 95.92]\n",
        "#8.02%\n",
        "[91.84, 89.8, 91.84, 93.88, 93.88, 93.88]\n",
        "[71.43, 89.8, 79.59, 81.63, 81.63, 73.47]\n",
        "[89.8, 89.8, 91.84, 93.88, 93.88, 91.84]\n",
        "[91.84, 89.8, 91.84, 93.88, 91.84, 91.84]\n",
        "[91.84, 75.51, 95.92, 91.84, 89.8, 85.71]\n",
        "\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "accuracy_data = {\n",
        "    '3.25%' : [97.87, 97.87, 95.74, 97.87, 95.74, 95.74],\n",
        "    '5.10%' : [95.83, 93.75, 93.75, 93.75, 91.67, 91.67],\n",
        "    '6.29%' : [93.75, 81.25, 89.58, 93.75, 97.92, 93.75]\n",
        "}\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Iris data (L-BFGS-B)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gC-p79Wmp78E",
        "outputId": "421b295d-fd28-4542-90c6-57e946bd2bd7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                3.25%  5.10%   6.88%\n",
            "Iris data (GA)                      \n",
            "L1              85.11  85.42   97.92\n",
            "L2              91.49  93.75   77.08\n",
            "Fair            80.85  75.00  100.00\n",
            "Cauchy          87.23  89.58   89.58\n",
            "Welsch          80.85  77.08   81.25\n",
            "Geman-McClure   85.11  81.25   77.08\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "outlier = ['3%', '5%', '10%']\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "\n",
        "accuracy_data = {\n",
        "    '3.25%' : [85.11, 91.49, 80.85, 87.23, 80.85, 85.11],\n",
        "    '5.10%' : [85.42, 93.75, 75.00, 89.58, 77.08, 81.25],\n",
        "    '6.88%' : [97.92, 77.08, 100.0, 89.58, 81.25, 77.08]\n",
        "}\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Iris data (GA)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LdHGIl4Ip76L",
        "outputId": "652ac1d4-b1d8-429d-911c-92ed2eceb0ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                 3.25%  5.10%  10.24%\n",
            "Iris data (SMO)                      \n",
            "L1               36.17  37.50    34.0\n",
            "L2               36.17  37.50    34.0\n",
            "Fair             97.87  91.67    90.0\n",
            "Cauchy           97.87  91.67    90.0\n",
            "Welsch           97.87  91.67    90.0\n",
            "Geman-McClure    97.87  91.67    90.0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "outlier = ['3%', '5%', '10%']\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "\n",
        "accuracy_data = {\n",
        "    '3.25%' : [36.17, 36.17, 97.87, 97.87, 97.87, 97.87],\n",
        "    '5.10%' : [37.50, 37.50, 91.67, 91.67, 91.67, 91.67],\n",
        "    '10.24%' : [34.0, 34.0, 90.0, 90.0, 90.0, 90.0]\n",
        "}\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Iris data (SMO)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vEaYO1Ep74Q",
        "outputId": "cdcce1e3-cccf-4510-8c6b-2a8a1d94f0f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                 3.25%  5.10%  10.24%\n",
            "Iris data (PSO)                      \n",
            "L1               97.87  95.83    92.0\n",
            "L2               97.87  93.75    76.0\n",
            "Fair             95.74  93.75    84.0\n",
            "Cauchy           97.87  93.75    94.0\n",
            "Welsch           95.74  91.67    94.0\n",
            "Geman-McClure    95.74  91.67    94.0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "outlier = ['3%', '5%', '10%']\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "\n",
        "accuracy_data = {\n",
        "    '3.25%' : [97.87, 97.87, 95.74, 97.87, 95.74, 95.74],\n",
        "    '5.10%' : [95.83, 93.75, 93.75, 93.75, 91.67, 91.67],\n",
        "    '10.24%' : [92.0, 76.0, 84.0, 94.0, 94.0, 94.0]\n",
        "}\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Iris data (PSO)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0eQhqpCapN8E",
        "outputId": "ddd63cb1-9bde-4630-dd72-cee768e6d7ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                 3.25%  5.10%  10.24%\n",
            "Iris data (ACO)                      \n",
            "L1               97.78  97.87   95.83\n",
            "L2               97.78  97.87   93.75\n",
            "Fair             97.78  97.87   93.75\n",
            "Cauchy           97.78  97.87   93.75\n",
            "Welsch           97.78  95.74   91.67\n",
            "Geman-McClure    97.78  95.74   91.67\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "outlier = ['3%', '5%', '10%']\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "\n",
        "accuracy_data = {\n",
        "    '3.25%' : [97.78, 97.78, 97.78, 97.78, 97.78, 97.78],\n",
        "    '5.10%' : [97.87, 97.87, 97.87, 97.87, 95.74, 95.74],\n",
        "    '10.24%' : [95.83, 93.75, 93.75, 93.75, 91.67, 91.67]\n",
        "}\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Iris data (ACO)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MALlXApRocLs",
        "outputId": "9ee28313-3294-4b0e-d649-8b4464f8b066"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                3.25%  5.10%  10.24%\n",
            "Iris data (HS)                      \n",
            "L1              91.49  93.75    84.0\n",
            "L2              87.23  91.67    72.0\n",
            "Fair            95.74  91.67    66.0\n",
            "Cauchy          95.74  95.83    92.0\n",
            "Welsch          91.49  91.67    92.0\n",
            "Geman-McClure   89.36  85.42    88.0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "accuracy_data = {\n",
        "    '3.25%' : [91.49, 87.23, 95.74, 95.74, 91.49, 89.36],\n",
        "    '5.10%' : [93.75, 91.67, 91.67, 95.83, 91.67, 85.42],\n",
        "    '10.24%' : [84.0, 72.0, 66.0, 92.0, 92.0, 88.0]\n",
        "}\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Iris data (HS)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dF2RFLZFo0OL"
      },
      "source": [
        "### segment data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ntbEK53go0OO",
        "outputId": "4a7b115f-26ee-482f-f36b-f9a33f898d08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "확장된 데이터셋 크기: (2374, 19)\n",
            "확장된 타겟 크기: (2374,)\n",
            "이상치가 포함된 데이터셋의 총 샘플 수: 2374\n",
            "전체 이상치 비율: 10.07%\n",
            "전체 이상치 개수: 239\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from scipy.stats import zscore\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# 데이터셋 로딩\n",
        "data = fetch_openml(name='segment', version=1)\n",
        "X = data.data\n",
        "# Use LabelEncoder to convert categorical target to numerical\n",
        "encoder = LabelEncoder()\n",
        "y = encoder.fit_transform(data.target)\n",
        "df = pd.DataFrame(X, columns=data.feature_names)\n",
        "df['target'] = y  # 타겟 변수 추가\n",
        "\n",
        "\n",
        "# 데이터 정규화\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "df_scaled = pd.DataFrame(X_scaled, columns=data.feature_names)\n",
        "\n",
        "# 이상치 생성\n",
        "num_outliers = int(0.028 * len(df))  # 이상치를 설정\n",
        "outliers = []\n",
        "\n",
        "for _ in range(num_outliers):\n",
        "    outlier_sample = df_scaled.sample(n=1).copy()  # 임의의 샘플 선택\n",
        "    for col in df_scaled.columns:\n",
        "        if np.random.rand() > 0.5:\n",
        "            # 최대값보다 큰 이상치 생성\n",
        "            outlier_sample[col] = df_scaled[col].max() + np.random.rand() * 3\n",
        "        else:\n",
        "            # 최소값보다 작은 이상치 생성\n",
        "            outlier_sample[col] = df_scaled[col].min() - np.random.rand() * 3\n",
        "    outliers.append(outlier_sample)\n",
        "\n",
        "outliers_df = pd.concat(outliers, ignore_index=True)\n",
        "\n",
        "# 이상치의 타겟 값 랜덤 할당\n",
        "outliers_df['target'] = np.random.choice(y, num_outliers)\n",
        "\n",
        "# 원본 데이터에 이상치 추가\n",
        "df_extended = pd.concat([df, outliers_df], ignore_index=True)\n",
        "\n",
        "# 이상치 추가 후 다시 정규화\n",
        "X_extended_scaled = scaler.fit_transform(df_extended.drop(columns=['target']))\n",
        "df_extended_scaled = pd.DataFrame(X_extended_scaled, columns=data.feature_names)\n",
        "\n",
        "# 새로운 X, y 정의\n",
        "X = df_extended_scaled.values  # NumPy 배열로 변환\n",
        "y = df_extended['target'].values  # 타겟 값 추출\n",
        "\n",
        "# 기존 이상치 식별\n",
        "df_zscores = df_scaled.apply(zscore)  # Z-score 계산\n",
        "is_existing_outlier = (df_zscores.abs() > 3).any(axis=1)  # 절대값이 3 이상인 경우 이상치로 판단\n",
        "\n",
        "total_existing_outliers = is_existing_outlier.sum()  # 기존 이상치 수\n",
        "total_outliers = total_existing_outliers + num_outliers  # 총 이상치 수\n",
        "total_samples = len(df_extended)  # 전체 샘플 수\n",
        "total_outlier_percentage = (total_outliers / total_samples) * 100  # 이상치 비율 계산\n",
        "\n",
        "# 데이터 확인\n",
        "print(f\"확장된 데이터셋 크기: {X.shape}\")\n",
        "print(f\"확장된 타겟 크기: {y.shape}\")\n",
        "print(f\"이상치가 포함된 데이터셋의 총 샘플 수: {len(X)}\")\n",
        "print(f\"전체 이상치 비율: {total_outlier_percentage:.2f}%\")\n",
        "print(f\"전체 이상치 개수: {total_outliers}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "ajUESHaje0O2",
        "outputId": "31dd5a24-580d-4e3b-d048-5700358e6661"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'numpy.ndarray' object has no attribute 'iloc'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-d1f080f170cb>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtrain_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mskf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'iloc'"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "results = []\n",
        "\n",
        "# 5-fold Stratified K-Fold 설정\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# OvO 방식을 사용하여 각 SVM 유형에 대해 학습 및 평가\n",
        "for svm_type in ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']:\n",
        "    acc_list = []\n",
        "\n",
        "    for train_index, test_index in skf.split(X, y):\n",
        "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "        y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "        # 표준화\n",
        "        scaler = StandardScaler()\n",
        "        X_train = scaler.fit_transform(X_train)\n",
        "        X_test = scaler.transform(X_test)\n",
        "\n",
        "        # OvO 학습 및 예측\n",
        "        models_ovo = train_ovo(X_train, y_train, C=1.0, svm_type=svm_type)\n",
        "        y_pred_ovo = predict_ovo(X_test, models_ovo)\n",
        "\n",
        "        # 데이터 타입 확인 및 변환 (필요 시)\n",
        "        if y_test.dtype != np.int64:\n",
        "            y_test = y_test.astype(int)\n",
        "        if y_pred_ovo.dtype != np.int64:\n",
        "            y_pred_ovo = y_pred_ovo.astype(int)\n",
        "\n",
        "        # 정확도 계산\n",
        "        acc = accuracy_score(y_test, y_pred_ovo)\n",
        "        acc_list.append(acc)\n",
        "\n",
        "    # 평균 정확도 계산\n",
        "    mean_acc = np.mean(acc_list)\n",
        "    results.append((svm_type, round(mean_acc * 100, 2)))\n",
        "\n",
        "# 결과를 DataFrame으로 출력\n",
        "results_df = pd.DataFrame(results, columns=['SVM Type', 'Accuracy'])\n",
        "print(results_df['Accuracy'].tolist())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJ8QcmQSo0ON",
        "outputId": "77da0946-8359-4dbf-d9fa-d8c43a92c571"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "기존 이상치 수: 175\n",
            "기존 이상치 비율: 7.58%\n",
            "데이터 총 개수: 2310\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from scipy.stats import zscore\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# 데이터셋 로딩\n",
        "data = fetch_openml(name='segment', version=1)\n",
        "X = data.data\n",
        "# Use LabelEncoder to convert categorical target to numerical\n",
        "encoder = LabelEncoder()\n",
        "y = encoder.fit_transform(data.target)\n",
        "df = pd.DataFrame(X, columns=data.feature_names)\n",
        "df['target'] = y  # 타겟 변수 추가\n",
        "\n",
        "# 데이터 정규화\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "df_scaled = pd.DataFrame(X_scaled, columns=data.feature_names)\n",
        "\n",
        "# 기존 이상치 식별\n",
        "df_zscores = df_scaled.apply(zscore)  # Z-score 계산\n",
        "is_existing_outlier = (df_zscores.abs() > 3).any(axis=1)  # 절대값이 3 이상인 경우 이상치로 판단\n",
        "\n",
        "# 기존 이상치 수 계산\n",
        "total_existing_outliers = is_existing_outlier.sum()\n",
        "\n",
        "# 기존 이상치 비율 계산\n",
        "total_samples = len(df)\n",
        "existing_outlier_percentage = (total_existing_outliers / total_samples) * 100\n",
        "\n",
        "print(f\"기존 이상치 수: {total_existing_outliers}\")\n",
        "print(f\"기존 이상치 비율: {existing_outlier_percentage:.2f}%\")\n",
        "print(f\"데이터 총 개수: {len(X)}\")\n",
        "\n",
        "# 타겟 포함 전체 데이터프레임 생성\n",
        "full_df = pd.concat([df_scaled, pd.Series(y, name='target')], axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKV7WgiLvgiy"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from scipy.stats import zscore\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# 데이터셋 로딩\n",
        "data = fetch_openml(name='segment', version=1)\n",
        "X = data.data\n",
        "# Use LabelEncoder to convert categorical target to numerical\n",
        "encoder = LabelEncoder()\n",
        "y = encoder.fit_transform(data.target)\n",
        "df = pd.DataFrame(X, columns=data.feature_names)\n",
        "df['target'] = y  # 타겟 변수 추가\n",
        "\n",
        "# 데이터 정규화\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "df_scaled = pd.DataFrame(X_scaled, columns=data.feature_names)\n",
        "\n",
        "# 기존 이상치 식별\n",
        "df_zscores = df_scaled.apply(zscore)  # Z-score 계산\n",
        "is_existing_outlier = (df_zscores.abs() > 4.5).any(axis=1)  # 절대값이 3 이상인 경우 이상치로 판단\n",
        "\n",
        "df_cleaned = df.loc[~is_existing_outlier].reset_index(drop=True)\n",
        "X_cleaned = df_cleaned.drop(columns='target')\n",
        "y_cleaned = df_cleaned['target']\n",
        "\n",
        "# 새로운 X, y 정의\n",
        "X = X_cleaned.values  # NumPy 배열로 변환\n",
        "y = y_cleaned.values  # 타겟 값 추출"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "id": "2jwo-w2to0OQ",
        "outputId": "2a46d978-f25a-4f08-88e5-6cc9d329f41f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-4e1a0b476768>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# OvO 방식을 사용하여 각 SVM 유형에 대해 학습 및 평가\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msvm_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'L1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'L2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Fair'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Cauchy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Welsch'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Geman-McClure'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mmodels_ovo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_ovo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msvm_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msvm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0my_pred_ovo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_ovo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels_ovo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-e05b4a6fa682>\u001b[0m in \u001b[0;36mtrain_ovo\u001b[0;34m(X, y, C, svm_type, gamma)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0my_bin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_bin\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mcl1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLBFGS_SVM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msvm_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msvm_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_bin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_bin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcl1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcl2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 모델 객체 자체 저장\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-03770742de03>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mconstraints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'type'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'eq'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fun'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfitness\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbounds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstraints\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/optimize/_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    748\u001b[0m                                **options)\n\u001b[1;32m    749\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'slsqp'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 750\u001b[0;31m         res = _minimize_slsqp(fun, x0, args, jac, bounds,\n\u001b[0m\u001b[1;32m    751\u001b[0m                               constraints, callback=callback, **options)\n\u001b[1;32m    752\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'trust-constr'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/optimize/_slsqp_py.py\u001b[0m in \u001b[0;36m_minimize_slsqp\u001b[0;34m(func, x0, args, jac, bounds, constraints, maxiter, ftol, iprint, disp, eps, callback, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0;31m# Call SLSQP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m         slsqp(m, meq, x, xl, xu, fx, c, g, a, acc, majiter, mode, w, jw,\n\u001b[0m\u001b[1;32m    430\u001b[0m               \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m               \u001b[0miexact\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincons\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mireset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitermx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from itertools import combinations\n",
        "\n",
        "results = []\n",
        "\n",
        "# 데이터 전처리 및 분할\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# OvO 방식을 사용하여 각 SVM 유형에 대해 학습 및 평가\n",
        "for svm_type in ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']:\n",
        "    models_ovo = train_ovo(X_train, y_train, C=1.0, svm_type=svm_type)\n",
        "    y_pred_ovo = predict_ovo(X_test, models_ovo)\n",
        "\n",
        "    # y_pred_ovo 변수를 정의한 후에 데이터 타입 확인 및 변환\n",
        "    if y_test.dtype != np.int64:\n",
        "        y_test = y_test.astype(int)\n",
        "\n",
        "    if y_pred_ovo.dtype != np.int64:\n",
        "        y_pred_ovo = y_pred_ovo.astype(int)\n",
        "\n",
        "    accuracy_ovo = accuracy_score(y_test, y_pred_ovo)\n",
        "    results.append((svm_type, round(accuracy_ovo * 100, 2)))\n",
        "\n",
        "# 결과를 DataFrame으로 변환 및 출력\n",
        "results_df = pd.DataFrame(results, columns=['SVM Type', 'Accuracy'])\n",
        "print(results_df['Accuracy'].tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqJ_eq70xVDm",
        "outputId": "24cf8837-59f6-46f8-cd40-4f578c203dc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                          3 sigma  4.5 sigma  full original data\n",
            "Segment data (L-BFGS-B)                                         \n",
            "L1                          94.38      95.54               93.65\n",
            "L2                          94.70      95.83               94.52\n",
            "Fair                        93.92      95.39               93.51\n",
            "Cauchy                      94.23      95.39               93.51\n",
            "Welsch                      94.54      94.35               93.80\n",
            "Geman-McClure               93.92      94.20               92.64\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "accuracy_data = {\n",
        "    ' 3 sigma' : [94.38, 94.70, 93.92, 94.23, 94.54, 93.92],\n",
        "    '4.5 sigma' : [95.54, 95.83, 95.39, 95.39, 94.35, 94.20],\n",
        "    'full original data' : [93.65, 94.52, 93.51, 93.51, 93.80, 92.64]\n",
        "}\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Segment data (L-BFGS-B)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkwkXTg3bBv3",
        "outputId": "38754220-f977-4fcb-f94c-12846c882148"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   3 sigma  4.5 sigma  full original data\n",
            "Segment data (GA)                                        \n",
            "L1                   82.68      87.95               85.57\n",
            "L2                   87.36      88.69               89.61\n",
            "Fair                 82.37      85.27               87.59\n",
            "Cauchy               86.43      85.27               86.72\n",
            "Welsch               85.34      85.42               84.85\n",
            "Geman-McClure        84.71      84.67               86.72\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "accuracy_data = {\n",
        "    '3 sigma' : [82.68, 87.36, 82.37, 86.43, 85.34, 84.71],\n",
        "    '4.5 sigma' : [87.95, 88.69, 85.27, 85.27, 85.42, 84.67],\n",
        "    'full original data' : [85.57, 89.61, 87.59, 86.72, 84.85, 86.72]\n",
        "}\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Segment data (GA)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "tiY7hvwkbCUY",
        "outputId": "39fc0caf-d31d-4d76-acdc-4f768a26f4de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                    3 sigma  4.5 sigma  full original data\n",
            "Segment data (SMO)                                        \n",
            "L1                    16.69      15.77               15.87\n",
            "L2                    16.69      15.77               15.87\n",
            "Fair                  95.63      95.98               94.37\n",
            "Cauchy                95.63      96.13               94.52\n",
            "Welsch                95.79      95.83               94.52\n",
            "Geman-McClure         95.63      95.83               94.52\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "accuracy_data = {\n",
        "    '3 sigma' : [16.69, 16.69, 95.63, 95.63, 95.79, 95.63],\n",
        "    '4.5 sigma' : [15.77, 15.77, 95.98, 96.13, 95.83, 95.83],\n",
        "    'full original data' : [15.87, 15.87, 94.37, 94.52, 94.52, 94.52]\n",
        "}\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Segment data (SMO)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHrcHS3qbBqF",
        "outputId": "25eb2220-7d5b-4889-8727-3937aad11895"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                    3 sigma  4.5 sigma  full original data\n",
            "Segment data (PSO)                                        \n",
            "L1                    93.29      92.56               92.34\n",
            "L2                    92.51      89.29               91.77\n",
            "Fair                  93.60      93.30               92.12\n",
            "Cauchy                90.33      89.73               91.95\n",
            "Welsch                92.51      92.56               91.95\n",
            "Geman-McClure         90.17      90.33               92.25\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "accuracy_data = {\n",
        "    '3 sigma' : [93.29, 92.51, 93.6, 90.33, 92.51, 90.17],\n",
        "    '4.5 sigma' : [92.56, 89.29, 93.3, 89.73, 92.56, 90.33],\n",
        "    'full original data' : [90.04, 90.91, 90.91, 89.47, 92.35, 90.91]\n",
        "}\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Segment data (PSO)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVqfydOGysrC",
        "outputId": "6056a064-b5f6-46b2-aafb-4d51fb056ea6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                    3 sigma  4.5 sigma  original data\n",
            "Segment data (ACO)                                   \n",
            "L1                    95.16      95.39          93.07\n",
            "L2                    92.98      94.79          92.93\n",
            "Fair                  94.85      94.49          93.51\n",
            "Cauchy                94.07      94.79          93.36\n",
            "Welsch                91.42      94.35          93.22\n",
            "Geman-McClure         93.60      91.96          91.92\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "accuracy_data = {\n",
        "     '3 sigma' : [95.16, 92.98, 94.85, 94.07, 91.42, 93.6],\n",
        "    '4.5 sigma' : [95.39, 94.79, 94.49, 94.79, 94.35, 91.96],\n",
        "    'original data' : [93.07, 92.93, 93.51, 93.36, 93.22, 91.92]\n",
        "}\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Segment data (ACO)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ypqwhXig1MUl",
        "outputId": "1a436d9f-5029-4cc4-ea43-6d97bfb2682f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   3 sigma  4.5 sigma  full original data\n",
            "Segment data (HS)                                        \n",
            "L1                   84.24      84.52               85.86\n",
            "L2                   81.90      84.52               78.64\n",
            "Fair                 84.24      83.48               82.25\n",
            "Cauchy               88.46      86.16               84.42\n",
            "Welsch               86.58      84.82               82.83\n",
            "Geman-McClure        84.40      81.55               81.10\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "accuracy_data = {\n",
        "    '3 sigma' : [84.24, 81.9, 84.24, 88.46, 86.58, 84.4],\n",
        "    '4.5 sigma' : [84.52, 84.52, 83.48, 86.16, 84.82, 81.55],\n",
        "    'full original data' : [85.86, 78.64, 82.25, 84.42, 82.83, 81.10]\n",
        "}\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Segment data (HS)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RiqZtMkxo0OR",
        "outputId": "0ef8d73f-7cf6-45f0-d739-444f58eef206"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                         7.58%  10.07%\n",
            "Segment data (L-BFGS-B)               \n",
            "L1                       93.65   92.71\n",
            "L2                       94.52   93.27\n",
            "Fair                     93.51   92.15\n",
            "Cauchy                   93.51   92.29\n",
            "Welsch                   93.80   93.41\n",
            "Geman-McClure            92.64   92.99\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "accuracy_data = {\n",
        "    '7.58%' : [93.65, 94.52, 93.51, 93.51, 93.80, 92.64],\n",
        "    '10.07%' : [92.71, 93.27, 92.15, 92.29, 93.41, 92.99]\n",
        "}\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Segment data (L-BFGS-B)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwrJ31u-o0OR",
        "outputId": "3f63cfab-5e23-4b7d-d37f-e053b344921e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   7.58%  10.07%\n",
            "Segment data (GA)               \n",
            "L1                 85.57   85.41\n",
            "L2                 89.61   85.55\n",
            "Fair               87.59   85.97\n",
            "Cauchy             86.72   87.80\n",
            "Welsch             84.85   85.97\n",
            "Geman-McClure      86.72   85.41\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "outlier = ['3%', '5%', '10%']\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "\n",
        "accuracy_data = {\n",
        "    '7.58%' : [85.57, 89.61, 87.59, 86.72, 84.85, 86.72],\n",
        "    '10.07%' : [85.41, 85.55, 85.97, 87.80, 85.97, 85.41\n",
        "]\n",
        "}\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Segment data (GA)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWgi-NdRo0OS",
        "outputId": "fc168c9e-991d-4c19-fe30-8662f66e70f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                    7.58%  10.07%\n",
            "Segment data (SMO)               \n",
            "L1                  15.87   14.87\n",
            "L2                  15.87   14.87\n",
            "Fair                94.37   94.39\n",
            "Cauchy              94.52   94.25\n",
            "Welsch              94.52   94.53\n",
            "Geman-McClure       94.52   94.67\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "outlier = ['3%', '5%', '10%']\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "\n",
        "accuracy_data = {\n",
        "    '7.58%' : [15.87, 15.87, 94.37, 94.52, 94.52, 94.52],\n",
        "    '10.07%' : [14.87, 14.87, 94.39, 94.25, 94.53, 94.67]\n",
        "}\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Segment data (SMO)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEGU7KxI56xb",
        "outputId": "f1f91225-dad0-43e4-8bc8-176ee1ad4917"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                    7.58%  10.07%\n",
            "Segment data (PSO)               \n",
            "L1                  90.04   91.87\n",
            "L2                  90.91   92.15\n",
            "Fair                90.91   91.73\n",
            "Cauchy              89.47   92.57\n",
            "Welsch              92.35   91.73\n",
            "Geman-McClure       90.91   92.85\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "outlier = ['3%', '5%', '10%']\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "\n",
        "accuracy_data = {\n",
        "    '7.58%' : [90.04, 90.91, 90.91, 89.47, 92.35, 90.91],\n",
        "    '10.07%' : [91.87, 92.15, 91.73, 92.57, 91.73, 92.85]\n",
        "}\n",
        "\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Segment data (PSO)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAE2w4CZ563F",
        "outputId": "454f6e27-e5f1-48c6-d2b3-1e25deabbe9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                    7.58%  10.07%\n",
            "Segment data (ACO)               \n",
            "L1                  93.07   92.15\n",
            "L2                  92.93   89.76\n",
            "Fair                93.51   92.29\n",
            "Cauchy              93.36   93.13\n",
            "Welsch              93.22   93.41\n",
            "Geman-McClure       91.92   92.43\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "outlier = ['3%', '5%', '10%']\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "\n",
        "accuracy_data = {\n",
        "    '7.58%' : [93.07, 92.93, 93.51, 93.36, 93.22, 91.92],\n",
        "    '10.07%' : [91.87, 92.15, 91.73, 92.57, 91.73, 92.85]\n",
        "}\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Segment data (ACO)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9QicRClH56-K",
        "outputId": "2673a7c2-287b-40b6-ac35-1d80440936f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   7.58%  10.07%\n",
            "Segment data (HS)               \n",
            "L1                 85.86   84.71\n",
            "L2                 78.64   72.65\n",
            "Fair               82.25   85.55\n",
            "Cauchy             84.42   83.73\n",
            "Welsch             82.83   86.12\n",
            "Geman-McClure      81.10   86.12\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "outlier = ['3%', '5%', '10%']\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "\n",
        "accuracy_data = {\n",
        "    '7.58%' : [85.86, 78.64, 82.25, 84.42, 82.83, 81.10],\n",
        "    '10.07%' : [84.71, 72.65, 85.55, 83.73, 86.12, 86.12]\n",
        "}\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Segment data (HS)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnhhupsifrXC"
      },
      "source": [
        "### vehicle data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FrKXBGn4gbVy",
        "outputId": "e9e965cf-b230-4535-ae53-fcdc3a90e231"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "기존 이상치 수: 22\n",
            "기존 이상치 비율: 2.60%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from scipy.stats import zscore\n",
        "import random\n",
        "\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# 데이터셋 로딩\n",
        "data = fetch_openml(name='vehicle', version=1)\n",
        "X = data.data\n",
        "encoder = LabelEncoder()\n",
        "y = encoder.fit_transform(data.target)\n",
        "df = pd.DataFrame(X, columns=data.feature_names)\n",
        "df['target'] = y  # 타겟 변수 추가\n",
        "\n",
        "# 데이터 정규화\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "df_scaled = pd.DataFrame(X_scaled, columns=data.feature_names)\n",
        "\n",
        "# 기존 이상치 식별\n",
        "df_zscores = df_scaled.apply(zscore)  # Z-score 계산\n",
        "is_existing_outlier = (df_zscores.abs() > 3).any(axis=1)  # 절대값이 3 이상인 경우 이상치로 판단\n",
        "\n",
        "# 기존 이상치 수 계산\n",
        "total_existing_outliers = is_existing_outlier.sum()\n",
        "\n",
        "# 기존 이상치 비율 계산\n",
        "total_samples = len(df)\n",
        "existing_outlier_percentage = (total_existing_outliers / total_samples) * 100\n",
        "\n",
        "print(f\"기존 이상치 수: {total_existing_outliers}\")\n",
        "print(f\"기존 이상치 비율: {existing_outlier_percentage:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYFOv6gvgbVz",
        "outputId": "d2fa1f9e-d42e-41df-b23b-678b3c4f1237"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "확장된 데이터셋 크기: (850, 18)\n",
            "확장된 타겟 크기: (850,)\n",
            "이상치가 포함된 데이터셋의 총 샘플 수: 850\n",
            "전체 이상치 비율: 3.06%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import load_iris\n",
        "from scipy.stats import zscore\n",
        "import random\n",
        "\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# 데이터셋 로딩\n",
        "data = fetch_openml(name='vehicle', version=1)\n",
        "X = data.data\n",
        "# Use LabelEncoder to convert categorical target to numerical\n",
        "encoder = LabelEncoder()\n",
        "y = encoder.fit_transform(data.target)\n",
        "df = pd.DataFrame(X, columns=data.feature_names)\n",
        "df['target'] = y  # 타겟 변수 추가\n",
        "\n",
        "\n",
        "# 데이터 정규화\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "df_scaled = pd.DataFrame(X_scaled, columns=data.feature_names)\n",
        "\n",
        "# 이상치 생성\n",
        "num_outliers = int(0.0048 * len(df))  # 이상치를 설정\n",
        "outliers = []\n",
        "\n",
        "for _ in range(num_outliers):\n",
        "    outlier_sample = df_scaled.sample(n=1).copy()  # 임의의 샘플 선택\n",
        "    for col in df_scaled.columns:\n",
        "        if np.random.rand() > 0.5:\n",
        "            # 최대값보다 큰 이상치 생성\n",
        "            outlier_sample[col] = df_scaled[col].max() + np.random.rand() * 3\n",
        "        else:\n",
        "            # 최소값보다 작은 이상치 생성\n",
        "            outlier_sample[col] = df_scaled[col].min() - np.random.rand() * 3\n",
        "    outliers.append(outlier_sample)\n",
        "\n",
        "outliers_df = pd.concat(outliers, ignore_index=True)\n",
        "\n",
        "# 이상치의 타겟 값 랜덤 할당\n",
        "outliers_df['target'] = np.random.choice(y, num_outliers)\n",
        "\n",
        "# 원본 데이터에 이상치 추가\n",
        "df_extended = pd.concat([df, outliers_df], ignore_index=True)\n",
        "\n",
        "# 이상치 추가 후 다시 정규화\n",
        "X_extended_scaled = scaler.fit_transform(df_extended.drop(columns=['target']))\n",
        "df_extended_scaled = pd.DataFrame(X_extended_scaled, columns=data.feature_names)\n",
        "\n",
        "# 새로운 X, y 정의\n",
        "X = df_extended_scaled.values  # NumPy 배열로 변환\n",
        "y = df_extended['target'].values  # 타겟 값 추출\n",
        "\n",
        "# 기존 이상치 식별\n",
        "df_zscores = df_scaled.apply(zscore)  # Z-score 계산\n",
        "is_existing_outlier = (df_zscores.abs() > 3).any(axis=1)  # 절대값이 3 이상인 경우 이상치로 판단\n",
        "\n",
        "total_existing_outliers = is_existing_outlier.sum()  # 기존 이상치 수\n",
        "total_outliers = total_existing_outliers + num_outliers  # 총 이상치 수\n",
        "total_samples = len(df_extended)  # 전체 샘플 수\n",
        "total_outlier_percentage = (total_outliers / total_samples) * 100  # 이상치 비율 계산\n",
        "\n",
        "# 데이터 확인\n",
        "print(f\"확장된 데이터셋 크기: {X.shape}\")\n",
        "print(f\"확장된 타겟 크기: {y.shape}\")\n",
        "print(f\"이상치가 포함된 데이터셋의 총 샘플 수: {len(X)}\")\n",
        "print(f\"전체 이상치 비율: {total_outlier_percentage:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z02LOEixu-Ks",
        "outputId": "1e1b908a-c58a-4a2f-c737-9c797dd8c239"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "확장된 데이터셋 크기: (868, 18)\n",
            "확장된 타겟 크기: (868,)\n",
            "이상치가 포함된 데이터셋의 총 샘플 수: 868\n",
            "전체 이상치 비율: 5.07%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import load_iris\n",
        "from scipy.stats import zscore\n",
        "import random\n",
        "\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# 데이터셋 로딩\n",
        "data = fetch_openml(name='vehicle', version=1)\n",
        "X = data.data\n",
        "# Use LabelEncoder to convert categorical target to numerical\n",
        "encoder = LabelEncoder()\n",
        "y = encoder.fit_transform(data.target)\n",
        "df = pd.DataFrame(X, columns=data.feature_names)\n",
        "df['target'] = y  # 타겟 변수 추가\n",
        "\n",
        "\n",
        "# 데이터 정규화\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "df_scaled = pd.DataFrame(X_scaled, columns=data.feature_names)\n",
        "\n",
        "# 이상치 생성\n",
        "num_outliers = int(0.027 * len(df))  # 이상치를 설정\n",
        "outliers = []\n",
        "\n",
        "for _ in range(num_outliers):\n",
        "    outlier_sample = df_scaled.sample(n=1).copy()  # 임의의 샘플 선택\n",
        "    for col in df_scaled.columns:\n",
        "        if np.random.rand() > 0.5:\n",
        "            # 최대값보다 큰 이상치 생성\n",
        "            outlier_sample[col] = df_scaled[col].max() + np.random.rand() * 3\n",
        "        else:\n",
        "            # 최소값보다 작은 이상치 생성\n",
        "            outlier_sample[col] = df_scaled[col].min() - np.random.rand() * 3\n",
        "    outliers.append(outlier_sample)\n",
        "\n",
        "outliers_df = pd.concat(outliers, ignore_index=True)\n",
        "\n",
        "# 이상치의 타겟 값 랜덤 할당\n",
        "outliers_df['target'] = np.random.choice(y, num_outliers)\n",
        "\n",
        "# 원본 데이터에 이상치 추가\n",
        "df_extended = pd.concat([df, outliers_df], ignore_index=True)\n",
        "\n",
        "# 이상치 추가 후 다시 정규화\n",
        "X_extended_scaled = scaler.fit_transform(df_extended.drop(columns=['target']))\n",
        "df_extended_scaled = pd.DataFrame(X_extended_scaled, columns=data.feature_names)\n",
        "\n",
        "# 새로운 X, y 정의\n",
        "X = df_extended_scaled.values  # NumPy 배열로 변환\n",
        "y = df_extended['target'].values  # 타겟 값 추출\n",
        "\n",
        "# 기존 이상치 식별\n",
        "df_zscores = df_scaled.apply(zscore)  # Z-score 계산\n",
        "is_existing_outlier = (df_zscores.abs() > 3).any(axis=1)  # 절대값이 3 이상인 경우 이상치로 판단\n",
        "\n",
        "total_existing_outliers = is_existing_outlier.sum()  # 기존 이상치 수\n",
        "total_outliers = total_existing_outliers + num_outliers  # 총 이상치 수\n",
        "total_samples = len(df_extended)  # 전체 샘플 수\n",
        "total_outlier_percentage = (total_outliers / total_samples) * 100  # 이상치 비율 계산\n",
        "\n",
        "# 데이터 확인\n",
        "print(f\"확장된 데이터셋 크기: {X.shape}\")\n",
        "print(f\"확장된 타겟 크기: {y.shape}\")\n",
        "print(f\"이상치가 포함된 데이터셋의 총 샘플 수: {len(X)}\")\n",
        "print(f\"전체 이상치 비율: {total_outlier_percentage:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ML-dqECu-96",
        "outputId": "66d47058-490e-4a72-8cca-2233519b11b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "확장된 데이터셋 크기: (916, 18)\n",
            "확장된 타겟 크기: (916,)\n",
            "이상치가 포함된 데이터셋의 총 샘플 수: 916\n",
            "전체 이상치 비율: 10.04%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import load_iris\n",
        "from scipy.stats import zscore\n",
        "import random\n",
        "\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# 데이터셋 로딩\n",
        "data = fetch_openml(name='vehicle', version=1)\n",
        "X = data.data\n",
        "# Use LabelEncoder to convert categorical target to numerical\n",
        "encoder = LabelEncoder()\n",
        "y = encoder.fit_transform(data.target)\n",
        "df = pd.DataFrame(X, columns=data.feature_names)\n",
        "df['target'] = y  # 타겟 변수 추가\n",
        "\n",
        "\n",
        "# 데이터 정규화\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "df_scaled = pd.DataFrame(X_scaled, columns=data.feature_names)\n",
        "\n",
        "# 이상치 생성\n",
        "num_outliers = int(0.083 * len(df))  # 이상치를 설정\n",
        "outliers = []\n",
        "\n",
        "for _ in range(num_outliers):\n",
        "    outlier_sample = df_scaled.sample(n=1).copy()  # 임의의 샘플 선택\n",
        "    for col in df_scaled.columns:\n",
        "        if np.random.rand() > 0.5:\n",
        "            # 최대값보다 큰 이상치 생성\n",
        "            outlier_sample[col] = df_scaled[col].max() + np.random.rand() * 3\n",
        "        else:\n",
        "            # 최소값보다 작은 이상치 생성\n",
        "            outlier_sample[col] = df_scaled[col].min() - np.random.rand() * 3\n",
        "    outliers.append(outlier_sample)\n",
        "\n",
        "outliers_df = pd.concat(outliers, ignore_index=True)\n",
        "\n",
        "# 이상치의 타겟 값 랜덤 할당\n",
        "outliers_df['target'] = np.random.choice(y, num_outliers)\n",
        "\n",
        "# 원본 데이터에 이상치 추가\n",
        "df_extended = pd.concat([df, outliers_df], ignore_index=True)\n",
        "\n",
        "# 이상치 추가 후 다시 정규화\n",
        "X_extended_scaled = scaler.fit_transform(df_extended.drop(columns=['target']))\n",
        "df_extended_scaled = pd.DataFrame(X_extended_scaled, columns=data.feature_names)\n",
        "\n",
        "# 새로운 X, y 정의\n",
        "X = df_extended_scaled.values  # NumPy 배열로 변환\n",
        "y = df_extended['target'].values  # 타겟 값 추출\n",
        "\n",
        "# 기존 이상치 식별\n",
        "df_zscores = df_scaled.apply(zscore)  # Z-score 계산\n",
        "is_existing_outlier = (df_zscores.abs() > 3).any(axis=1)  # 절대값이 3 이상인 경우 이상치로 판단\n",
        "\n",
        "total_existing_outliers = is_existing_outlier.sum()  # 기존 이상치 수\n",
        "total_outliers = total_existing_outliers + num_outliers  # 총 이상치 수\n",
        "total_samples = len(df_extended)  # 전체 샘플 수\n",
        "total_outlier_percentage = (total_outliers / total_samples) * 100  # 이상치 비율 계산\n",
        "\n",
        "# 데이터 확인\n",
        "print(f\"확장된 데이터셋 크기: {X.shape}\")\n",
        "print(f\"확장된 타겟 크기: {y.shape}\")\n",
        "print(f\"이상치가 포함된 데이터셋의 총 샘플 수: {len(X)}\")\n",
        "print(f\"전체 이상치 비율: {total_outlier_percentage:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3gtvpWugbVz",
        "outputId": "ebf8161d-9acc-476d-e802-99742d1f28c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[88.64, 88.08, 89.62, 89.48, 89.34, 87.66]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from itertools import combinations\n",
        "\n",
        "results = []\n",
        "\n",
        "# 데이터 전처리 및 분할\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=40, stratify=y )\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# OvO 방식을 사용하여 각 SVM 유형에 대해 학습 및 평가\n",
        "for svm_type in ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']:\n",
        "    models_ovo = train_ovo(X_train, y_train, C=1.0, svm_type=svm_type)\n",
        "    y_pred_ovo = predict_ovo(X_test, models_ovo)\n",
        "\n",
        "    # y_pred_ovo 변수를 정의한 후에 데이터 타입 확인 및 변환\n",
        "    if y_test.dtype != np.int64:\n",
        "        y_test = y_test.astype(int)\n",
        "\n",
        "    if y_pred_ovo.dtype != np.int64:\n",
        "        y_pred_ovo = y_pred_ovo.astype(int)\n",
        "\n",
        "    accuracy_ovo = accuracy_score(y_test, y_pred_ovo)\n",
        "    results.append((svm_type, round(accuracy_ovo * 100, 2)))\n",
        "\n",
        "# 결과를 DataFrame으로 변환 및 출력\n",
        "results_df = pd.DataFrame(results, columns=['SVM Type', 'Accuracy'])\n",
        "print(results_df['Accuracy'].tolist())\n",
        "[87.66, 86.68, 88.5, 87.24, 88.92, 88.78]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Dr9Mx_CwyUc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "accuracy_data = {\n",
        "    '3.06%' : [25.49, 25.49, 78.43, 78.04, 78.82, 78.43],\n",
        "    '5.07%' : [73.56, 77.01, 73.95, 73.56, 73.18, 72.80], #해야함\n",
        "    '10.04%' : [70.18, 72.36, 70.91, 70.91, 65.82, 69.45] #해야함\n",
        "}\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Vehicle data (SMO)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZtNBkb7gbVz",
        "outputId": "b24756f1-2968-429e-ad9e-a8f5e90fd307"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                         3.06%  5.07%  10.04%\n",
            "Vehicle data (L-BFGS-B)                      \n",
            "L1                       76.86  73.56   70.18\n",
            "L2                       78.43  77.01   72.36\n",
            "Fair                     77.65  73.95   70.91\n",
            "Cauchy                   76.86  73.56   70.91\n",
            "Welsch                   72.55  73.18   65.82\n",
            "Geman-McClure            74.51  72.80   69.45\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "accuracy_data = {\n",
        "    '3.06%' : [76.86, 78.43, 77.65, 76.86, 72.55, 74.51],\n",
        "    '5.07%' : [73.56, 77.01, 73.95, 73.56, 73.18, 72.80],\n",
        "    '10.04%' : [70.18, 72.36, 70.91, 70.91, 65.82, 69.45]\n",
        "}\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Vehicle data (L-BFGS-B)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xg6AHmyGwRR3",
        "outputId": "4b423b7c-8a60-478e-8f7a-42128c0e3c55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   3.06%  5.07%  10.04%\n",
            "Vehicle data (GA)                      \n",
            "L1                 50.98  49.04   51.27\n",
            "L2                 47.84  55.56   42.91\n",
            "Fair               56.08  51.72   45.09\n",
            "Cauchy             47.06  51.34   53.45\n",
            "Welsch             46.67  49.04   51.27\n",
            "Geman-McClure      61.57  56.32   45.09\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "accuracy_data = {\n",
        "    '3.06%' : [50.98, 47.84, 56.08, 47.06, 46.67, 61.57],\n",
        "    '5.07%' : [49.04, 55.56, 51.72, 51.34, 49.04, 56.32],\n",
        "    '10.04%' : [51.27, 42.91, 45.09, 53.45, 51.27, 45.09]\n",
        "}\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Vehicle data (GA)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cp_9j6JpwyQT",
        "outputId": "94a47cd0-8944-47cf-ddbb-23a53722b26e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                    3.06%  5.07%  10.04%\n",
            "Vehicle data (PSO)                      \n",
            "L1                  71.37  67.82   63.64\n",
            "L2                  69.02  65.52   65.09\n",
            "Fair                70.20  67.82   63.64\n",
            "Cauchy              72.55  66.28   67.64\n",
            "Welsch              71.37  68.97   66.91\n",
            "Geman-McClure       70.59  73.95   65.82\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "accuracy_data = {\n",
        "    '3.06%' : [71.37, 69.02, 70.2, 72.55, 71.37, 70.59],\n",
        "    '5.07%' : [67.82, 65.52, 67.82, 66.28, 68.97, 73.95],\n",
        "    '10.04%' : [63.64, 65.09, 63.64, 67.64, 66.91, 65.82]\n",
        "}\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Vehicle data (PSO)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBKawSBxwyLt",
        "outputId": "264b318f-fcfc-4765-83c4-992fd10f3de4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                    3.06%  5.07%  10.04%\n",
            "Vehicle data (ACO)                      \n",
            "L1                  76.86  73.56   68.73\n",
            "L2                  76.08  74.71   70.18\n",
            "Fair                75.69  72.80   69.45\n",
            "Cauchy              77.65  71.26   70.55\n",
            "Welsch              74.90  66.28   69.09\n",
            "Geman-McClure       74.12  69.73   69.82\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "accuracy_data = {\n",
        "    '3.06%' : [76.86, 76.08, 75.69, 77.65, 74.90, 74.12],\n",
        "    '5.07%' : [73.56, 74.71, 72.8, 71.26, 66.28, 69.73],\n",
        "    '10.04%' : [68.73, 70.18, 69.45, 70.55, 69.09, 69.82]\n",
        "}\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Vehicle data (ACO)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tvK_QY8owyC7",
        "outputId": "a258e7cd-3461-4094-b1f4-ce7158a5903f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   3.06%  5.07%  10.04%\n",
            "Vehicle data (HS)                      \n",
            "L1                 64.31  50.57   41.45\n",
            "L2                 49.41  41.00   43.27\n",
            "Fair               47.06  55.56   42.18\n",
            "Cauchy             59.61  63.60   53.82\n",
            "Welsch             64.71  59.00   51.64\n",
            "Geman-McClure      64.31  62.45   56.00\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "accuracy_data = {\n",
        "    '3.06%' : [64.31, 49.41, 47.06, 59.61, 64.71, 64.31],\n",
        "    '5.07%' : [50.57, 41.00, 55.56, 63.60, 59.00, 62.45],\n",
        "    '10.04%' : [41.45, 43.27, 42.18, 53.82, 51.64, 56.00]\n",
        "}\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Vehicle data (HS)'\n",
        "# 데이터프레임 출력\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lzo1BW9Q7BHi"
      },
      "source": [
        "#**OvR with various outliers**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nej9DwNY7MDY"
      },
      "source": [
        "###iris data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "goD2hIQQ7MDZ",
        "outputId": "0512a454-d73e-4a24-9625-770024954594"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "기존 이상치 수: 1\n",
            "기존 이상치 비율: 0.67%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.stats import zscore\n",
        "\n",
        "# 데이터셋 로딩\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "df = pd.DataFrame(X, columns=data.feature_names)\n",
        "df['target'] = y  # 타겟 변수 추가\n",
        "\n",
        "# 데이터 정규화\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "df_scaled = pd.DataFrame(X_scaled, columns=data.feature_names)\n",
        "\n",
        "# 기존 이상치 식별\n",
        "df_zscores = df_scaled.apply(zscore)  # Z-score 계산\n",
        "is_existing_outlier = (df_zscores.abs() > 3).any(axis=1)  # 절대값이 3 이상인 경우 이상치로 판단\n",
        "\n",
        "# 기존 이상치 수 계산\n",
        "total_existing_outliers = is_existing_outlier.sum()\n",
        "\n",
        "# 기존 이상치 비율 계산\n",
        "total_samples = len(df)\n",
        "existing_outlier_percentage = (total_existing_outliers / total_samples) * 100\n",
        "\n",
        "print(f\"기존 이상치 수: {total_existing_outliers}\")\n",
        "print(f\"기존 이상치 비율: {existing_outlier_percentage:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m32_LkFN7MDZ",
        "outputId": "ffab61c5-725e-4919-a8c5-a15a2de2ad04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "확장된 데이터셋 크기: (154, 4)\n",
            "확장된 타겟 크기: (154,)\n",
            "이상치가 포함된 데이터셋의 총 샘플 수: 154\n",
            "전체 이상치 비율: 3.25%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import load_iris\n",
        "from scipy.stats import zscore\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# 데이터셋 로딩\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "df = pd.DataFrame(X, columns=iris.feature_names)\n",
        "df['target'] = y  # 타겟 변수 추가\n",
        "\n",
        "# 데이터 정규화\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "df_scaled = pd.DataFrame(X_scaled, columns=iris.feature_names)\n",
        "\n",
        "# 이상치 생성\n",
        "num_outliers = int(0.03 * len(df))  # 이상치를 설정\n",
        "outliers = []\n",
        "\n",
        "for _ in range(num_outliers):\n",
        "    outlier_sample = df_scaled.sample(n=1).copy()  # 임의의 샘플 선택\n",
        "    for col in df_scaled.columns:\n",
        "        if np.random.rand() > 0.5:\n",
        "            # 최대값보다 큰 이상치 생성\n",
        "            outlier_sample[col] = df_scaled[col].max() + np.random.rand() * 3\n",
        "        else:\n",
        "            # 최소값보다 작은 이상치 생성\n",
        "            outlier_sample[col] = df_scaled[col].min() - np.random.rand() * 3\n",
        "    outliers.append(outlier_sample)\n",
        "\n",
        "outliers_df = pd.concat(outliers, ignore_index=True)\n",
        "\n",
        "# 이상치의 타겟 값 랜덤 할당\n",
        "outliers_df['target'] = np.random.choice(y, num_outliers)\n",
        "\n",
        "# 원본 데이터에 이상치 추가\n",
        "df_extended = pd.concat([df, outliers_df], ignore_index=True)\n",
        "\n",
        "# 이상치 추가 후 다시 정규화\n",
        "X_extended_scaled = scaler.fit_transform(df_extended.drop(columns=['target']))\n",
        "df_extended_scaled = pd.DataFrame(X_extended_scaled, columns=iris.feature_names)\n",
        "\n",
        "# 새로운 X, y 정의\n",
        "X = df_extended_scaled.values  # NumPy 배열로 변환\n",
        "y = df_extended['target'].values  # 타겟 값 추출\n",
        "\n",
        "# 기존 이상치 식별\n",
        "df_zscores = df_scaled.apply(zscore)  # Z-score 계산\n",
        "is_existing_outlier = (df_zscores.abs() > 3).any(axis=1)  # 절대값이 3 이상인 경우 이상치로 판단\n",
        "\n",
        "total_existing_outliers = is_existing_outlier.sum()  # 기존 이상치 수\n",
        "total_outliers = total_existing_outliers + num_outliers  # 총 이상치 수\n",
        "total_samples = len(df_extended)  # 전체 샘플 수\n",
        "total_outlier_percentage = (total_outliers / total_samples) * 100  # 이상치 비율 계산\n",
        "\n",
        "# 데이터 확인\n",
        "print(f\"확장된 데이터셋 크기: {X.shape}\")\n",
        "print(f\"확장된 타겟 크기: {y.shape}\")\n",
        "print(f\"이상치가 포함된 데이터셋의 총 샘플 수: {len(X)}\")\n",
        "print(f\"전체 이상치 비율: {total_outlier_percentage:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eGCaXs8G7MDZ",
        "outputId": "d0136c44-97c8-4cce-c832-892d9a1a1f73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "확장된 데이터셋 크기: (157, 4)\n",
            "확장된 타겟 크기: (157,)\n",
            "이상치가 포함된 데이터셋의 총 샘플 수: 157\n",
            "전체 이상치 비율: 5.10%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import load_iris\n",
        "from scipy.stats import zscore\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# 데이터셋 로딩\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "df = pd.DataFrame(X, columns=iris.feature_names)\n",
        "df['target'] = y  # 타겟 변수 추가\n",
        "\n",
        "# 데이터 정규화\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "df_scaled = pd.DataFrame(X_scaled, columns=iris.feature_names)\n",
        "\n",
        "# 이상치 생성\n",
        "num_outliers = int(0.05 * len(df))  # 이상치를 설정\n",
        "outliers = []\n",
        "\n",
        "for _ in range(num_outliers):\n",
        "    outlier_sample = df_scaled.sample(n=1).copy()  # 임의의 샘플 선택\n",
        "    for col in df_scaled.columns:\n",
        "        if np.random.rand() > 0.5:\n",
        "            # 최대값보다 큰 이상치 생성\n",
        "            outlier_sample[col] = df_scaled[col].max() + np.random.rand() * 3\n",
        "        else:\n",
        "            # 최소값보다 작은 이상치 생성\n",
        "            outlier_sample[col] = df_scaled[col].min() - np.random.rand() * 3\n",
        "    outliers.append(outlier_sample)\n",
        "\n",
        "outliers_df = pd.concat(outliers, ignore_index=True)\n",
        "\n",
        "# 이상치의 타겟 값 랜덤 할당\n",
        "outliers_df['target'] = np.random.choice(y, num_outliers)\n",
        "\n",
        "# 원본 데이터에 이상치 추가\n",
        "df_extended = pd.concat([df, outliers_df], ignore_index=True)\n",
        "\n",
        "# 이상치 추가 후 다시 정규화\n",
        "X_extended_scaled = scaler.fit_transform(df_extended.drop(columns=['target']))\n",
        "df_extended_scaled = pd.DataFrame(X_extended_scaled, columns=iris.feature_names)\n",
        "\n",
        "# 새로운 X, y 정의\n",
        "X = df_extended_scaled.values  # NumPy 배열로 변환\n",
        "y = df_extended['target'].values  # 타겟 값 추출\n",
        "\n",
        "# 기존 이상치 식별\n",
        "df_zscores = df_scaled.apply(zscore)  # Z-score 계산\n",
        "is_existing_outlier = (df_zscores.abs() > 3).any(axis=1)  # 절대값이 3 이상인 경우 이상치로 판단\n",
        "\n",
        "total_existing_outliers = is_existing_outlier.sum()  # 기존 이상치 수\n",
        "total_outliers = total_existing_outliers + num_outliers  # 총 이상치 수\n",
        "total_samples = len(df_extended)  # 전체 샘플 수\n",
        "total_outlier_percentage = (total_outliers / total_samples) * 100  # 이상치 비율 계산\n",
        "\n",
        "# 데이터 확인\n",
        "print(f\"확장된 데이터셋 크기: {X.shape}\")\n",
        "print(f\"확장된 타겟 크기: {y.shape}\")\n",
        "print(f\"이상치가 포함된 데이터셋의 총 샘플 수: {len(X)}\")\n",
        "print(f\"전체 이상치 비율: {total_outlier_percentage:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTMSvFGS7MDa",
        "outputId": "2f3cdb7b-daef-4b6d-f6da-6f6000e35535"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "확장된 데이터셋 크기: (156, 4)\n",
            "확장된 타겟 크기: (156,)\n",
            "이상치가 포함된 데이터셋의 총 샘플 수: 156\n",
            "전체 이상치 비율: 4.49%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import load_iris\n",
        "from scipy.stats import zscore\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# 데이터셋 로딩\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "df = pd.DataFrame(X, columns=iris.feature_names)\n",
        "df['target'] = y  # 타겟 변수 추가\n",
        "\n",
        "# 데이터 정규화\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "df_scaled = pd.DataFrame(X_scaled, columns=iris.feature_names)\n",
        "\n",
        "# 이상치 생성\n",
        "num_outliers = int(0.04 * len(df))  # 이상치를 설정\n",
        "outliers = []\n",
        "\n",
        "for _ in range(num_outliers):\n",
        "    outlier_sample = df_scaled.sample(n=1).copy()  # 임의의 샘플 선택\n",
        "    for col in df_scaled.columns:\n",
        "        if np.random.rand() > 0.5:\n",
        "            # 최대값보다 큰 이상치 생성\n",
        "            outlier_sample[col] = df_scaled[col].max() + np.random.rand() * 3\n",
        "        else:\n",
        "            # 최소값보다 작은 이상치 생성\n",
        "            outlier_sample[col] = df_scaled[col].min() - np.random.rand() * 3\n",
        "    outliers.append(outlier_sample)\n",
        "\n",
        "outliers_df = pd.concat(outliers, ignore_index=True)\n",
        "\n",
        "# 이상치의 타겟 값 랜덤 할당\n",
        "outliers_df['target'] = np.random.choice(y, num_outliers)\n",
        "\n",
        "# 원본 데이터에 이상치 추가\n",
        "df_extended = pd.concat([df, outliers_df], ignore_index=True)\n",
        "\n",
        "# 이상치 추가 후 다시 정규화\n",
        "X_extended_scaled = scaler.fit_transform(df_extended.drop(columns=['target']))\n",
        "df_extended_scaled = pd.DataFrame(X_extended_scaled, columns=iris.feature_names)\n",
        "\n",
        "# 새로운 X, y 정의\n",
        "X = df_extended_scaled.values  # NumPy 배열로 변환\n",
        "y = df_extended['target'].values  # 타겟 값 추출\n",
        "\n",
        "# 기존 이상치 식별\n",
        "df_zscores = df_scaled.apply(zscore)  # Z-score 계산\n",
        "is_existing_outlier = (df_zscores.abs() > 3).any(axis=1)  # 절대값이 3 이상인 경우 이상치로 판단\n",
        "\n",
        "total_existing_outliers = is_existing_outlier.sum()  # 기존 이상치 수\n",
        "total_outliers = total_existing_outliers + num_outliers  # 총 이상치 수\n",
        "total_samples = len(df_extended)  # 전체 샘플 수\n",
        "total_outlier_percentage = (total_outliers / total_samples) * 100  # 이상치 비율 계산\n",
        "\n",
        "# 데이터 확인\n",
        "print(f\"확장된 데이터셋 크기: {X.shape}\")\n",
        "print(f\"확장된 타겟 크기: {y.shape}\")\n",
        "print(f\"이상치가 포함된 데이터셋의 총 샘플 수: {len(X)}\")\n",
        "print(f\"전체 이상치 비율: {total_outlier_percentage:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFbS0xHz7MDa",
        "outputId": "e86f005c-3fa0-487b-9865-a2f4849e30c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[95.56, 100.0, 95.56, 91.11, 84.44, 84.44]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from itertools import combinations\n",
        "\n",
        "results = []\n",
        "\n",
        "# 데이터 전처리 및 분할\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# OvR 방식을 사용하여 각 SVM 유형에 대해 학습 및 평가\n",
        "for svm_type in ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']:\n",
        "    models_ovr = train_ovr(X_train, y_train, C=1.0, svm_type=svm_type)\n",
        "    y_pred_ovr = predict_ovr(X_test, models_ovr)\n",
        "\n",
        "    # y_pred_ovr 변수를 정의한 후에 데이터 타입 확인 및 변환\n",
        "    if y_test.dtype != np.int64:\n",
        "        y_test = y_test.astype(int)\n",
        "\n",
        "    if y_pred_ovr.dtype != np.int64:\n",
        "        y_pred_ovr = y_pred_ovr.astype(int)\n",
        "\n",
        "    accuracy_ovr = accuracy_score(y_test, y_pred_ovr)\n",
        "    results.append((svm_type, round(accuracy_ovr * 100, 2)))\n",
        "\n",
        "# 결과를 DataFrame으로 변환 및 출력\n",
        "results_df = pd.DataFrame(results, columns=['SVM Type', 'Accuracy'])\n",
        "print(results_df['Accuracy'].tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbm4I4DG7MDa",
        "outputId": "b917b5d3-212a-4092-d31a-f75e71462d7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                      3.25%  5.10%  10.24%\n",
            "Iris data (L-BFGS-B)                      \n",
            "L1                    82.98  87.50    66.0\n",
            "L2                    91.49  93.75    64.0\n",
            "Fair                  89.36  91.67    64.0\n",
            "Cauchy                85.11  89.58    76.0\n",
            "Welsch                72.34  68.75    70.0\n",
            "Geman-McClure         78.72  68.75    68.0\n"
          ]
        }
      ],
      "source": [
        "#0.67% (L-BFGS-B,PSO,HS)\n",
        "[95.56, 100.0, 95.56, 91.11, 84.44, 84.44]\n",
        "[95.56, 100.0, 95.56, 91.11, 84.44, 84.44]\n",
        "[84.44, 88.89, 93.33, 86.67, 84.44, 84.44]\n",
        "\n",
        "#3.25%\n",
        "[82.98, 91.49, 89.36, 85.11, 72.34, 78.72]\n",
        "[82.98, 91.49, 89.36, 85.11, 74.47, 78.72]\n",
        "[91.49, 87.23, 95.74, 95.74, 91.49, 89.36]\n",
        "\n",
        "%4.49\n",
        "[65.96, 63.83, 65.96, 82.98, 63.83, 74.47]\n",
        "[65.96, 63.83, 65.96, 82.98, 68.09, 74.47]\n",
        "[63.83, 59.57, 78.72, 68.09, 76.6, 78.72]\n",
        "\n",
        "\n",
        "#3.25%\n",
        "[82.98, 91.49, 89.36, 85.11, 72.34, 78.72]\n",
        "[74.47, 80.85, 76.60, 76.60, 74.47, 74.47]\n",
        "[82.98, 91.49, 89.36, 85.11, 74.47, 78.72]\n",
        "[82.98, 91.49, 87.23, 85.11, 74.47, 78.72]\n",
        "[91.49, 87.23, 95.74, 95.74, 91.49, 89.36]\n",
        "%4.49\n",
        "[65.96, 63.83, 65.96, 82.98, 63.83, 74.47]\n",
        "[63.83, 68.09, 63.83, 76.6, 72.34, 72.34]\n",
        "[65.96, 63.83, 65.96, 82.98, 68.09, 74.47]\n",
        "[65.96, 61.7, 65.96, 82.98, 63.83, 72.34]\n",
        "[63.83, 59.57, 78.72, 68.09, 76.6, 78.72]\n",
        "#3.87%\n",
        "[82.98, 85.11, 80.85, 87.23, 68.09, 74.47]\n",
        "[82.98, 74.47, 70.21, 76.6, 76.6, 76.6]\n",
        "[82.98, 85.11, 80.85, 87.23, 68.09, 74.47]\n",
        "[82.98, 85.11, 80.85, 87.23, 68.09, 74.47]\n",
        "[74.47, 74.47, 76.6, 87.23, 61.7, 70.21]\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "accuracy_data = {\n",
        "    '3.25%' : [82.98, 91.49, 89.36, 85.11, 72.34, 78.72],\n",
        "    '5.10%' : [87.50, 93.75, 91.67, 89.58, 68.75, 68.75],\n",
        "    '10.24%' : [66.0, 64.0, 64.0, 76.0, 70.0, 68.0]\n",
        "}\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Iris data (L-BFGS-B)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijmGdE8M7MDa",
        "outputId": "9b2559c1-37fb-4030-b76c-0513f3cc4d14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                3.25%  5.10%  10.24%\n",
            "Iris data (GA)                      \n",
            "L1              74.47  70.83    64.0\n",
            "L2              80.85  93.75    76.0\n",
            "Fair            76.60  79.17    74.0\n",
            "Cauchy          76.60  72.92    64.0\n",
            "Welsch          74.47  70.83    64.0\n",
            "Geman-McClure   74.47  64.58    64.0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "outlier = ['3%', '5%', '10%']\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "\n",
        "accuracy_data = {\n",
        "    '3.25%' : [74.47, 80.85, 76.60, 76.60, 74.47, 74.47],\n",
        "    '5.10%' : [70.83, 93.75, 79.17, 72.92, 70.83, 64.58],\n",
        "    '10.24%' : [64.0, 76.0, 74.0, 64.0, 64.0, 64.0]\n",
        "}\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Iris data (GA)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgAXRK0X7MDa",
        "outputId": "24b0fabf-08b9-4630-e764-2dac2a90c866"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                 3.25%  5.10%  10.24%\n",
            "Iris data (SMO)                      \n",
            "L1               31.91  29.17    34.0\n",
            "L2               31.91  29.17    34.0\n",
            "Fair             87.23  89.58    70.0\n",
            "Cauchy           87.23  89.58    70.0\n",
            "Welsch           87.23  89.58    70.0\n",
            "Geman-McClure    87.23  89.58    70.0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "outlier = ['3%', '5%', '10%']\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "\n",
        "accuracy_data = {\n",
        "    '3.25%' : [31.91,31.91, 87.23, 87.23, 87.23, 87.23],\n",
        "    '5.10%' : [29.17, 29.17, 89.58, 89.58, 89.58, 89.58],\n",
        "    '10.24%' : [34.0, 34.0, 70.0, 70.0, 70.0, 70.0]\n",
        "}\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Iris data (SMO)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-oN83mf7MDa",
        "outputId": "c6a19c69-8bb2-4a10-a552-0c3f498c5387"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                 3.25%  5.10%  10.24%\n",
            "Iris data (PSO)                      \n",
            "L1               82.98  85.42    66.0\n",
            "L2               91.49  93.75    64.0\n",
            "Fair             89.36  91.67    64.0\n",
            "Cauchy           85.11  89.58    76.0\n",
            "Welsch           74.47  60.42    64.0\n",
            "Geman-McClure    78.72  68.75    66.0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "outlier = ['3%', '5%', '10%']\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "\n",
        "accuracy_data = {\n",
        "    '3.25%' : [82.98, 91.49, 89.36, 85.11, 74.47, 78.72],\n",
        "    '5.10%' : [85.42, 93.75, 91.67, 89.58, 60.42, 68.75],\n",
        "    '10.24%' : [66.0, 64.0, 64.0, 76.0, 64.0, 66.0]\n",
        "}\n",
        "\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Iris data (PSO)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6tC1EBl7MDa",
        "outputId": "674b8190-79e7-46fd-d649-277e5ff1bae5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                 3.25%  5.10%  10.24%\n",
            "Iris data (ACO)                      \n",
            "L1               82.98  87.50    66.0\n",
            "L2               91.49  93.75    64.0\n",
            "Fair             87.23  91.67    64.0\n",
            "Cauchy           85.11  89.58    80.0\n",
            "Welsch           74.47  66.67    64.0\n",
            "Geman-McClure    78.72  72.92    66.0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "outlier = ['3%', '5%', '10%']\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "\n",
        "accuracy_data = {\n",
        "    '3.25%' : [82.98, 91.49, 87.23, 85.11, 74.47, 78.72],\n",
        "    '5.10%' : [87.50, 93.75, 91.67, 89.58, 66.67, 72.92],\n",
        "    '10.24%' : [66.0, 64.0, 64.0, 80.0, 64.0, 66.0]\n",
        "}\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Iris data (ACO)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1N5Fb6YC7MDa",
        "outputId": "6c481513-c596-4ffb-d77b-705df732e32f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                3.25%  5.10%  10.24%\n",
            "Iris data (HS)                      \n",
            "L1              91.49  78.72    64.0\n",
            "L2              87.23  85.11    66.0\n",
            "Fair            95.74  80.85    64.0\n",
            "Cauchy          95.74  80.85    68.0\n",
            "Welsch          91.49  74.47    64.0\n",
            "Geman-McClure   89.36  65.96    64.0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "accuracy_data = {\n",
        "    '3.25%' : [91.49, 87.23, 95.74, 95.74, 91.49, 89.36],\n",
        "    '5.10%' : [78.72, 85.11, 80.85, 80.85, 74.47, 65.96],\n",
        "    '10.24%' : [64.0, 66.0, 64.0, 68.0, 64.0, 64.0]\n",
        "}\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Iris data (HS)'\n",
        "\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md1-hHosjg5d"
      },
      "source": [
        "###segment data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kj2xTmcpjg5f",
        "outputId": "0c4e9bc5-ab8a-4e1d-b5ca-0ed472b2954b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "확장된 데이터셋 크기: (2374, 19)\n",
            "확장된 타겟 크기: (2374,)\n",
            "이상치가 포함된 데이터셋의 총 샘플 수: 2374\n",
            "전체 이상치 비율: 10.07%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from scipy.stats import zscore\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# 데이터셋 로딩\n",
        "data = fetch_openml(name='segment', version=1)\n",
        "X = data.data\n",
        "# Use LabelEncoder to convert categorical target to numerical\n",
        "encoder = LabelEncoder()\n",
        "y = encoder.fit_transform(data.target)\n",
        "df = pd.DataFrame(X, columns=data.feature_names)\n",
        "df['target'] = y  # 타겟 변수 추가\n",
        "\n",
        "# 데이터 정규화\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "df_scaled = pd.DataFrame(X_scaled, columns=data.feature_names)\n",
        "\n",
        "# 이상치 생성\n",
        "num_outliers = int(0.028 * len(df))  # 이상치를 설정\n",
        "outliers = []\n",
        "\n",
        "for _ in range(num_outliers):\n",
        "    outlier_sample = df_scaled.sample(n=1).copy()  # 임의의 샘플 선택\n",
        "    for col in df_scaled.columns:\n",
        "        if np.random.rand() > 0.5:\n",
        "            # 최대값보다 큰 이상치 생성\n",
        "            outlier_sample[col] = df_scaled[col].max() + np.random.rand() * 3\n",
        "        else:\n",
        "            # 최소값보다 작은 이상치 생성\n",
        "            outlier_sample[col] = df_scaled[col].min() - np.random.rand() * 3\n",
        "    outliers.append(outlier_sample)\n",
        "\n",
        "outliers_df = pd.concat(outliers, ignore_index=True)\n",
        "\n",
        "# 이상치의 타겟 값 랜덤 할당\n",
        "outliers_df['target'] = np.random.choice(y, num_outliers)\n",
        "\n",
        "# 원본 데이터에 이상치 추가\n",
        "df_extended = pd.concat([df, outliers_df], ignore_index=True)\n",
        "\n",
        "# 이상치 추가 후 다시 정규화\n",
        "X_extended_scaled = scaler.fit_transform(df_extended.drop(columns=['target']))\n",
        "df_extended_scaled = pd.DataFrame(X_extended_scaled, columns=data.feature_names)\n",
        "\n",
        "# 새로운 X, y 정의\n",
        "X = df_extended_scaled.values  # NumPy 배열로 변환\n",
        "y = df_extended['target'].values  # 타겟 값 추출\n",
        "\n",
        "# 기존 이상치 식별\n",
        "df_zscores = df_scaled.apply(zscore)  # Z-score 계산\n",
        "is_existing_outlier = (df_zscores.abs() > 3).any(axis=1)  # 절대값이 3 이상인 경우 이상치로 판단\n",
        "\n",
        "total_existing_outliers = is_existing_outlier.sum()  # 기존 이상치 수\n",
        "total_outliers = total_existing_outliers + num_outliers  # 총 이상치 수\n",
        "total_samples = len(df_extended)  # 전체 샘플 수\n",
        "total_outlier_percentage = (total_outliers / total_samples) * 100  # 이상치 비율 계산\n",
        "\n",
        "# 데이터 확인\n",
        "print(f\"확장된 데이터셋 크기: {X.shape}\")\n",
        "print(f\"확장된 타겟 크기: {y.shape}\")\n",
        "print(f\"이상치가 포함된 데이터셋의 총 샘플 수: {len(X)}\")\n",
        "print(f\"전체 이상치 비율: {total_outlier_percentage:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qgOOCoyujg5e",
        "outputId": "5ec81a92-8074-4e44-e2d1-e884593a938b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "기존 이상치 수: 175\n",
            "기존 이상치 비율: 7.58%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from scipy.stats import zscore\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# 데이터셋 로딩\n",
        "data = fetch_openml(name='segment', version=1)\n",
        "X = data.data\n",
        "# Use LabelEncoder to convert categorical target to numerical\n",
        "encoder = LabelEncoder()\n",
        "y = encoder.fit_transform(data.target)\n",
        "df = pd.DataFrame(X, columns=data.feature_names)\n",
        "df['target'] = y  # 타겟 변수 추가\n",
        "\n",
        "# 데이터 정규화\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "df_scaled = pd.DataFrame(X_scaled, columns=data.feature_names)\n",
        "\n",
        "# 기존 이상치 식별\n",
        "df_zscores = df_scaled.apply(zscore)  # Z-score 계산\n",
        "is_existing_outlier = (df_zscores.abs() > 3).any(axis=1)  # 절대값이 3 이상인 경우 이상치로 판단\n",
        "\n",
        "# 기존 이상치 수 계산\n",
        "total_existing_outliers = is_existing_outlier.sum()\n",
        "\n",
        "# 기존 이상치 비율 계산\n",
        "total_samples = len(df)\n",
        "existing_outlier_percentage = (total_existing_outliers / total_samples) * 100\n",
        "\n",
        "print(f\"기존 이상치 수: {total_existing_outliers}\")\n",
        "print(f\"기존 이상치 비율: {existing_outlier_percentage:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "piW25vZA1Tv1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from scipy.stats import zscore\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# 데이터셋 로딩\n",
        "data = fetch_openml(name='segment', version=1)\n",
        "X = data.data\n",
        "# Use LabelEncoder to convert categorical target to numerical\n",
        "encoder = LabelEncoder()\n",
        "y = encoder.fit_transform(data.target)\n",
        "df = pd.DataFrame(X, columns=data.feature_names)\n",
        "df['target'] = y  # 타겟 변수 추가\n",
        "\n",
        "# 데이터 정규화\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "df_scaled = pd.DataFrame(X_scaled, columns=data.feature_names)\n",
        "\n",
        "# 기존 이상치 식별\n",
        "df_zscores = df_scaled.apply(zscore)  # Z-score 계산\n",
        "is_existing_outlier = (df_zscores.abs() > 4.5).any(axis=1)  # 절대값이 3 이상인 경우 이상치로 판단\n",
        "\n",
        "df_cleaned = df.loc[~is_existing_outlier].reset_index(drop=True)\n",
        "X_cleaned = df_cleaned.drop(columns='target')\n",
        "y_cleaned = df_cleaned['target']\n",
        "\n",
        "# 새로운 X, y 정의\n",
        "X = X_cleaned.values  # NumPy 배열로 변환\n",
        "y = y_cleaned.values  # 타겟 값 추출"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hn2ORtZHjg5h",
        "outputId": "74aef7ae-9acd-4ff0-cf6b-b3ed1fea5f0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[85.71, 88.54, 87.05, 84.67, 64.43, 81.7]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from itertools import combinations\n",
        "\n",
        "results = []\n",
        "\n",
        "# 데이터 전처리 및 분할\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# OvR 방식을 사용하여 각 SVM 유형에 대해 학습 및 평가\n",
        "for svm_type in ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']:\n",
        "    models_ovr = train_ovr(X_train, y_train, C=1.0, svm_type=svm_type)\n",
        "    y_pred_ovr = predict_ovr(X_test, models_ovr)\n",
        "\n",
        "    # y_pred_ovr 변수를 정의한 후에 데이터 타입 확인 및 변환\n",
        "    if y_test.dtype != np.int64:\n",
        "        y_test = y_test.astype(int)\n",
        "\n",
        "    if y_pred_ovr.dtype != np.int64:\n",
        "        y_pred_ovr = y_pred_ovr.astype(int)\n",
        "\n",
        "    accuracy_ovr = accuracy_score(y_test, y_pred_ovr)\n",
        "    results.append((svm_type, round(accuracy_ovr * 100, 2)))\n",
        "\n",
        "# 결과를 DataFrame으로 변환 및 출력\n",
        "results_df = pd.DataFrame(results, columns=['SVM Type', 'Accuracy'])\n",
        "print(results_df['Accuracy'].tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4kpp39G1MrF",
        "outputId": "2cdb6d78-7bbd-40e4-a05a-0fb09053a6c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                          3 sigma  4.5 sigma  full original data\n",
            "Segment data (L-BFGS-B)                                         \n",
            "L1                          93.14      92.41               90.91\n",
            "L2                          92.04      91.67               90.76\n",
            "Fair                        91.89      91.22               90.19\n",
            "Cauchy                      93.60      92.41               90.76\n",
            "Welsch                      89.24      85.71               89.47\n",
            "Geman-McClure               91.58      90.33               89.47\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "accuracy_data = {\n",
        "    ' 3 sigma' : [93.14, 92.04, 91.89, 93.6, 89.24, 91.58],\n",
        "    '4.5 sigma' : [92.41, 91.67, 91.22, 92.41, 85.71, 90.33],\n",
        "    'full original data' : [90.91, 90.76, 90.19, 90.76, 89.47, 89.47]\n",
        "}\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Segment data (L-BFGS-B)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WW3J4iQ21czR",
        "outputId": "cd3e4758-416e-43b3-b32f-f91de4b15e3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                    3 sigma  4.5 sigma  full original data\n",
            "Segment data (GA)                                         \n",
            "L1                    72.39      69.05               69.99\n",
            "L2                    66.15      58.63               70.56\n",
            "Fair                  69.58      75.45               76.19\n",
            "Cauchy                80.34      61.90               57.14\n",
            "Welsch                54.76      41.37               36.36\n",
            "Geman-McClure         43.84      55.51               52.67\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "accuracy_data = {\n",
        "    ' 3 sigma' : [72.39, 66.15, 69.58, 80.34, 54.76, 43.84],\n",
        "    '4.5 sigma' : [69.05, 58.63, 75.45, 61.9, 41.37, 55.51],\n",
        "    'full original data' : [69.99, 70.56, 76.19, 57.14, 36.36, 52.67]\n",
        "}\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Segment data (GA)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5AM_GGx91c3A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abf3b259-219b-4c89-f0d7-996036943403"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                     3 sigma  4.5 sigma  full original data\n",
            "Segment data (SMO)                                         \n",
            "L1                     17.00      14.58               12.41\n",
            "L2                     17.00      14.58               12.41\n",
            "Fair                   93.76      92.56               91.05\n",
            "Cauchy                 94.07      92.41               90.62\n",
            "Welsch                 93.76      92.71               90.76\n",
            "Geman-McClure          93.92      92.56               91.05\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "accuracy_data = {\n",
        "    ' 3 sigma' : [17.0, 17.0, 93.76, 94.07, 93.76, 93.92],\n",
        "    '4.5 sigma' : [14.58, 14.58, 92.56, 92.41, 92.71, 92.56],\n",
        "    'full original data' : [12.41, 12.41, 91.05, 90.62, 90.76, 91.05]\n",
        "}\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Segment data (SMO)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vo6LfPeW1c6F",
        "outputId": "4d923f0d-d6dc-46a7-9f9a-f817508b5004"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                     3 sigma  4.5 sigma  full original data\n",
            "Segment data (PSO)                                         \n",
            "L1                     88.92      85.71               85.71\n",
            "L2                     86.74      88.54               87.88\n",
            "Fair                   87.99      87.05               87.59\n",
            "Cauchy                 87.83      84.67               83.12\n",
            "Welsch                 80.97      64.43               71.57\n",
            "Geman-McClure          85.80      81.70               83.84\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "accuracy_data = {\n",
        "    ' 3 sigma' : [88.92, 86.74, 87.99, 87.83, 80.97, 85.8],\n",
        "    '4.5 sigma' : [85.71, 88.54, 87.05, 84.67, 64.43, 81.7],\n",
        "    'full original data' : [85.71, 87.88, 87.59, 83.12, 71.57, 83.84]\n",
        "}\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Segment data (PSO)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMbxwknM1c8_",
        "outputId": "079c25f1-de12-411c-a72f-e1b6a6141892"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                     3 sigma  4.5 sigma  full original data\n",
            "Segment data (ACO)                                         \n",
            "L1                     88.92      85.71               89.75\n",
            "L2                     86.74      88.54               90.04\n",
            "Fair                   87.99      87.05               90.62\n",
            "Cauchy                 87.83      84.67               89.18\n",
            "Welsch                 80.97      64.43               89.03\n",
            "Geman-McClure          85.80      81.70               86.72\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "accuracy_data = {\n",
        "    ' 3 sigma' : [88.92, 86.74, 87.99, 87.83, 80.97, 85.8],\n",
        "    '4.5 sigma' : [85.71, 88.54, 87.05, 84.67, 64.43, 81.7],\n",
        "    'full original data' : [89.75, 90.04, 90.62, 89.18, 89.03, 86.72]\n",
        "}\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Segment data (ACO)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Bl4f3le1c_2",
        "outputId": "5fe56a28-4e17-43f5-a49d-5b033b2dfce4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                    3 sigma  4.5 sigma  full original data\n",
            "Segment data (HS)                                         \n",
            "L1                    68.95      65.33               68.25\n",
            "L2                    55.23      58.18               54.83\n",
            "Fair                  57.10      67.56               61.04\n",
            "Cauchy                56.47      61.61               55.56\n",
            "Welsch                56.94      60.71               55.12\n",
            "Geman-McClure         55.69      61.31               53.54\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "accuracy_data = {\n",
        "    ' 3 sigma' : [68.95, 55.23, 57.1, 56.47, 56.94, 55.69],\n",
        "    '4.5 sigma' : [65.33, 58.18, 67.56, 61.61, 60.71, 61.31],\n",
        "    'full original data' : [68.25, 54.83, 61.04, 55.56, 55.12, 53.54]\n",
        "}\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Segment data (HS)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLFQmBIajg5h",
        "outputId": "667afc08-0cee-4174-82da-63a5d8f5fa25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                         7.58%  10.07%\n",
            "Segment data (L-BFGS-B)               \n",
            "L1                       90.91   89.06\n",
            "L2                       90.76   89.76\n",
            "Fair                     90.19   89.48\n",
            "Cauchy                   90.76   90.32\n",
            "Welsch                   89.47   84.85\n",
            "Geman-McClure            89.47   88.22\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "accuracy_data = {\n",
        "    '7.58%' : [90.91, 90.76, 90.19, 90.76, 89.47, 89.47],\n",
        "    '10.07%' : [89.06, 89.76, 89.48, 90.32, 84.85, 88.22]\n",
        "}\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Segment data (L-BFGS-B)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffLYnM-9kR1o",
        "outputId": "d1253e81-dcde-445b-82e2-fd332ee6a819"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   7.58%  10.07%\n",
            "Segment data (GA)               \n",
            "L1                 69.99   75.60\n",
            "L2                 70.56   68.44\n",
            "Fair               76.19   66.06\n",
            "Cauchy             57.14   64.38\n",
            "Welsch             36.36   53.58\n",
            "Geman-McClure      52.67   67.32\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "accuracy_data = {\n",
        "    '7.58%' : [69.99, 70.56, 76.19, 57.14, 36.36, 52.67],\n",
        "    '10.07%' : [75.60, 68.44, 66.06, 64.38, 53.58, 67.32]\n",
        "}\n",
        "\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Segment data (GA)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QosEv8iXkTTx",
        "outputId": "53bbd8f2-b684-4ab0-88b6-63e4beb50f3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                    7.58%  10.07%\n",
            "Segment data (SMO)               \n",
            "L1                  12.41   14.87\n",
            "L2                  12.41   14.87\n",
            "Fair                91.05   91.02\n",
            "Cauchy              90.62   90.46\n",
            "Welsch              90.76   91.16\n",
            "Geman-McClure       91.05   90.74\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "accuracy_data = {\n",
        "    '7.58%' : [12.41, 12.41, 91.05, 90.62, 90.76, 91.05],\n",
        "    '10.07%' : [14.87, 14.87, 91.02, 90.46, 91.16, 90.74]\n",
        "}\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Segment data (SMO)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNMupjuskTxm",
        "outputId": "3e17ac34-08fd-4223-9323-30fa6be17344"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                    7.58%  10.07%\n",
            "Segment data (PSO)               \n",
            "L1                  85.71   82.75\n",
            "L2                  87.88   83.31\n",
            "Fair                87.59   85.41\n",
            "Cauchy              83.12   86.96\n",
            "Welsch              71.57   76.72\n",
            "Geman-McClure       83.84   74.19\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "accuracy_data = {\n",
        "    '7.58%' : [85.71, 87.88, 87.59, 83.12, 71.57, 83.84],\n",
        "    '10.07%' : [82.75, 83.31, 85.41, 86.96, 76.72, 74.19]\n",
        "}\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Segment data (PSO)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FifTK7GkUFz",
        "outputId": "ab71df5a-243a-4ab1-b3de-f58573fa35b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                    7.58%  10.07%\n",
            "Segment data (ACO)               \n",
            "L1                  89.75   88.36\n",
            "L2                  90.04   89.48\n",
            "Fair                90.62   88.50\n",
            "Cauchy              89.18   88.78\n",
            "Welsch              89.03   84.29\n",
            "Geman-McClure       86.72   86.82\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "accuracy_data = {\n",
        "    '7.58%' : [89.75, 90.04, 90.62, 89.18, 89.03, 86.72],\n",
        "    '10.07%' : [88.36, 89.48, 88.50, 88.78, 84.29, 86.82]\n",
        "}\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Segment data (ACO)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aulCqMJJkTox",
        "outputId": "c3c9c209-895b-4eb7-b733-9527f76f678b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   7.58%  10.07%\n",
            "Segment data (HS)               \n",
            "L1                 68.25   67.18\n",
            "L2                 54.83   50.91\n",
            "Fair               61.04   60.59\n",
            "Cauchy             55.56   68.86\n",
            "Welsch             55.12   63.25\n",
            "Geman-McClure      53.54   64.66\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "accuracy_data = {\n",
        "    '7.58%' : [68.25, 54.83, 61.04, 55.56, 55.12, 53.54],\n",
        "    '10.07%' : [67.18, 50.91, 60.59, 68.86, 63.25, 64.66]\n",
        "}\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Segment data (HS)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJhvImfn1ZJ8"
      },
      "source": [
        "### vehicle data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SjGg0iOh1ZJ9",
        "outputId": "be2a0065-5e0c-4db3-adb2-35b7fcb6e10d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "기존 이상치 수: 22\n",
            "기존 이상치 비율: 2.60%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from scipy.stats import zscore\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# 데이터셋 로딩\n",
        "data = fetch_openml(name='vehicle', version=1)\n",
        "X = data.data\n",
        "encoder = LabelEncoder()\n",
        "y = encoder.fit_transform(data.target)\n",
        "df = pd.DataFrame(X, columns=data.feature_names)\n",
        "df['target'] = y  # 타겟 변수 추가\n",
        "\n",
        "# 데이터 정규화\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "df_scaled = pd.DataFrame(X_scaled, columns=data.feature_names)\n",
        "\n",
        "# 기존 이상치 식별\n",
        "df_zscores = df_scaled.apply(zscore)  # Z-score 계산\n",
        "is_existing_outlier = (df_zscores.abs() > 3).any(axis=1)  # 절대값이 3 이상인 경우 이상치로 판단\n",
        "\n",
        "# 기존 이상치 수 계산\n",
        "total_existing_outliers = is_existing_outlier.sum()\n",
        "\n",
        "# 기존 이상치 비율 계산\n",
        "total_samples = len(df)\n",
        "existing_outlier_percentage = (total_existing_outliers / total_samples) * 100\n",
        "\n",
        "print(f\"기존 이상치 수: {total_existing_outliers}\")\n",
        "print(f\"기존 이상치 비율: {existing_outlier_percentage:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNTYqkpF1ZJ-",
        "outputId": "12ef8abd-8af0-4d47-98ca-5ac99f48b72e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "확장된 데이터셋 크기: (850, 18)\n",
            "확장된 타겟 크기: (850,)\n",
            "이상치가 포함된 데이터셋의 총 샘플 수: 850\n",
            "전체 이상치 비율: 3.06%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import load_iris\n",
        "from scipy.stats import zscore\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# 데이터셋 로딩\n",
        "data = fetch_openml(name='vehicle', version=1)\n",
        "X = data.data\n",
        "# Use LabelEncoder to convert categorical target to numerical\n",
        "encoder = LabelEncoder()\n",
        "y = encoder.fit_transform(data.target)\n",
        "df = pd.DataFrame(X, columns=data.feature_names)\n",
        "df['target'] = y  # 타겟 변수 추가\n",
        "\n",
        "\n",
        "# 데이터 정규화\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "df_scaled = pd.DataFrame(X_scaled, columns=data.feature_names)\n",
        "\n",
        "# 이상치 생성\n",
        "num_outliers = int(0.0048 * len(df))  # 이상치를 설정\n",
        "outliers = []\n",
        "\n",
        "for _ in range(num_outliers):\n",
        "    outlier_sample = df_scaled.sample(n=1).copy()  # 임의의 샘플 선택\n",
        "    for col in df_scaled.columns:\n",
        "        if np.random.rand() > 0.5:\n",
        "            # 최대값보다 큰 이상치 생성\n",
        "            outlier_sample[col] = df_scaled[col].max() + np.random.rand() * 3\n",
        "        else:\n",
        "            # 최소값보다 작은 이상치 생성\n",
        "            outlier_sample[col] = df_scaled[col].min() - np.random.rand() * 3\n",
        "    outliers.append(outlier_sample)\n",
        "\n",
        "outliers_df = pd.concat(outliers, ignore_index=True)\n",
        "\n",
        "# 이상치의 타겟 값 랜덤 할당\n",
        "outliers_df['target'] = np.random.choice(y, num_outliers)\n",
        "\n",
        "# 원본 데이터에 이상치 추가\n",
        "df_extended = pd.concat([df, outliers_df], ignore_index=True)\n",
        "\n",
        "# 이상치 추가 후 다시 정규화\n",
        "X_extended_scaled = scaler.fit_transform(df_extended.drop(columns=['target']))\n",
        "df_extended_scaled = pd.DataFrame(X_extended_scaled, columns=data.feature_names)\n",
        "\n",
        "# 새로운 X, y 정의\n",
        "X = df_extended_scaled.values  # NumPy 배열로 변환\n",
        "y = df_extended['target'].values  # 타겟 값 추출\n",
        "\n",
        "# 기존 이상치 식별\n",
        "df_zscores = df_scaled.apply(zscore)  # Z-score 계산\n",
        "is_existing_outlier = (df_zscores.abs() > 3).any(axis=1)  # 절대값이 3 이상인 경우 이상치로 판단\n",
        "\n",
        "total_existing_outliers = is_existing_outlier.sum()  # 기존 이상치 수\n",
        "total_outliers = total_existing_outliers + num_outliers  # 총 이상치 수\n",
        "total_samples = len(df_extended)  # 전체 샘플 수\n",
        "total_outlier_percentage = (total_outliers / total_samples) * 100  # 이상치 비율 계산\n",
        "\n",
        "# 데이터 확인\n",
        "print(f\"확장된 데이터셋 크기: {X.shape}\")\n",
        "print(f\"확장된 타겟 크기: {y.shape}\")\n",
        "print(f\"이상치가 포함된 데이터셋의 총 샘플 수: {len(X)}\")\n",
        "print(f\"전체 이상치 비율: {total_outlier_percentage:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m12YFMRB1ZJ-",
        "outputId": "b618394c-29ef-4a8a-86e6-f2ca8d86e99f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "확장된 데이터셋 크기: (868, 18)\n",
            "확장된 타겟 크기: (868,)\n",
            "이상치가 포함된 데이터셋의 총 샘플 수: 868\n",
            "전체 이상치 비율: 5.07%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import load_iris\n",
        "from scipy.stats import zscore\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# 데이터셋 로딩\n",
        "data = fetch_openml(name='vehicle', version=1)\n",
        "X = data.data\n",
        "# Use LabelEncoder to convert categorical target to numerical\n",
        "encoder = LabelEncoder()\n",
        "y = encoder.fit_transform(data.target)\n",
        "df = pd.DataFrame(X, columns=data.feature_names)\n",
        "df['target'] = y  # 타겟 변수 추가\n",
        "\n",
        "\n",
        "# 데이터 정규화\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "df_scaled = pd.DataFrame(X_scaled, columns=data.feature_names)\n",
        "\n",
        "# 이상치 생성\n",
        "num_outliers = int(0.027 * len(df))  # 이상치를 설정\n",
        "outliers = []\n",
        "\n",
        "for _ in range(num_outliers):\n",
        "    outlier_sample = df_scaled.sample(n=1).copy()  # 임의의 샘플 선택\n",
        "    for col in df_scaled.columns:\n",
        "        if np.random.rand() > 0.5:\n",
        "            # 최대값보다 큰 이상치 생성\n",
        "            outlier_sample[col] = df_scaled[col].max() + np.random.rand() * 3\n",
        "        else:\n",
        "            # 최소값보다 작은 이상치 생성\n",
        "            outlier_sample[col] = df_scaled[col].min() - np.random.rand() * 3\n",
        "    outliers.append(outlier_sample)\n",
        "\n",
        "outliers_df = pd.concat(outliers, ignore_index=True)\n",
        "\n",
        "# 이상치의 타겟 값 랜덤 할당\n",
        "outliers_df['target'] = np.random.choice(y, num_outliers)\n",
        "\n",
        "# 원본 데이터에 이상치 추가\n",
        "df_extended = pd.concat([df, outliers_df], ignore_index=True)\n",
        "\n",
        "# 이상치 추가 후 다시 정규화\n",
        "X_extended_scaled = scaler.fit_transform(df_extended.drop(columns=['target']))\n",
        "df_extended_scaled = pd.DataFrame(X_extended_scaled, columns=data.feature_names)\n",
        "\n",
        "# 새로운 X, y 정의\n",
        "X = df_extended_scaled.values  # NumPy 배열로 변환\n",
        "y = df_extended['target'].values  # 타겟 값 추출\n",
        "\n",
        "# 기존 이상치 식별\n",
        "df_zscores = df_scaled.apply(zscore)  # Z-score 계산\n",
        "is_existing_outlier = (df_zscores.abs() > 3).any(axis=1)  # 절대값이 3 이상인 경우 이상치로 판단\n",
        "\n",
        "total_existing_outliers = is_existing_outlier.sum()  # 기존 이상치 수\n",
        "total_outliers = total_existing_outliers + num_outliers  # 총 이상치 수\n",
        "total_samples = len(df_extended)  # 전체 샘플 수\n",
        "total_outlier_percentage = (total_outliers / total_samples) * 100  # 이상치 비율 계산\n",
        "\n",
        "# 데이터 확인\n",
        "print(f\"확장된 데이터셋 크기: {X.shape}\")\n",
        "print(f\"확장된 타겟 크기: {y.shape}\")\n",
        "print(f\"이상치가 포함된 데이터셋의 총 샘플 수: {len(X)}\")\n",
        "print(f\"전체 이상치 비율: {total_outlier_percentage:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zw8ZMpkO1ZJ_",
        "outputId": "fd7f4413-f966-4e4e-e5a2-c60140491c7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "확장된 데이터셋 크기: (916, 18)\n",
            "확장된 타겟 크기: (916,)\n",
            "이상치가 포함된 데이터셋의 총 샘플 수: 916\n",
            "전체 이상치 비율: 10.04%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import load_iris\n",
        "from scipy.stats import zscore\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# 데이터셋 로딩\n",
        "data = fetch_openml(name='vehicle', version=1)\n",
        "X = data.data\n",
        "# Use LabelEncoder to convert categorical target to numerical\n",
        "encoder = LabelEncoder()\n",
        "y = encoder.fit_transform(data.target)\n",
        "df = pd.DataFrame(X, columns=data.feature_names)\n",
        "df['target'] = y  # 타겟 변수 추가\n",
        "\n",
        "\n",
        "# 데이터 정규화\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "df_scaled = pd.DataFrame(X_scaled, columns=data.feature_names)\n",
        "\n",
        "# 이상치 생성\n",
        "num_outliers = int(0.083 * len(df))  # 이상치를 설정\n",
        "outliers = []\n",
        "\n",
        "for _ in range(num_outliers):\n",
        "    outlier_sample = df_scaled.sample(n=1).copy()  # 임의의 샘플 선택\n",
        "    for col in df_scaled.columns:\n",
        "        if np.random.rand() > 0.5:\n",
        "            # 최대값보다 큰 이상치 생성\n",
        "            outlier_sample[col] = df_scaled[col].max() + np.random.rand() * 3\n",
        "        else:\n",
        "            # 최소값보다 작은 이상치 생성\n",
        "            outlier_sample[col] = df_scaled[col].min() - np.random.rand() * 3\n",
        "    outliers.append(outlier_sample)\n",
        "\n",
        "outliers_df = pd.concat(outliers, ignore_index=True)\n",
        "\n",
        "# 이상치의 타겟 값 랜덤 할당\n",
        "outliers_df['target'] = np.random.choice(y, num_outliers)\n",
        "\n",
        "# 원본 데이터에 이상치 추가\n",
        "df_extended = pd.concat([df, outliers_df], ignore_index=True)\n",
        "\n",
        "# 이상치 추가 후 다시 정규화\n",
        "X_extended_scaled = scaler.fit_transform(df_extended.drop(columns=['target']))\n",
        "df_extended_scaled = pd.DataFrame(X_extended_scaled, columns=data.feature_names)\n",
        "\n",
        "# 새로운 X, y 정의\n",
        "X = df_extended_scaled.values  # NumPy 배열로 변환\n",
        "y = df_extended['target'].values  # 타겟 값 추출\n",
        "\n",
        "# 기존 이상치 식별\n",
        "df_zscores = df_scaled.apply(zscore)  # Z-score 계산\n",
        "is_existing_outlier = (df_zscores.abs() > 3).any(axis=1)  # 절대값이 3 이상인 경우 이상치로 판단\n",
        "\n",
        "total_existing_outliers = is_existing_outlier.sum()  # 기존 이상치 수\n",
        "total_outliers = total_existing_outliers + num_outliers  # 총 이상치 수\n",
        "total_samples = len(df_extended)  # 전체 샘플 수\n",
        "total_outlier_percentage = (total_outliers / total_samples) * 100  # 이상치 비율 계산\n",
        "\n",
        "# 데이터 확인\n",
        "print(f\"확장된 데이터셋 크기: {X.shape}\")\n",
        "print(f\"확장된 타겟 크기: {y.shape}\")\n",
        "print(f\"이상치가 포함된 데이터셋의 총 샘플 수: {len(X)}\")\n",
        "print(f\"전체 이상치 비율: {total_outlier_percentage:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1Y5HyHI1ZJ_",
        "outputId": "f99bd983-9cac-4618-910c-523d1c84dbb7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[76.47, 72.16, 72.94, 71.76, 67.45, 68.24]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from itertools import combinations\n",
        "\n",
        "results = []\n",
        "\n",
        "# 데이터 전처리 및 분할\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# OvO 방식을 사용하여 각 SVM 유형에 대해 학습 및 평가\n",
        "for svm_type in ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']:\n",
        "    models_ovo = train_ovo(X_train, y_train, C=1.0, svm_type=svm_type)\n",
        "    y_pred_ovo = predict_ovo(X_test, models_ovo)\n",
        "\n",
        "    # y_pred_ovo 변수를 정의한 후에 데이터 타입 확인 및 변환\n",
        "    if y_test.dtype != np.int64:\n",
        "        y_test = y_test.astype(int)\n",
        "\n",
        "    if y_pred_ovo.dtype != np.int64:\n",
        "        y_pred_ovo = y_pred_ovo.astype(int)\n",
        "\n",
        "    accuracy_ovo = accuracy_score(y_test, y_pred_ovo)\n",
        "    results.append((svm_type, round(accuracy_ovo * 100, 2)))\n",
        "\n",
        "# 결과를 DataFrame으로 변환 및 출력\n",
        "results_df = pd.DataFrame(results, columns=['SVM Type', 'Accuracy'])\n",
        "print(results_df['Accuracy'].tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PM6nUpoB2xU-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "accuracy_data = {\n",
        "    '3.06%' : [71.37, 69.02, 70.2, 72.55, 71.37, 70.59],\n",
        "    '5.07%' : [73.56, 77.01, 73.95, 73.56, 73.18, 72.80],\n",
        "    '10.04%' : [70.18, 72.36, 70.91, 70.91, 65.82, 69.45]\n",
        "}\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Vehicle data (ACO)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P0NKKKS91ZKA"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "accuracy_data = {\n",
        "    '3.06%' : [25.49, 25.49, 78.43, 78.04, 78.82, 78.43],\n",
        "    '5.07%' : [21.07, 21.07, 78.54, 77.78, 77.78, 78.93],\n",
        "    '10.04%' : [70.18, 72.36, 70.91, 70.91, 65.82, 69.45]#하기\n",
        "}\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Vehicle data (L-BFGS-B)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KA352w3l17as"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "accuracy_data = {\n",
        "    '3.06%' : [76.86, 78.43, 77.65, 76.86, 72.55, 74.51],\n",
        "    '5.07%' : [73.56, 77.01, 73.95, 73.56, 73.18, 72.80],\n",
        "    '10.04%' : [70.18, 72.36, 70.91, 70.91, 65.82, 69.45]\n",
        "}\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Vehicle data (L-BFGS-B)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9iFu2H8j174V"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "accuracy_data = {\n",
        "    '3.06%' : [76.86, 78.43, 77.65, 76.86, 72.55, 74.51],\n",
        "    '5.07%' : [73.56, 77.01, 73.95, 73.56, 73.18, 72.80],\n",
        "    '10.04%' : [70.18, 72.36, 70.91, 70.91, 65.82, 69.45]\n",
        "}\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Vehicle data (HS)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJ2rcmKa18O2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "accuracy_data = {\n",
        "    '3.06%' : [76.86, 78.43, 77.65, 76.86, 72.55, 74.51],\n",
        "    '5.07%' : [73.56, 77.01, 73.95, 73.56, 73.18, 72.80],\n",
        "    '10.04%' : [70.18, 72.36, 70.91, 70.91, 65.82, 69.45]\n",
        "}\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Vehicle data (SMO)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q8ZDpLfU18Wi"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "accuracy_data = {\n",
        "    '3.06%' : [76.86, 78.43, 77.65, 76.86, 72.55, 74.51],\n",
        "    '5.07%' : [73.56, 77.01, 73.95, 73.56, 73.18, 72.80],\n",
        "    '10.04%' : [70.18, 72.36, 70.91, 70.91, 65.82, 69.45]\n",
        "}\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Vehicle data (GA)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Rz6V3cU18EN",
        "outputId": "3ee26f14-b249-43e4-bdc6-22906a97966e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                    3.06%  5.07%  10.04%\n",
            "Vehicle data (PSO)                      \n",
            "L1                  71.37  67.82   63.64\n",
            "L2                  69.02  65.52   65.09\n",
            "Fair                70.20  67.82   63.64\n",
            "Cauchy              72.55  66.28   67.64\n",
            "Welsch              71.37  68.97   66.91\n",
            "Geman-McClure       70.59  73.95   65.82\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "accuracy_data = {\n",
        "    '3.06%' : [71.37, 69.02, 70.2, 72.55, 71.37, 70.59],\n",
        "    '5.07%' : [67.82, 65.52, 67.82, 66.28, 68.97, 73.95],\n",
        "    '10.04%' : [63.64, 65.09, 63.64, 67.64, 66.91, 65.82]\n",
        "}\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Vehicle data (PSO)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CHq01pvGEfc"
      },
      "source": [
        "#**Direct with various outliers**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGrUQYooGEfd"
      },
      "source": [
        "###iris data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5l3Dd_plGEfe",
        "outputId": "26c5b259-92c0-44bf-b9e6-85ed9ec9053e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "기존 이상치 수: 1\n",
            "기존 이상치 비율: 0.67%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.stats import zscore\n",
        "\n",
        "# 데이터셋 로딩\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "df = pd.DataFrame(X, columns=data.feature_names)\n",
        "df['target'] = y  # 타겟 변수 추가\n",
        "\n",
        "# 데이터 정규화\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "df_scaled = pd.DataFrame(X_scaled, columns=data.feature_names)\n",
        "\n",
        "# 기존 이상치 식별\n",
        "df_zscores = df_scaled.apply(zscore)  # Z-score 계산\n",
        "is_existing_outlier = (df_zscores.abs() > 3).any(axis=1)  # 절대값이 3 이상인 경우 이상치로 판단\n",
        "\n",
        "# 기존 이상치 수 계산\n",
        "total_existing_outliers = is_existing_outlier.sum()\n",
        "\n",
        "# 기존 이상치 비율 계산\n",
        "total_samples = len(df)\n",
        "existing_outlier_percentage = (total_existing_outliers / total_samples) * 100\n",
        "\n",
        "print(f\"기존 이상치 수: {total_existing_outliers}\")\n",
        "print(f\"기존 이상치 비율: {existing_outlier_percentage:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jfWH9vvMGEff",
        "outputId": "0cd9b0d0-3785-490d-cebe-4b93bc08a277"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "확장된 데이터셋 크기: (154, 4)\n",
            "확장된 타겟 크기: (154,)\n",
            "이상치가 포함된 데이터셋의 총 샘플 수: 154\n",
            "전체 이상치 비율: 3.25%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import load_iris\n",
        "from scipy.stats import zscore\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# 데이터셋 로딩\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "df = pd.DataFrame(X, columns=iris.feature_names)\n",
        "df['target'] = y  # 타겟 변수 추가\n",
        "\n",
        "# 데이터 정규화\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "df_scaled = pd.DataFrame(X_scaled, columns=iris.feature_names)\n",
        "\n",
        "# 이상치 생성\n",
        "num_outliers = int(0.03 * len(df))  # 이상치를 설정\n",
        "outliers = []\n",
        "\n",
        "for _ in range(num_outliers):\n",
        "    outlier_sample = df_scaled.sample(n=1).copy()  # 임의의 샘플 선택\n",
        "    for col in df_scaled.columns:\n",
        "        if np.random.rand() > 0.5:\n",
        "            # 최대값보다 큰 이상치 생성\n",
        "            outlier_sample[col] = df_scaled[col].max() + np.random.rand() * 3\n",
        "        else:\n",
        "            # 최소값보다 작은 이상치 생성\n",
        "            outlier_sample[col] = df_scaled[col].min() - np.random.rand() * 3\n",
        "    outliers.append(outlier_sample)\n",
        "\n",
        "outliers_df = pd.concat(outliers, ignore_index=True)\n",
        "\n",
        "# 이상치의 타겟 값 랜덤 할당\n",
        "outliers_df['target'] = np.random.choice(y, num_outliers)\n",
        "\n",
        "# 원본 데이터에 이상치 추가\n",
        "df_extended = pd.concat([df, outliers_df], ignore_index=True)\n",
        "\n",
        "# 이상치 추가 후 다시 정규화\n",
        "X_extended_scaled = scaler.fit_transform(df_extended.drop(columns=['target']))\n",
        "df_extended_scaled = pd.DataFrame(X_extended_scaled, columns=iris.feature_names)\n",
        "\n",
        "# 새로운 X, y 정의\n",
        "X = df_extended_scaled.values  # NumPy 배열로 변환\n",
        "y = df_extended['target'].values  # 타겟 값 추출\n",
        "\n",
        "# 기존 이상치 식별\n",
        "df_zscores = df_scaled.apply(zscore)  # Z-score 계산\n",
        "is_existing_outlier = (df_zscores.abs() > 3).any(axis=1)  # 절대값이 3 이상인 경우 이상치로 판단\n",
        "\n",
        "total_existing_outliers = is_existing_outlier.sum()  # 기존 이상치 수\n",
        "total_outliers = total_existing_outliers + num_outliers  # 총 이상치 수\n",
        "total_samples = len(df_extended)  # 전체 샘플 수\n",
        "total_outlier_percentage = (total_outliers / total_samples) * 100  # 이상치 비율 계산\n",
        "\n",
        "# 데이터 확인\n",
        "print(f\"확장된 데이터셋 크기: {X.shape}\")\n",
        "print(f\"확장된 타겟 크기: {y.shape}\")\n",
        "print(f\"이상치가 포함된 데이터셋의 총 샘플 수: {len(X)}\")\n",
        "print(f\"전체 이상치 비율: {total_outlier_percentage:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q21x3nUEGEff",
        "outputId": "dbebfd31-39aa-4077-82a4-ec90b48cc991"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "확장된 데이터셋 크기: (157, 4)\n",
            "확장된 타겟 크기: (157,)\n",
            "이상치가 포함된 데이터셋의 총 샘플 수: 157\n",
            "전체 이상치 비율: 5.10%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import load_iris\n",
        "from scipy.stats import zscore\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# 데이터셋 로딩\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "df = pd.DataFrame(X, columns=iris.feature_names)\n",
        "df['target'] = y  # 타겟 변수 추가\n",
        "\n",
        "# 데이터 정규화\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "df_scaled = pd.DataFrame(X_scaled, columns=iris.feature_names)\n",
        "\n",
        "# 이상치 생성\n",
        "num_outliers = int(0.05 * len(df))  # 이상치를 설정\n",
        "outliers = []\n",
        "\n",
        "for _ in range(num_outliers):\n",
        "    outlier_sample = df_scaled.sample(n=1).copy()  # 임의의 샘플 선택\n",
        "    for col in df_scaled.columns:\n",
        "        if np.random.rand() > 0.5:\n",
        "            # 최대값보다 큰 이상치 생성\n",
        "            outlier_sample[col] = df_scaled[col].max() + np.random.rand() * 3\n",
        "        else:\n",
        "            # 최소값보다 작은 이상치 생성\n",
        "            outlier_sample[col] = df_scaled[col].min() - np.random.rand() * 3\n",
        "    outliers.append(outlier_sample)\n",
        "\n",
        "outliers_df = pd.concat(outliers, ignore_index=True)\n",
        "\n",
        "# 이상치의 타겟 값 랜덤 할당\n",
        "outliers_df['target'] = np.random.choice(y, num_outliers)\n",
        "\n",
        "# 원본 데이터에 이상치 추가\n",
        "df_extended = pd.concat([df, outliers_df], ignore_index=True)\n",
        "\n",
        "# 이상치 추가 후 다시 정규화\n",
        "X_extended_scaled = scaler.fit_transform(df_extended.drop(columns=['target']))\n",
        "df_extended_scaled = pd.DataFrame(X_extended_scaled, columns=iris.feature_names)\n",
        "\n",
        "# 새로운 X, y 정의\n",
        "X = df_extended_scaled.values  # NumPy 배열로 변환\n",
        "y = df_extended['target'].values  # 타겟 값 추출\n",
        "\n",
        "# 기존 이상치 식별\n",
        "df_zscores = df_scaled.apply(zscore)  # Z-score 계산\n",
        "is_existing_outlier = (df_zscores.abs() > 3).any(axis=1)  # 절대값이 3 이상인 경우 이상치로 판단\n",
        "\n",
        "total_existing_outliers = is_existing_outlier.sum()  # 기존 이상치 수\n",
        "total_outliers = total_existing_outliers + num_outliers  # 총 이상치 수\n",
        "total_samples = len(df_extended)  # 전체 샘플 수\n",
        "total_outlier_percentage = (total_outliers / total_samples) * 100  # 이상치 비율 계산\n",
        "\n",
        "# 데이터 확인\n",
        "print(f\"확장된 데이터셋 크기: {X.shape}\")\n",
        "print(f\"확장된 타겟 크기: {y.shape}\")\n",
        "print(f\"이상치가 포함된 데이터셋의 총 샘플 수: {len(X)}\")\n",
        "print(f\"전체 이상치 비율: {total_outlier_percentage:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzDahwVhGEfg",
        "outputId": "10d1208f-4b64-4096-9986-68fa5ee84010"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "확장된 데이터셋 크기: (156, 4)\n",
            "확장된 타겟 크기: (156,)\n",
            "이상치가 포함된 데이터셋의 총 샘플 수: 156\n",
            "전체 이상치 비율: 4.49%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import load_iris\n",
        "from scipy.stats import zscore\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# 데이터셋 로딩\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "df = pd.DataFrame(X, columns=iris.feature_names)\n",
        "df['target'] = y  # 타겟 변수 추가\n",
        "\n",
        "# 데이터 정규화\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "df_scaled = pd.DataFrame(X_scaled, columns=iris.feature_names)\n",
        "\n",
        "# 이상치 생성\n",
        "num_outliers = int(0.04 * len(df))  # 이상치를 설정\n",
        "outliers = []\n",
        "\n",
        "for _ in range(num_outliers):\n",
        "    outlier_sample = df_scaled.sample(n=1).copy()  # 임의의 샘플 선택\n",
        "    for col in df_scaled.columns:\n",
        "        if np.random.rand() > 0.5:\n",
        "            # 최대값보다 큰 이상치 생성\n",
        "            outlier_sample[col] = df_scaled[col].max() + np.random.rand() * 3\n",
        "        else:\n",
        "            # 최소값보다 작은 이상치 생성\n",
        "            outlier_sample[col] = df_scaled[col].min() - np.random.rand() * 3\n",
        "    outliers.append(outlier_sample)\n",
        "\n",
        "outliers_df = pd.concat(outliers, ignore_index=True)\n",
        "\n",
        "# 이상치의 타겟 값 랜덤 할당\n",
        "outliers_df['target'] = np.random.choice(y, num_outliers)\n",
        "\n",
        "# 원본 데이터에 이상치 추가\n",
        "df_extended = pd.concat([df, outliers_df], ignore_index=True)\n",
        "\n",
        "# 이상치 추가 후 다시 정규화\n",
        "X_extended_scaled = scaler.fit_transform(df_extended.drop(columns=['target']))\n",
        "df_extended_scaled = pd.DataFrame(X_extended_scaled, columns=iris.feature_names)\n",
        "\n",
        "# 새로운 X, y 정의\n",
        "X = df_extended_scaled.values  # NumPy 배열로 변환\n",
        "y = df_extended['target'].values  # 타겟 값 추출\n",
        "\n",
        "# 기존 이상치 식별\n",
        "df_zscores = df_scaled.apply(zscore)  # Z-score 계산\n",
        "is_existing_outlier = (df_zscores.abs() > 3).any(axis=1)  # 절대값이 3 이상인 경우 이상치로 판단\n",
        "\n",
        "total_existing_outliers = is_existing_outlier.sum()  # 기존 이상치 수\n",
        "total_outliers = total_existing_outliers + num_outliers  # 총 이상치 수\n",
        "total_samples = len(df_extended)  # 전체 샘플 수\n",
        "total_outlier_percentage = (total_outliers / total_samples) * 100  # 이상치 비율 계산\n",
        "\n",
        "# 데이터 확인\n",
        "print(f\"확장된 데이터셋 크기: {X.shape}\")\n",
        "print(f\"확장된 타겟 크기: {y.shape}\")\n",
        "print(f\"이상치가 포함된 데이터셋의 총 샘플 수: {len(X)}\")\n",
        "print(f\"전체 이상치 비율: {total_outlier_percentage:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iu7k0voWGEfg",
        "outputId": "0c87903f-918a-43c8-c6c9-4207ee87447d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[100.0, 100.0, 97.78, 97.78, 97.78, 97.78]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from itertools import combinations\n",
        "\n",
        "results = []\n",
        "\n",
        "# 데이터 전처리 및 분할\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Direct 방식을 사용하여 각 SVM 유형에 대해 학습 및 평가\n",
        "for svm_type in ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']:\n",
        "    num_classes = len(np.unique(y))\n",
        "    weights, biases = train_direct(X_train, y_train, C=1.0, svm_type=svm_type, num_classes=num_classes)\n",
        "    y_pred_direct = predict_direct(X_test, weights, biases)\n",
        "\n",
        "\n",
        "    # y_pred_direct 변수를 정의한 후에 데이터 타입 확인 및 변환\n",
        "    if y_test.dtype != np.int64:\n",
        "        y_test = y_test.astype(int)\n",
        "\n",
        "    if y_pred_direct.dtype != np.int64:\n",
        "        y_pred_direct = y_pred_direct.astype(int)\n",
        "\n",
        "    accuracy_direct = accuracy_score(y_test, y_pred_direct)\n",
        "    results.append((svm_type, round(accuracy_direct * 100, 2)))\n",
        "\n",
        "# 결과를 DataFrame으로 변환 및 출력\n",
        "results_df = pd.DataFrame(results, columns=['SVM Type', 'Accuracy'])\n",
        "print(results_df['Accuracy'].tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5DFZWr9GEfh",
        "outputId": "b5b66b19-84c8-4ace-a3c4-7f6597351144"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                       3.25%  5.10%  10.24%\n",
            "Iris data (L-BFGS-B)                       \n",
            "L1                    100.00  91.67    64.0\n",
            "L2                     97.87  93.75    76.0\n",
            "Fair                   95.74  91.67    76.0\n",
            "Cauchy                 95.74  91.67    94.0\n",
            "Welsch                 95.74  91.67    92.0\n",
            "Geman-McClure          95.74  91.67    92.0\n"
          ]
        }
      ],
      "source": [
        "#0.67% (L-BFGS-B,PSO,HS)\n",
        "[24.44, 37.78, 62.22, 62.22, 64.44, 48.89]\n",
        "[100.0, 100.0, 97.78, 97.78, 97.78, 97.78]\n",
        "[80.0, 86.67, 73.33, 84.44, 86.67, 80.0]\n",
        "\n",
        "#3.25%\n",
        "[100.00, 97.87, 95.74, 95.74, 95.74, 95.74]\n",
        "[91.49, 95.74, 91.49, 97.87, 89.36, 95.74]\n",
        "[78.72, 87.23, 76.60, 89.36, 74.47, 80.85]\n",
        "\n",
        "#4.49%\n",
        "[48.94, 25.53, 23.4, 48.94, 25.53, 29.79]\n",
        "[65.96, 68.09, 65.96, 93.62, 89.36, 95.74]\n",
        "[72.34, 59.57, 51.06, 91.49, 70.21, 91.49]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#3.25%\n",
        "[100.00, 97.87, 95.74, 95.74, 95.74, 95.74]\n",
        "[87.23, 78.72, 78.72, 78.72, 74.47, 74.47]\n",
        "[91.49, 95.74, 91.49, 97.87, 89.36, 95.74]\n",
        "[95.74, 97.87, 95.74, 97.87, 95.74, 95.74]\n",
        "[78.72, 87.23, 76.60, 89.36, 74.47, 80.85]\n",
        "\n",
        "#3.87%\n",
        "[27.66, 51.06, 27.66, 29.79, 55.32, 55.32]\n",
        "[61.7, 78.72, 70.21, 80.85, 72.34, 74.47]\n",
        "[70.21, 85.11, 87.23, 97.87, 95.74, 93.62]\n",
        "[87.23, 89.36, 87.23, 95.74, 97.87, 95.74]\n",
        "[95.74, 82.98, 80.85, 91.49, 91.49, 82.98]\n",
        "\n",
        "#4.49%\n",
        "[48.94, 25.53, 23.4, 48.94, 25.53, 29.79]\n",
        "[76.6, 61.7, 65.96, 63.83, 70.21, 76.6]\n",
        "[65.96, 68.09, 65.96, 93.62, 89.36, 95.74]\n",
        "[65.96, 63.83, 78.72, 95.74, 93.62, 93.62]\n",
        "[72.34, 59.57, 51.06, 91.49, 70.21, 91.49]\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "accuracy_data = {\n",
        "    '3.25%' : [100.00, 97.87, 95.74, 95.74, 95.74, 95.74],\n",
        "    '5.10%' : [91.67, 93.75, 91.67, 91.67, 91.67, 91.67],\n",
        "    '10.24%' : [64.0, 76.0, 76.0, 94.0, 92.0, 92.0]\n",
        "}\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Iris data (L-BFGS-B)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GnzwgoZmGEfh",
        "outputId": "ecfd27c3-1ee6-47bd-8b37-51c28739f6ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                3.25%  5.10%  10.24%\n",
            "Iris data (GA)                      \n",
            "L1              87.23  70.83    70.0\n",
            "L2              78.72  41.67    64.0\n",
            "Fair            78.72  72.92    80.0\n",
            "Cauchy          78.72  81.25    68.0\n",
            "Welsch          74.47  70.83    72.0\n",
            "Geman-McClure   74.47  60.42    90.0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "outlier = ['3%', '5%', '10%']\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "\n",
        "accuracy_data = {\n",
        "    '3.25%' : [87.23, 78.72, 78.72, 78.72, 74.47, 74.47],\n",
        "    '5.10%' : [70.83, 41.67, 72.92, 81.25, 70.83, 60.42],\n",
        "    '10.24%' : [70.0, 64.0, 80.0, 68.0, 72.0, 90.0]\n",
        "}\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Iris data (GA)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKuEWMuRGEfi",
        "outputId": "9fa6f022-a082-4a69-eb7d-0a9d393cbc34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                 3.25%  5.10%  10.24%\n",
            "Iris data (SMO)                      \n",
            "L1               89.36  93.75    88.0\n",
            "L2               89.36  93.75    88.0\n",
            "Fair             87.23  89.58    94.0\n",
            "Cauchy           87.23  87.50    94.0\n",
            "Welsch           89.36  89.58    94.0\n",
            "Geman-McClure    89.36  89.58    94.0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "outlier = ['3%', '5%', '10%']\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "\n",
        "accuracy_data = {\n",
        "    '3.25%' : [89.36, 89.36, 87.23, 87.23, 89.36, 89.36],\n",
        "    '5.10%' : [93.75, 93.75, 89.58, 87.50, 89.58, 89.58],\n",
        "    '10.24%' : [88.0, 88.0, 94.0, 94.0, 94.0, 94.0]\n",
        "}\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Iris data (SMO)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcF4f3MfGEfi",
        "outputId": "d65f10c8-b400-4835-91e4-863ad8dce7af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                 3.25%  5.10%  10.24%\n",
            "Iris data (PSO)                      \n",
            "L1               91.49  91.67    68.0\n",
            "L2               95.74  93.75    64.0\n",
            "Fair             91.49  93.75    80.0\n",
            "Cauchy           97.87  89.58    92.0\n",
            "Welsch           89.36  91.67    64.0\n",
            "Geman-McClure    95.74  91.67    90.0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "outlier = ['3%', '5%', '10%']\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "\n",
        "accuracy_data = {\n",
        "    '3.25%' : [91.49, 95.74, 91.49, 97.87, 89.36, 95.74],\n",
        "    '5.10%' : [91.67, 93.75, 93.75, 89.58, 91.67, 91.67],\n",
        "    '10.24%' : [68.0, 64.0, 80.0, 92.0, 64.0, 90.0]\n",
        "}\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Iris data (PSO)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_BTYGh6yGEfi",
        "outputId": "674b8190-79e7-46fd-d649-277e5ff1bae5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                 3.25%  5.10%  10.24%\n",
            "Iris data (ACO)                      \n",
            "L1               82.98  87.50    66.0\n",
            "L2               91.49  93.75    64.0\n",
            "Fair             87.23  91.67    64.0\n",
            "Cauchy           85.11  89.58    80.0\n",
            "Welsch           74.47  66.67    64.0\n",
            "Geman-McClure    78.72  72.92    66.0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "outlier = ['3%', '5%', '10%']\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "\n",
        "accuracy_data = {\n",
        "    '3.25%' : [95.74, 97.87, 95.74, 97.87, 95.74, 95.74],\n",
        "    '5.10%' : [93.75, 93.75, 93.75, 91.67, 91.67, 91.67],\n",
        "    '10.24%' : [64.0, 86.0, 82.0, 94.0, 92.0, 92.0]\n",
        "}\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Iris data (ACO)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecwnwjtOGEfi",
        "outputId": "72de91d4-57d6-4364-c611-0afdc42a5ea2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                3.25%  5.10%  10.24%\n",
            "Iris data (HS)                      \n",
            "L1              78.72  66.67    84.0\n",
            "L2              87.23  68.75    18.0\n",
            "Fair            76.60  70.83    70.0\n",
            "Cauchy          89.36  75.00    74.0\n",
            "Welsch          74.47  81.25    66.0\n",
            "Geman-McClure   80.85  83.33    78.0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "accuracy_data = {\n",
        "    '3.25%' : [78.72, 87.23, 76.60, 89.36, 74.47, 80.85],\n",
        "    '5.10%' : [66.67, 68.75, 70.83, 75.00, 81.25, 83.33],\n",
        "    '10.24%' : [84.0, 18.0, 70.0, 74.0, 66.0, 78.0]\n",
        "}\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Iris data (HS)'\n",
        "\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFztGPM4tITa"
      },
      "source": [
        "### segment data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FnXeMi-NtI1c",
        "outputId": "d66e631c-827b-4c45-d4f9-9ac2ebb05bc8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "확장된 데이터셋 크기: (2374, 19)\n",
            "확장된 타겟 크기: (2374,)\n",
            "이상치가 포함된 데이터셋의 총 샘플 수: 2374\n",
            "전체 이상치 비율: 10.07%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from scipy.stats import zscore\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# 데이터셋 로딩\n",
        "data = fetch_openml(name='segment', version=1)\n",
        "X = data.data\n",
        "# Use LabelEncoder to convert categorical target to numerical\n",
        "encoder = LabelEncoder()\n",
        "y = encoder.fit_transform(data.target)\n",
        "df = pd.DataFrame(X, columns=data.feature_names)\n",
        "df['target'] = y  # 타겟 변수 추가\n",
        "\n",
        "# 데이터 정규화\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "df_scaled = pd.DataFrame(X_scaled, columns=data.feature_names)\n",
        "\n",
        "# 이상치 생성\n",
        "num_outliers = int(0.028 * len(df))  # 이상치를 설정\n",
        "outliers = []\n",
        "\n",
        "for _ in range(num_outliers):\n",
        "    outlier_sample = df_scaled.sample(n=1).copy()  # 임의의 샘플 선택\n",
        "    for col in df_scaled.columns:\n",
        "        if np.random.rand() > 0.5:\n",
        "            # 최대값보다 큰 이상치 생성\n",
        "            outlier_sample[col] = df_scaled[col].max() + np.random.rand() * 3\n",
        "        else:\n",
        "            # 최소값보다 작은 이상치 생성\n",
        "            outlier_sample[col] = df_scaled[col].min() - np.random.rand() * 3\n",
        "    outliers.append(outlier_sample)\n",
        "\n",
        "outliers_df = pd.concat(outliers, ignore_index=True)\n",
        "\n",
        "# 이상치의 타겟 값 랜덤 할당\n",
        "outliers_df['target'] = np.random.choice(y, num_outliers)\n",
        "\n",
        "# 원본 데이터에 이상치 추가\n",
        "df_extended = pd.concat([df, outliers_df], ignore_index=True)\n",
        "\n",
        "# 이상치 추가 후 다시 정규화\n",
        "X_extended_scaled = scaler.fit_transform(df_extended.drop(columns=['target']))\n",
        "df_extended_scaled = pd.DataFrame(X_extended_scaled, columns=data.feature_names)\n",
        "\n",
        "# 새로운 X, y 정의\n",
        "X = df_extended_scaled.values  # NumPy 배열로 변환\n",
        "y = df_extended['target'].values  # 타겟 값 추출\n",
        "\n",
        "# 기존 이상치 식별\n",
        "df_zscores = df_scaled.apply(zscore)  # Z-score 계산\n",
        "is_existing_outlier = (df_zscores.abs() > 3).any(axis=1)  # 절대값이 3 이상인 경우 이상치로 판단\n",
        "\n",
        "total_existing_outliers = is_existing_outlier.sum()  # 기존 이상치 수\n",
        "total_outliers = total_existing_outliers + num_outliers  # 총 이상치 수\n",
        "total_samples = len(df_extended)  # 전체 샘플 수\n",
        "total_outlier_percentage = (total_outliers / total_samples) * 100  # 이상치 비율 계산\n",
        "\n",
        "# 데이터 확인\n",
        "print(f\"확장된 데이터셋 크기: {X.shape}\")\n",
        "print(f\"확장된 타겟 크기: {y.shape}\")\n",
        "print(f\"이상치가 포함된 데이터셋의 총 샘플 수: {len(X)}\")\n",
        "print(f\"전체 이상치 비율: {total_outlier_percentage:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBva2Fc6tI1c",
        "outputId": "e3253056-a51d-4ab6-979b-a631b2d81e1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "기존 이상치 수: 175\n",
            "기존 이상치 비율: 7.58%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from scipy.stats import zscore\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# 데이터셋 로딩\n",
        "data = fetch_openml(name='segment', version=1)\n",
        "X = data.data\n",
        "# Use LabelEncoder to convert categorical target to numerical\n",
        "encoder = LabelEncoder()\n",
        "y = encoder.fit_transform(data.target)\n",
        "df = pd.DataFrame(X, columns=data.feature_names)\n",
        "df['target'] = y  # 타겟 변수 추가\n",
        "\n",
        "# 데이터 정규화\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "df_scaled = pd.DataFrame(X_scaled, columns=data.feature_names)\n",
        "\n",
        "# 기존 이상치 식별\n",
        "df_zscores = df_scaled.apply(zscore)  # Z-score 계산\n",
        "is_existing_outlier = (df_zscores.abs() > 3).any(axis=1)  # 절대값이 3 이상인 경우 이상치로 판단\n",
        "\n",
        "# 기존 이상치 수 계산\n",
        "total_existing_outliers = is_existing_outlier.sum()\n",
        "\n",
        "# 기존 이상치 비율 계산\n",
        "total_samples = len(df)\n",
        "existing_outlier_percentage = (total_existing_outliers / total_samples) * 100\n",
        "\n",
        "print(f\"기존 이상치 수: {total_existing_outliers}\")\n",
        "print(f\"기존 이상치 비율: {existing_outlier_percentage:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z2nCRfSh2DXc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from scipy.stats import zscore\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# 데이터셋 로딩\n",
        "data = fetch_openml(name='segment', version=1)\n",
        "X = data.data\n",
        "# Use LabelEncoder to convert categorical target to numerical\n",
        "encoder = LabelEncoder()\n",
        "y = encoder.fit_transform(data.target)\n",
        "df = pd.DataFrame(X, columns=data.feature_names)\n",
        "df['target'] = y  # 타겟 변수 추가\n",
        "\n",
        "# 데이터 정규화\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "df_scaled = pd.DataFrame(X_scaled, columns=data.feature_names)\n",
        "\n",
        "# 기존 이상치 식별\n",
        "df_zscores = df_scaled.apply(zscore)  # Z-score 계산\n",
        "is_existing_outlier = (df_zscores.abs() > 3).any(axis=1)  # 절대값이 3 이상인 경우 이상치로 판단\n",
        "\n",
        "df_cleaned = df.loc[~is_existing_outlier].reset_index(drop=True)\n",
        "X_cleaned = df_cleaned.drop(columns='target')\n",
        "y_cleaned = df_cleaned['target']\n",
        "\n",
        "# 새로운 X, y 정의\n",
        "X = X_cleaned.values  # NumPy 배열로 변환\n",
        "y = y_cleaned.values  # 타겟 값 추출"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LsmZU_dptI1e",
        "outputId": "eb997863-249c-48cc-ed53-07595184466c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[85.65, 85.65, 17.0, 17.0, 17.0, 17.0]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from itertools import combinations\n",
        "\n",
        "results = []\n",
        "\n",
        "# 데이터 전처리 및 분할\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Direct 방식을 사용하여 각 SVM 유형에 대해 학습 및 평가\n",
        "for svm_type in ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']:\n",
        "    num_classes = len(np.unique(y))\n",
        "    weights, biases = train_direct(X_train, y_train, C=1.0, svm_type=svm_type, num_classes=num_classes)\n",
        "    y_pred_direct = predict_direct(X_test, weights, biases)\n",
        "\n",
        "\n",
        "    # y_pred_direct 변수를 정의한 후에 데이터 타입 확인 및 변환\n",
        "    if y_test.dtype != np.int64:\n",
        "        y_test = y_test.astype(int)\n",
        "\n",
        "    if y_pred_direct.dtype != np.int64:\n",
        "        y_pred_direct = y_pred_direct.astype(int)\n",
        "\n",
        "    accuracy_direct = accuracy_score(y_test, y_pred_direct)\n",
        "    results.append((svm_type, round(accuracy_direct * 100, 2)))\n",
        "\n",
        "# 결과를 DataFrame으로 변환 및 출력\n",
        "results_df = pd.DataFrame(results, columns=['SVM Type', 'Accuracy'])\n",
        "print(results_df['Accuracy'].tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0hC4HOt1_Pq",
        "outputId": "cde4d56d-0d11-441a-d8a4-a48813127ac7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                          3 sigma  4.5 sigma  full original data\n",
            "Segment data (L-BFGS-B)                                         \n",
            "L1                          18.72      12.50               21.79\n",
            "L2                          17.94      17.56               17.75\n",
            "Fair                         9.98      17.41               18.04\n",
            "Cauchy                      18.10      15.62               18.47\n",
            "Welsch                      18.10      24.11               19.91\n",
            "Geman-McClure               20.28      19.79               18.04\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "accuracy_data = {\n",
        "    ' 3 sigma' : [18.72, 17.94, 9.98, 18.1, 18.1, 20.28],\n",
        "    '4.5 sigma' : [12.5, 17.56, 17.41, 15.62, 24.11, 19.79],\n",
        "    'full original data' : [21.79, 17.75, 18.04, 18.47, 19.91, 18.04]\n",
        "}\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Segment data (L-BFGS-B)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4DoyXqOF1_Ps",
        "outputId": "9a9c17c4-3ba0-486f-d0da-11707ecf7249"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                    3 sigma  4.5 sigma  full original data\n",
            "Segment data (GA)                                         \n",
            "L1                    35.73      41.37               50.94\n",
            "L2                    37.44      31.55               37.81\n",
            "Fair                  49.14      51.64               35.93\n",
            "Cauchy                46.18      48.21               34.92\n",
            "Welsch                37.75      37.05               40.26\n",
            "Geman-McClure         36.97      27.08               49.64\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "accuracy_data = {\n",
        "    ' 3 sigma' : [35.73, 37.44, 49.14, 46.18, 37.75, 36.97],\n",
        "    '4.5 sigma' : [41.37, 31.55, 51.64, 48.21, 37.05, 27.08],\n",
        "    'full original data' : [50.94, 37.81, 35.93, 34.92, 40.26, 49.64]\n",
        "}\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Segment data (GA)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6d8-k4mW1_Pt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25555ca1-ad4a-42f4-c296-4b0ef3572897"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                     3 sigma  4.5 sigma  full original data\n",
            "Segment data (SMO)                                         \n",
            "L1                     85.65      86.01               85.86\n",
            "L2                     85.65      86.01               85.86\n",
            "Fair                   17.00      14.58               12.41\n",
            "Cauchy                 17.00      14.58               12.41\n",
            "Welsch                 17.00      14.58               12.41\n",
            "Geman-McClure          17.00      14.58               12.41\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "accuracy_data = {\n",
        "    ' 3 sigma' : [85.65, 85.65, 17.0, 17.0, 17.0, 17.0],\n",
        "    '4.5 sigma' : [86.01, 86.01, 14.58, 14.58, 14.58, 14.58],\n",
        "    'full original data' : [85.86, 85.86, 12.41, 12.41, 12.41, 12.41]\n",
        "}\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Segment data (SMO)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8tj5CGF1_Pt",
        "outputId": "0f4d5854-9904-4dcb-e5aa-024ddd52fcb3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                     3 sigma  4.5 sigma  full original data\n",
            "Segment data (PSO)                                         \n",
            "L1                     57.72      45.83               48.34\n",
            "L2                     36.97      39.29               33.77\n",
            "Fair                   69.73      58.63               68.83\n",
            "Cauchy                 54.45      52.38               43.87\n",
            "Welsch                 72.39      67.26               72.29\n",
            "Geman-McClure          63.49      58.48               49.35\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "accuracy_data = {\n",
        "    ' 3 sigma' : [57.72, 36.97, 69.73, 54.45, 72.39, 63.49],\n",
        "    '4.5 sigma' : [45.83, 39.29, 58.63, 52.38, 67.26, 58.48],\n",
        "    'full original data' : [48.34, 33.77, 68.83, 43.87, 72.29, 49.35]\n",
        "}\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Segment data (PSO)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9Pgt8ln1_Pt",
        "outputId": "1a575fdd-884f-4a71-f586-6aafbdbc2e90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                     3 sigma  4.5 sigma  full original data\n",
            "Segment data (ACO)                                         \n",
            "L1                     88.14      86.31               85.71\n",
            "L2                     89.70      86.01               86.72\n",
            "Fair                   88.77      89.14               85.71\n",
            "Cauchy                 87.68      88.99               87.01\n",
            "Welsch                 83.93      88.99               82.97\n",
            "Geman-McClure          90.80      86.31               83.98\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "accuracy_data = {\n",
        "    ' 3 sigma' : [88.14, 89.7, 88.77, 87.68, 83.93, 90.8],\n",
        "    '4.5 sigma' : [86.31, 86.01, 89.14, 88.99, 88.99, 86.31],\n",
        "    'full original data' : [85.71, 86.72, 85.71, 87.01, 82.97, 83.98]\n",
        "}\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Segment data (ACO)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SmSUJ9zo1_Pt",
        "outputId": "0344e63f-5583-4854-91ab-f26446a7f930"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                    3 sigma  4.5 sigma  full original data\n",
            "Segment data (HS)                                         \n",
            "L1                    52.89      47.77               38.82\n",
            "L2                    29.80      39.73               31.60\n",
            "Fair                  47.27      47.47               56.71\n",
            "Cauchy                36.35      55.65               45.31\n",
            "Welsch                55.38      46.58               50.36\n",
            "Geman-McClure         51.48      37.50               40.40\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "accuracy_data = {\n",
        "    ' 3 sigma' : [52.89, 29.8, 47.27, 36.35, 55.38, 51.48],\n",
        "    '4.5 sigma' : [47.77, 39.73, 47.47, 55.65, 46.58, 37.5],\n",
        "    'full original data' : [38.82, 31.60, 56.71, 45.31, 50.36, 40.40]\n",
        "}\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Segment data (HS)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j15XWHdstpRt",
        "outputId": "cc288e63-7118-4d31-e8b4-d36d85b422aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                         7.58%  10.07%\n",
            "Segment data (L-BFGS-B)               \n",
            "L1                       21.79   19.07\n",
            "L2                       17.75   20.06\n",
            "Fair                     18.04   19.21\n",
            "Cauchy                   18.47   19.07\n",
            "Welsch                   19.91   19.50\n",
            "Geman-McClure            18.04   19.35\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "accuracy_data = {\n",
        "    '7.58%' : [21.79,17.75, 18.04, 18.47, 19.91, 18.04],\n",
        "    '10.07%' : [19.07, 20.06, 19.21, 19.07, 19.50, 19.35]\n",
        "}\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Segment data (L-BFGS-B)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zX4dexUytpRu",
        "outputId": "b6b0c3bd-be30-44d5-b34c-fea9d38dfdf4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   7.58%  10.07%\n",
            "Segment data (GA)               \n",
            "L1                 50.94   37.03\n",
            "L2                 37.81   32.54\n",
            "Fair               35.93   54.84\n",
            "Cauchy             34.92   53.72\n",
            "Welsch             40.26   41.51\n",
            "Geman-McClure      49.64   40.25\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "accuracy_data = {\n",
        "    '7.58%' : [ 50.94,37.81, 35.93, 34.92, 40.26, 49.64],\n",
        "    '10.07%' : [37.03, 32.54, 54.84, 53.72, 41.51, 40.25\n",
        "]\n",
        "}\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Segment data (GA)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yAcWJ85OtpRs",
        "outputId": "fd4fadba-fb30-4b99-8f8f-2b3d54d97913"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                    7.58%  10.07%\n",
            "Segment data (SMO)               \n",
            "L1                  85.86   84.99\n",
            "L2                  85.86   84.99\n",
            "Fair                12.41   14.87\n",
            "Cauchy              12.41   14.87\n",
            "Welsch              12.41   14.87\n",
            "Geman-McClure       12.41   14.87\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd #해야함\n",
        "\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "accuracy_data = {\n",
        "    '7.58%' : [ 85.86, 85.86, 12.41, 12.41, 12.41, 12.41],\n",
        "    '10.07%' : [84.99, 84.99, 14.87, 14.87, 14.87, 14.87]\n",
        "}\n",
        "\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Segment data (SMO)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WasNvHa5tpRu",
        "outputId": "ceb4e12c-2d85-49e6-8c8f-9b16ce93e3ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                    7.58%  10.07%\n",
            "Segment data (PSO)               \n",
            "L1                  48.34   44.74\n",
            "L2                  33.77   48.39\n",
            "Fair                68.83   60.73\n",
            "Cauchy              43.87   59.05\n",
            "Welsch              72.29   43.90\n",
            "Geman-McClure       49.35   59.47\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "accuracy_data = {\n",
        "    '7.58%' : [48.34, 33.77, 68.83, 43.87, 72.29, 49.35],\n",
        "    '10.07%' : [44.74, 48.39, 60.73, 59.05, 43.90, 59.47]\n",
        "}\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Segment data (PSO)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMmFEJuBtpRv",
        "outputId": "38be1fdd-bc05-4483-f052-a5543b3d6efe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                    7.58%  10.07%\n",
            "Segment data (ACO)               \n",
            "L1                  85.71   83.17\n",
            "L2                  86.72   67.04\n",
            "Fair                85.71   85.69\n",
            "Cauchy              87.01   85.13\n",
            "Welsch              82.97   84.85\n",
            "Geman-McClure       83.98   87.52\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "accuracy_data = {\n",
        "    '7.58%' : [85.71, 86.72, 85.71, 87.01, 82.97, 83.98],\n",
        "    '10.07%' : [83.17, 67.04, 85.69, 85.13, 84.85, 87.52]\n",
        "}\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Segment data (ACO)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Va9W_CYtpRv",
        "outputId": "0fd2442b-0ed1-4324-a666-56630e35299f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   7.58%  10.07%\n",
            "Segment data (HS)               \n",
            "L1                 38.82   53.16\n",
            "L2                 31.60   36.19\n",
            "Fair               56.71   38.99\n",
            "Cauchy             45.31   39.97\n",
            "Welsch             50.36   46.28\n",
            "Geman-McClure      40.40   52.45\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "accuracy_data = {\n",
        "    '7.58%' : [ 38.82, 31.60, 56.71, 45.31, 50.36, 40.40],\n",
        "    '10.07%' : [53.16, 36.19, 38.99, 39.97, 46.28, 52.45]\n",
        "}\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Segment data (HS)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AExVCWzu1ceU"
      },
      "source": [
        "### vehicle data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4GirKKpo1ceV",
        "outputId": "39783c43-d06b-4b6f-9dd7-16e174b41d61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "기존 이상치 수: 22\n",
            "기존 이상치 비율: 2.60%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from scipy.stats import zscore\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# 데이터셋 로딩\n",
        "data = fetch_openml(name='vehicle', version=1)\n",
        "X = data.data\n",
        "encoder = LabelEncoder()\n",
        "y = encoder.fit_transform(data.target)\n",
        "df = pd.DataFrame(X, columns=data.feature_names)\n",
        "df['target'] = y  # 타겟 변수 추가\n",
        "\n",
        "# 데이터 정규화\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "df_scaled = pd.DataFrame(X_scaled, columns=data.feature_names)\n",
        "\n",
        "# 기존 이상치 식별\n",
        "df_zscores = df_scaled.apply(zscore)  # Z-score 계산\n",
        "is_existing_outlier = (df_zscores.abs() > 3).any(axis=1)  # 절대값이 3 이상인 경우 이상치로 판단\n",
        "\n",
        "# 기존 이상치 수 계산\n",
        "total_existing_outliers = is_existing_outlier.sum()\n",
        "\n",
        "# 기존 이상치 비율 계산\n",
        "total_samples = len(df)\n",
        "existing_outlier_percentage = (total_existing_outliers / total_samples) * 100\n",
        "\n",
        "print(f\"기존 이상치 수: {total_existing_outliers}\")\n",
        "print(f\"기존 이상치 비율: {existing_outlier_percentage:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "739MgnUm1ceW",
        "outputId": "7b7ed294-d4c0-4ec1-ee18-a74c2f01600d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "확장된 데이터셋 크기: (850, 18)\n",
            "확장된 타겟 크기: (850,)\n",
            "이상치가 포함된 데이터셋의 총 샘플 수: 850\n",
            "전체 이상치 비율: 3.06%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import load_iris\n",
        "from scipy.stats import zscore\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# 데이터셋 로딩\n",
        "data = fetch_openml(name='vehicle', version=1)\n",
        "X = data.data\n",
        "# Use LabelEncoder to convert categorical target to numerical\n",
        "encoder = LabelEncoder()\n",
        "y = encoder.fit_transform(data.target)\n",
        "df = pd.DataFrame(X, columns=data.feature_names)\n",
        "df['target'] = y  # 타겟 변수 추가\n",
        "\n",
        "\n",
        "# 데이터 정규화\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "df_scaled = pd.DataFrame(X_scaled, columns=data.feature_names)\n",
        "\n",
        "# 이상치 생성\n",
        "num_outliers = int(0.0048 * len(df))  # 이상치를 설정\n",
        "outliers = []\n",
        "\n",
        "for _ in range(num_outliers):\n",
        "    outlier_sample = df_scaled.sample(n=1).copy()  # 임의의 샘플 선택\n",
        "    for col in df_scaled.columns:\n",
        "        if np.random.rand() > 0.5:\n",
        "            # 최대값보다 큰 이상치 생성\n",
        "            outlier_sample[col] = df_scaled[col].max() + np.random.rand() * 3\n",
        "        else:\n",
        "            # 최소값보다 작은 이상치 생성\n",
        "            outlier_sample[col] = df_scaled[col].min() - np.random.rand() * 3\n",
        "    outliers.append(outlier_sample)\n",
        "\n",
        "outliers_df = pd.concat(outliers, ignore_index=True)\n",
        "\n",
        "# 이상치의 타겟 값 랜덤 할당\n",
        "outliers_df['target'] = np.random.choice(y, num_outliers)\n",
        "\n",
        "# 원본 데이터에 이상치 추가\n",
        "df_extended = pd.concat([df, outliers_df], ignore_index=True)\n",
        "\n",
        "# 이상치 추가 후 다시 정규화\n",
        "X_extended_scaled = scaler.fit_transform(df_extended.drop(columns=['target']))\n",
        "df_extended_scaled = pd.DataFrame(X_extended_scaled, columns=data.feature_names)\n",
        "\n",
        "# 새로운 X, y 정의\n",
        "X = df_extended_scaled.values  # NumPy 배열로 변환\n",
        "y = df_extended['target'].values  # 타겟 값 추출\n",
        "\n",
        "# 기존 이상치 식별\n",
        "df_zscores = df_scaled.apply(zscore)  # Z-score 계산\n",
        "is_existing_outlier = (df_zscores.abs() > 3).any(axis=1)  # 절대값이 3 이상인 경우 이상치로 판단\n",
        "\n",
        "total_existing_outliers = is_existing_outlier.sum()  # 기존 이상치 수\n",
        "total_outliers = total_existing_outliers + num_outliers  # 총 이상치 수\n",
        "total_samples = len(df_extended)  # 전체 샘플 수\n",
        "total_outlier_percentage = (total_outliers / total_samples) * 100  # 이상치 비율 계산\n",
        "\n",
        "# 데이터 확인\n",
        "print(f\"확장된 데이터셋 크기: {X.shape}\")\n",
        "print(f\"확장된 타겟 크기: {y.shape}\")\n",
        "print(f\"이상치가 포함된 데이터셋의 총 샘플 수: {len(X)}\")\n",
        "print(f\"전체 이상치 비율: {total_outlier_percentage:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlb8XkcR1ceW",
        "outputId": "1e1b908a-c58a-4a2f-c737-9c797dd8c239"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "확장된 데이터셋 크기: (868, 18)\n",
            "확장된 타겟 크기: (868,)\n",
            "이상치가 포함된 데이터셋의 총 샘플 수: 868\n",
            "전체 이상치 비율: 5.07%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import load_iris\n",
        "from scipy.stats import zscore\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# 데이터셋 로딩\n",
        "data = fetch_openml(name='vehicle', version=1)\n",
        "X = data.data\n",
        "# Use LabelEncoder to convert categorical target to numerical\n",
        "encoder = LabelEncoder()\n",
        "y = encoder.fit_transform(data.target)\n",
        "df = pd.DataFrame(X, columns=data.feature_names)\n",
        "df['target'] = y  # 타겟 변수 추가\n",
        "\n",
        "\n",
        "# 데이터 정규화\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "df_scaled = pd.DataFrame(X_scaled, columns=data.feature_names)\n",
        "\n",
        "# 이상치 생성\n",
        "num_outliers = int(0.027 * len(df))  # 이상치를 설정\n",
        "outliers = []\n",
        "\n",
        "for _ in range(num_outliers):\n",
        "    outlier_sample = df_scaled.sample(n=1).copy()  # 임의의 샘플 선택\n",
        "    for col in df_scaled.columns:\n",
        "        if np.random.rand() > 0.5:\n",
        "            # 최대값보다 큰 이상치 생성\n",
        "            outlier_sample[col] = df_scaled[col].max() + np.random.rand() * 3\n",
        "        else:\n",
        "            # 최소값보다 작은 이상치 생성\n",
        "            outlier_sample[col] = df_scaled[col].min() - np.random.rand() * 3\n",
        "    outliers.append(outlier_sample)\n",
        "\n",
        "outliers_df = pd.concat(outliers, ignore_index=True)\n",
        "\n",
        "# 이상치의 타겟 값 랜덤 할당\n",
        "outliers_df['target'] = np.random.choice(y, num_outliers)\n",
        "\n",
        "# 원본 데이터에 이상치 추가\n",
        "df_extended = pd.concat([df, outliers_df], ignore_index=True)\n",
        "\n",
        "# 이상치 추가 후 다시 정규화\n",
        "X_extended_scaled = scaler.fit_transform(df_extended.drop(columns=['target']))\n",
        "df_extended_scaled = pd.DataFrame(X_extended_scaled, columns=data.feature_names)\n",
        "\n",
        "# 새로운 X, y 정의\n",
        "X = df_extended_scaled.values  # NumPy 배열로 변환\n",
        "y = df_extended['target'].values  # 타겟 값 추출\n",
        "\n",
        "# 기존 이상치 식별\n",
        "df_zscores = df_scaled.apply(zscore)  # Z-score 계산\n",
        "is_existing_outlier = (df_zscores.abs() > 3).any(axis=1)  # 절대값이 3 이상인 경우 이상치로 판단\n",
        "\n",
        "total_existing_outliers = is_existing_outlier.sum()  # 기존 이상치 수\n",
        "total_outliers = total_existing_outliers + num_outliers  # 총 이상치 수\n",
        "total_samples = len(df_extended)  # 전체 샘플 수\n",
        "total_outlier_percentage = (total_outliers / total_samples) * 100  # 이상치 비율 계산\n",
        "\n",
        "# 데이터 확인\n",
        "print(f\"확장된 데이터셋 크기: {X.shape}\")\n",
        "print(f\"확장된 타겟 크기: {y.shape}\")\n",
        "print(f\"이상치가 포함된 데이터셋의 총 샘플 수: {len(X)}\")\n",
        "print(f\"전체 이상치 비율: {total_outlier_percentage:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftb54d--1ceX",
        "outputId": "b5c85968-3463-4b90-c8a1-4b67eec1caed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "확장된 데이터셋 크기: (916, 18)\n",
            "확장된 타겟 크기: (916,)\n",
            "이상치가 포함된 데이터셋의 총 샘플 수: 916\n",
            "전체 이상치 비율: 10.04%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import load_iris\n",
        "from scipy.stats import zscore\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# 데이터셋 로딩\n",
        "data = fetch_openml(name='vehicle', version=1)\n",
        "X = data.data\n",
        "# Use LabelEncoder to convert categorical target to numerical\n",
        "encoder = LabelEncoder()\n",
        "y = encoder.fit_transform(data.target)\n",
        "df = pd.DataFrame(X, columns=data.feature_names)\n",
        "df['target'] = y  # 타겟 변수 추가\n",
        "\n",
        "\n",
        "# 데이터 정규화\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "df_scaled = pd.DataFrame(X_scaled, columns=data.feature_names)\n",
        "\n",
        "# 이상치 생성\n",
        "num_outliers = int(0.083 * len(df))  # 이상치를 설정\n",
        "outliers = []\n",
        "\n",
        "for _ in range(num_outliers):\n",
        "    outlier_sample = df_scaled.sample(n=1).copy()  # 임의의 샘플 선택\n",
        "    for col in df_scaled.columns:\n",
        "        if np.random.rand() > 0.5:\n",
        "            # 최대값보다 큰 이상치 생성\n",
        "            outlier_sample[col] = df_scaled[col].max() + np.random.rand() * 3\n",
        "        else:\n",
        "            # 최소값보다 작은 이상치 생성\n",
        "            outlier_sample[col] = df_scaled[col].min() - np.random.rand() * 3\n",
        "    outliers.append(outlier_sample)\n",
        "\n",
        "outliers_df = pd.concat(outliers, ignore_index=True)\n",
        "\n",
        "# 이상치의 타겟 값 랜덤 할당\n",
        "outliers_df['target'] = np.random.choice(y, num_outliers)\n",
        "\n",
        "# 원본 데이터에 이상치 추가\n",
        "df_extended = pd.concat([df, outliers_df], ignore_index=True)\n",
        "\n",
        "# 이상치 추가 후 다시 정규화\n",
        "X_extended_scaled = scaler.fit_transform(df_extended.drop(columns=['target']))\n",
        "df_extended_scaled = pd.DataFrame(X_extended_scaled, columns=data.feature_names)\n",
        "\n",
        "# 새로운 X, y 정의\n",
        "X = df_extended_scaled.values  # NumPy 배열로 변환\n",
        "y = df_extended['target'].values  # 타겟 값 추출\n",
        "\n",
        "# 기존 이상치 식별\n",
        "df_zscores = df_scaled.apply(zscore)  # Z-score 계산\n",
        "is_existing_outlier = (df_zscores.abs() > 3).any(axis=1)  # 절대값이 3 이상인 경우 이상치로 판단\n",
        "\n",
        "total_existing_outliers = is_existing_outlier.sum()  # 기존 이상치 수\n",
        "total_outliers = total_existing_outliers + num_outliers  # 총 이상치 수\n",
        "total_samples = len(df_extended)  # 전체 샘플 수\n",
        "total_outlier_percentage = (total_outliers / total_samples) * 100  # 이상치 비율 계산\n",
        "\n",
        "# 데이터 확인\n",
        "print(f\"확장된 데이터셋 크기: {X.shape}\")\n",
        "print(f\"확장된 타겟 크기: {y.shape}\")\n",
        "print(f\"이상치가 포함된 데이터셋의 총 샘플 수: {len(X)}\")\n",
        "print(f\"전체 이상치 비율: {total_outlier_percentage:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AScVTdP01ceX"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from itertools import combinations\n",
        "\n",
        "results = []\n",
        "\n",
        "# 데이터 전처리 및 분할\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# OvO 방식을 사용하여 각 SVM 유형에 대해 학습 및 평가\n",
        "for svm_type in ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']:\n",
        "    models_ovo = train_ovo(X_train, y_train, C=1.0, svm_type=svm_type)\n",
        "    y_pred_ovo = predict_ovo(X_test, models_ovo)\n",
        "\n",
        "    # y_pred_ovo 변수를 정의한 후에 데이터 타입 확인 및 변환\n",
        "    if y_test.dtype != np.int64:\n",
        "        y_test = y_test.astype(int)\n",
        "\n",
        "    if y_pred_ovo.dtype != np.int64:\n",
        "        y_pred_ovo = y_pred_ovo.astype(int)\n",
        "\n",
        "    accuracy_ovo = accuracy_score(y_test, y_pred_ovo)\n",
        "    results.append((svm_type, round(accuracy_ovo * 100, 2)))\n",
        "\n",
        "# 결과를 DataFrame으로 변환 및 출력\n",
        "results_df = pd.DataFrame(results, columns=['SVM Type', 'Accuracy'])\n",
        "print(results_df['Accuracy'].tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NdH3SowD1ceY"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "accuracy_data = {\n",
        "    '3.06%' : [76.86, 78.43, 77.65, 76.86, 72.55, 74.51],\n",
        "    '5.07%' : [73.56, 77.01, 73.95, 73.56, 73.18, 72.80],\n",
        "    '10.04%' : [70.18, 72.36, 70.91, 70.91, 65.82, 69.45]\n",
        "}\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Vehicle data (L-BFGS-B)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dI8C-Fmy2gsL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "accuracy_data = {\n",
        "    '3.06%' : [76.86, 78.43, 77.65, 76.86, 72.55, 74.51],\n",
        "    '5.07%' : [73.56, 77.01, 73.95, 73.56, 73.18, 72.80],\n",
        "    '10.04%' : [70.18, 72.36, 70.91, 70.91, 65.82, 69.45]\n",
        "}\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Vehicle data (HS)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iLIUsmCB2nTD"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "accuracy_data = {\n",
        "    '3.06%' : [76.86, 78.43, 77.65, 76.86, 72.55, 74.51],\n",
        "    '5.07%' : [73.56, 77.01, 73.95, 73.56, 73.18, 72.80],\n",
        "    '10.04%' : [70.18, 72.36, 70.91, 70.91, 65.82, 69.45]\n",
        "}\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Vehicle data (ACO)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vi7AC1kr2gsM"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "accuracy_data = {\n",
        "    '3.06%' : [76.86, 78.43, 77.65, 76.86, 72.55, 74.51],\n",
        "    '5.07%' : [73.56, 77.01, 73.95, 73.56, 73.18, 72.80],\n",
        "    '10.04%' : [70.18, 72.36, 70.91, 70.91, 65.82, 69.45]\n",
        "}\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Vehicle data (PSO)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PEndMdYA2gsM"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "accuracy_data = {\n",
        "    '3.06%' : [76.86, 78.43, 77.65, 76.86, 72.55, 74.51],\n",
        "    '5.07%' : [73.56, 77.01, 73.95, 73.56, 73.18, 72.80],\n",
        "    '10.04%' : [70.18, 72.36, 70.91, 70.91, 65.82, 69.45]\n",
        "}\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Vehicle data (SMO)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6zuAyFM72gsM"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "svm_types = ['L1', 'L2', 'Fair', 'Cauchy', 'Welsch', 'Geman-McClure']\n",
        "accuracy_data = {\n",
        "    '3.06%' : [76.86, 78.43, 77.65, 76.86, 72.55, 74.51],\n",
        "    '5.07%' : [73.56, 77.01, 73.95, 73.56, 73.18, 72.80],\n",
        "    '10.04%' : [70.18, 72.36, 70.91, 70.91, 65.82, 69.45]\n",
        "}\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame(accuracy_data, index=svm_types)\n",
        "df.index.name = 'Vehicle data (GA)'\n",
        "\n",
        "# 데이터프레임 출력\n",
        "print(df)\n"
      ]
    }
  ]
}